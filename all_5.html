<!doctype html>
<html class="no-js" lang="en">
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?fdc936c9f5a3b72177541183cdeb8cb3";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  techlarry
  
  </title>
  
  
  <link href="atom.xml" rel="alternate" title="techlarry" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />
    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/github.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
  function before_search(){
    var searchVal = 'site:larryim.cc ' + document.getElementById('search_input').value;
    document.getElementById('search_q').value = searchVal;
    return true;
  }
</script>
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>


  <section class="top-bar-section">
  <div class="row">
      <div style="position: relative;width:100%;"><div style="position: absolute; width:100%;">
        <ul id="main-menu" class="left">
        
        <li id=""><a target="_self" href="index.html">Home</a></li>
        
        <li id=""><a target="_self" href="archives.html">Archives</a></li>
        
        <li id=""><a target="_self" href="about.html">About</a></li>
        
        <li id=""><a target="_self" href="category.html">Category</a></li>
        
        </ul>

        <ul class="right" id="search-wrap">
          <li>
<form target="_blank" onsubmit="return before_search();" action="http://google.com/search" method="get">
    <input type="hidden" id="search_q" name="q" value="" />
    <input tabindex="1" type="search" id="search_input"  placeholder="Search"/>
</form>
</li>
          </ul>
      </div></div>
  </div>
  </section>

</nav>

        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; techlarry</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
        
        <li><a target="_self" href="index.html">Home</a></li>
        
        <li><a target="_self" href="archives.html">Archives</a></li>
        
        <li><a target="_self" href="about.html">About</a></li>
        
        <li><a target="_self" href="category.html">Category</a></li>
        

    <li><label>Categories</label></li>

        
            <li><a href="Leetcode.html">Leetcode</a></li>
        
            <li><a href="C/C++.html">C/C++</a></li>
        
            <li><a href="Python%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95.html">Python数据结构与算法</a></li>
        
            <li><a href="Python%E7%89%B9%E6%80%A7.html">Python特性</a></li>
        
            <li><a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html">机器学习</a></li>
        
            <li><a href="Python%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E4%B8%89%E7%BB%B4%E5%8F%AF%E8%A7%86%E5%8C%96.html">Python科学计算三维可视化</a></li>
        
            <li><a href="English.html">English</a></li>
        
            <li><a href="Computer%20System.html">Computer System</a></li>
        
            <li><a href="Deep%20Learning.html">Deep Learning</a></li>
        
            <li><a href="Latex.html">Latex</a></li>
        
            <li><a href="%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.html">操作系统</a></li>
        
            <li><a href="Linux%20%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B.html">Linux 系统编程</a></li>
        
            <li><a href="%E6%95%B0%E6%8D%AE%E5%BA%93.html">数据库</a></li>
        
            <li><a href="Tensorflow.html">Tensorflow</a></li>
        
            <li><a href="Data%20Science.html">Data Science</a></li>
        
            <li><a href="Big%20Data.html">Big Data</a></li>
         

      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>


        <section id="main-content" role="main" class="scroll-container">
        
       

 <script type="text/javascript">
	$(function(){
		$('#menu_item_index').addClass('is_active');
	});
</script>
<div class="row">
	<div class="large-8 medium-8 columns">
		<div class="markdown-body home-categories">
		
			<div class="article">
                <a class="clearlink" href="anomaly-detection.html">
                
                  <h1>Machine Learning(9): Anomaly Detection AND Recommender System</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">ML:Anomaly Detection</a>
<ul>
<li>
<a href="#toc_1">Problem Motivation</a>
</li>
<li>
<a href="#toc_2">Gaussian Distribution</a>
</li>
<li>
<a href="#toc_3">Algorithm</a>
</li>
<li>
<a href="#toc_4">Developing and Evaluating an Anomaly Detection System</a>
</li>
<li>
<a href="#toc_5">Anomaly Detection vs. Supervised Learning</a>
</li>
<li>
<a href="#toc_6">Choosing What Features to Use</a>
</li>
<li>
<a href="#toc_7">Multivariate Gaussian Distribution</a>
</li>
<li>
<a href="#toc_8">Anomaly Detection using the Multivariate Gaussian Distribution</a>
</li>
<li>
<a href="#toc_9">ML:Recommender Systems</a>
</li>
<li>
<a href="#toc_10">Problem Formulation</a>
</li>
<li>
<a href="#toc_11">Content Based Recommendations</a>
</li>
<li>
<a href="#toc_12">Collaborative Filtering</a>
</li>
<li>
<a href="#toc_13">Collaborative Filtering Algorithm</a>
</li>
<li>
<a href="#toc_14">Vectorization: Low Rank Matrix Factorization</a>
</li>
<li>
<a href="#toc_15">Implementation Detail: Mean Normalization</a>
</li>
</ul>
</li>
</ul>


<h1 id="toc_0">ML:Anomaly Detection</h1>

<h2 id="toc_1">Problem Motivation</h2>

<p>Just like in other learning problems, we are given a dataset \({x^{(1)}, x^{(2)},\dots,x^{(m)}}\).</p>

<p>We are then given a new example, \(x_{test}\), and we want to know whether this new example is abnormal/anomalous.</p>

<p>We define a &quot;model&quot; p(x) that tells us the probability the example is not anomalous. We also use a threshold ϵ (epsilon) as a dividing line so we can say which examples are anomalous and which are not.</p>

<p>A very common application of anomaly detection is detecting fraud:</p>

<ul>
<li><p>\(x^{(i)} =\) features of user i&#39;s activities</p></li>
<li><p>Model p(x) from the data.</p></li>
<li><p>Identify unusual users by checking which have p(x)&lt;ϵ.</p></li>
</ul>

<p>If our anomaly detector is flagging <strong>too many</strong> anomalous examples, then we need to <strong>decrease</strong> our threshold ϵ</p>

<h2 id="toc_2">Gaussian Distribution</h2>

<p>The Gaussian Distribution is a familiar bell-shaped curve that can be described by a function \(\mathcal{N}(\mu,\sigma^2)\)</p>

<p>Let x∈ℝ. If the probability distribution of x is Gaussian with mean μ, variance \(\sigma^2\), then:</p>

<p>\(x \sim \mathcal{N}(\mu, \sigma^2)\)</p>

<p>The little ∼ or &#39;tilde&#39; can be read as &quot;distributed as.&quot;</p>

<p>The Gaussian Distribution is parameterized by a mean and a variance.</p>

<p>Mu, or μ, describes the center of the curve, called the mean. The width of the curve is described by sigma, or σ, called the standard deviation.</p>

<p>The full function is as follows:</p>

<p>\(\large p(x;\mu,\sigma^2) = \dfrac{1}{\sigma\sqrt{(2\pi)}}e^{-\dfrac{1}{2}(\dfrac{x - \mu}{\sigma})^2}\)</p>

<p>We can estimate the parameter μ from a given dataset by simply taking the average of all the examples:</p>

<p>\(\mu = \dfrac{1}{m}\displaystyle \sum_{i=1}^m x^{(i)}\)</p>

<p>We can estimate the other parameter, \(\sigma^2\), with our familiar squared error formula:</p>

<p>\(\sigma^2 = \dfrac{1}{m}\displaystyle \sum_{i=1}^m(x^{(i)} - \mu)^2\)</p>

<p><img src="media/15045011015887/15045018120355.png" alt="Gaussian distribution"/></p>

<h2 id="toc_3">Algorithm</h2>

<p>Given a training set of examples, \(\lbrace x^{(1)},\dots,x^{(m)}\rbrace\) where each example is a vector, \(x \in \mathbb{R}^n\).</p>

<p>\(p(x) = p(x_1;\mu_1,\sigma_1^2)p(x_2;\mu_2,\sigma^2_2)\cdots p(x_n;\mu_n,\sigma^2_n)\)</p>

<p>In statistics, this is called an &quot;independence assumption&quot; on the values of the features inside training example x.</p>

<p>More compactly, the above expression can be written as follows:</p>

<p>\(= \displaystyle \prod^n_{j=1} p(x_j;\mu_j,\sigma_j^2)\)</p>

<p><strong>The algorithm</strong></p>

<p>Choose features \(x_i\) that you think might be indicative of anomalous examples.</p>

<p>Fit parameters \(\mu_1,\dots,\mu_n,\sigma_1^2,\dots,\sigma_n^2\)</p>

<p>Calculate \(\mu_j = \dfrac{1}{m}\displaystyle \sum_{i=1}^m x_j^{(i)}\)</p>

<p>Calculate \(\sigma^2_j = \dfrac{1}{m}\displaystyle \sum_{i=1}^m(x_j^{(i)} - \mu_j)^2\)</p>

<p>Given a new example x, compute p(x):</p>

<p>\(p(x) = \displaystyle \prod^n_{j=1} p(x_j;\mu_j,\sigma_j^2) = \prod\limits^n_{j=1} \dfrac{1}{\sqrt{2\pi}\sigma_j}exp(-\dfrac{(x_j - \mu_j)^2}{2\sigma^2_j})\)</p>

<p>Anomaly if p(x)&lt;ϵ</p>

<p>A vectorized version of the calculation for μ is \(\mu = \dfrac{1}{m}\displaystyle \sum_{i=1}^m x^{(i)}\). You can vectorize \(\sigma^2\) similarly.</p>

<h2 id="toc_4">Developing and Evaluating an Anomaly Detection System</h2>

<p>To evaluate our learning algorithm, we take some labeled data, categorized into anomalous and non-anomalous examples ( y = 0 if normal, y = 1 if anomalous).</p>

<p>Among that data, take a large proportion of <strong>good</strong> , non-anomalous data for the training set on which to train p(x).</p>

<p>Then, take a smaller proportion of mixed anomalous and non-anomalous examples (you will usually have many more non-anomalous examples) for your cross-validation and test sets.</p>

<p>For example, we may have a set where 0.2% of the data is anomalous. We take 60% of those examples, all of which are good (y=0) for the training set. We then take 20% of the examples for the cross-validation set (with 0.1% of the anomalous examples) and another 20% from the test set (with another 0.1% of the anomalous).</p>

<p>In other words, we split the data 60/20/20 training/CV/test and then split the anomalous examples 50/50 between the CV and test sets.</p>

<p><strong>Algorithm evaluation:</strong></p>

<p>Fit model p(x) on training set \(\lbrace x^{(1)},\dots,x^{(m)} \rbrace\)</p>

<p>On a cross validation/test example x, predict:</p>

<p>If p(x) &lt; ϵ ( <strong>anomaly</strong> ), then y=1</p>

<p>If p(x) ≥ ϵ ( <strong>normal</strong> ), then y=0</p>

<p>Possible evaluation metrics (see &quot;Machine Learning System Design&quot; section):</p>

<ul>
<li><p>True positive, false positive, false negative, true negative.</p></li>
<li><p>Precision/recall</p></li>
<li><p>\(F_1\) score</p></li>
</ul>

<p>Note that we use the cross-validation set to choose parameter ϵ</p>

<h2 id="toc_5">Anomaly Detection vs. Supervised Learning</h2>

<p>When do we use anomaly detection and when do we use supervised learning?</p>

<p>Use anomaly detection when...</p>

<ul>
<li><p>We have a very small number of positive examples (y=1 ... 0-20 examples is common) and a large number of negative (y=0) examples.</p></li>
<li><p>We have many different &quot;types&quot; of anomalies and it is hard for any algorithm to learn from positive examples what the anomalies look like; future anomalies may look nothing like any of the anomalous examples we&#39;ve seen so far.</p></li>
</ul>

<p>Use supervised learning when...</p>

<ul>
<li><p>We have a large number of both positive and negative examples. In other words, the training set is more evenly divided into classes.</p></li>
<li><p>We have enough positive examples for the algorithm to get a sense of what new positives examples look like. The future positive examples are likely similar to the ones in the training set.</p></li>
</ul>

<h2 id="toc_6">Choosing What Features to Use</h2>

<p>The features will greatly affect how well your anomaly detection algorithm works.</p>

<p>We can check that our features are <strong>gaussian</strong> by plotting a histogram of our data and checking for the bell-shaped curve.</p>

<p>Some <strong>transforms</strong> we can try on an example feature x that does not have the bell-shaped curve are:</p>

<ul>
<li><p>log(x)</p></li>
<li><p>log(x+1)</p></li>
<li><p>log(x+c) for some constant</p></li>
<li><p>\(\sqrt{x}\)</p></li>
<li><p>\(x^{1/3}\)</p></li>
</ul>

<p>We can play with each of these to try and achieve the gaussian shape in our data.</p>

<p>There is an <strong>error analysis procedure</strong> for anomaly detection that is very similar to the one in supervised learning.</p>

<p>Our goal is for p(x) to be large for normal examples and small for anomalous examples.</p>

<p>One common problem is when p(x) is similar for both types of examples. In this case, you need to examine the anomalous examples that are giving high probability in detail and try to figure out new features that will better distinguish the data.</p>

<p>In general, choose features that might take on unusually large or small values in the event of an anomaly.</p>

<h2 id="toc_7">Multivariate Gaussian Distribution</h2>

<p>The multivariate gaussian distribution is an extension of anomaly detection and may (or may not) catch more anomalies.</p>

<p>Instead of modeling \(p(x_1),p(x_2),\dots\) separately, we will model p(x) all in one go. Our parameters will be: \(\mu \in \mathbb{R}^n\) and \(\Sigma \in \mathbb{R}^{n \times n}\)</p>

<p>\(p(x;\mu,\Sigma) = \dfrac{1}{(2\pi)^{n/2} |\Sigma|^{1/2}} exp(-1/2(x-\mu)^T\Sigma^{-1}(x-\mu))\)</p>

<p>The important effect is that we can model oblong gaussian contours, allowing us to better fit data that might not fit into the normal circular contours.</p>

<p>Varying Σ changes the shape, width, and orientation of the contours. Changing μ will move the center of the distribution.</p>

<p>Check also:</p>

<ul>
<li>  <a href="http://cs229.stanford.edu/section/gaussians.pdf">The Multivariate Gaussian Distribution</a> <a href="http://cs229.stanford.edu/section/gaussians.pdf">http://cs229.stanford.edu/section/gaussians.pdf</a> Chuong B. Do, October 10, 2008.</li>
</ul>

<p>Following examples illustrate the basic meaning of parameters in multivariable gaussian distribution:</p>

<pre><code>mean = [0, 0]
cov = [[1, 0], [0, 1]]  # diagonal covariance
# Draw random samples from a multivariate normal distribution
x, y = np.random.multivariate_normal(mean, cov, 5000).T
plt.plot(x, y, &#39;x&#39;,color=&#39;y&#39;)
plt.axis(&#39;equal&#39;)
plt.hold

# change mean
mean = [0, 10]
x, y = np.random.multivariate_normal(mean, cov, 5000).T
plt.plot(x, y, &#39;.&#39;, color=&#39;b&#39;)

# change variances
mean = [10, 10]
cov = [[1, 0], [0, 10]]  # diagonal covariance
x, y = np.random.multivariate_normal(mean, cov, 5000).T
plt.plot(x, y, &#39;-&#39;, color=&#39;r&#39;)
plt.show()
</code></pre>

<p><img src="media/15045011015887/15045091983941.jpg" alt="demo of multivarible gaussian"/></p>

<h2 id="toc_8">Anomaly Detection using the Multivariate Gaussian Distribution</h2>

<p>When doing anomaly detection with multivariate gaussian distribution, we compute μ and Σ normally. We then compute p(x) using the new formula in the previous section and flag an anomaly if p(x) &lt; ϵ.</p>

<p>The original model for p(x) corresponds to a multivariate Gaussian where the contours of \(p(x;\mu,\Sigma)\) are axis-aligned.</p>

<p>The multivariate Gaussian model can automatically capture correlations between different features of x.</p>

<p>However, the original model maintains some advantages: it is computationally cheaper (no matrix to invert, which is costly for large number of features) and it performs well even with small training set size (in multivariate Gaussian model, it should be greater than the number of features for Σ to be invertible).</p>

<h2 id="toc_9">ML:Recommender Systems</h2>

<h2 id="toc_10">Problem Formulation</h2>

<p>Recommendation is currently a very popular application of machine learning.</p>

<p>Say we are trying to recommend movies to customers. We can use the following definitions</p>

<ul>
<li><p>\(n_u =\) number of users</p></li>
<li><p>\(n_m =\) number of movies</p></li>
<li><p>\(r(i,j) = 1\) if user j has rated movie i</p></li>
<li><p>\(y(i,j) =\) rating given by user j to movie i (defined only if r(i,j)=1)</p></li>
</ul>

<h2 id="toc_11">Content Based Recommendations</h2>

<p>We can introduce two features, \(x_1\) and \(x_2\) which represents how much romance or how much action a movie may have (on a scale of 0−1).</p>

<p>One approach is that we could do linear regression for every single user. For each user j, learn a parameter \(\theta^{(j)} \in \mathbb{R}^3\). Predict user j as rating movie i with \((\theta^{(j)})^Tx^{(i)}\) stars.</p>

<ul>
<li><p>\(\theta^{(j)} =\) parameter vector for user j</p></li>
<li><p>\(x^{(i)} =\) feature vector for movie i</p></li>
</ul>

<p>For user j, movie i, predicted rating: \((\theta^{(j)})^T(x^{(i)})\)</p>

<ul>
<li>  \(m^{(j)} =\) number of movies rated by user j</li>
</ul>

<p>To learn \(\theta^{(j)}\), we do the following</p>

<p>\(min_{\theta^{(j)}} = \dfrac{1}{2}\displaystyle \sum_{i:r(i,j)=1} ((\theta^{(j)})^T(x^{(i)}) - y^{(i,j)})^2 + \dfrac{\lambda}{2} \sum_{k=1}^n(\theta_k^{(j)})^2\)</p>

<p>This is our familiar linear regression. The base of the first summation is choosing all i such that \(r(i,j) = 1\).</p>

<p>To get the parameters for all our users, we do the following:</p>

<p>\(min_{\theta^{(1)},\dots,\theta^{(n_u)}} = \dfrac{1}{2}\displaystyle \sum_{j=1}^{n_u} \sum_{i:r(i,j)=1} ((\theta^{(j)})^T(x^{(i)}) - y^{(i,j)})^2 + \dfrac{\lambda}{2} \sum_{j=1}^{n_u} \sum_{k=1}^n(\theta_k^{(j)})^2\)</p>

<p>We can apply our linear regression gradient descent update using the above cost function.</p>

<p>The only real difference is that we <strong>eliminate the constant</strong> \(\dfrac{1}{m}\).</p>

<h2 id="toc_12">Collaborative Filtering</h2>

<p>It can be very difficult to find features such as &quot;amount of romance&quot; or &quot;amount of action&quot; in a movie. To figure this out, we can use <u>feature finders</u> .</p>

<p>We can let the users tell us how much they like the different genres, providing their parameter vector immediately for us.</p>

<p>To infer the features from given parameters, we use the squared error function with regularization over all the users:</p>

<p>\(min_{x^{(1)},\dots,x^{(n_m)}} \dfrac{1}{2} \displaystyle \sum_{i=1}^{n_m} \sum_{j:r(i,j)=1} ((\theta^{(j)})^T x^{(i)} - y^{(i,j)})^2 + \dfrac{\lambda}{2}\sum_{i=1}^{n_m} \sum_{k=1}^{n} (x_k^{(i)})^2\)</p>

<p>You can also <strong>randomly guess</strong> the values for theta to guess the features repeatedly. You will actually converge to a good set of features.</p>

<h2 id="toc_13">Collaborative Filtering Algorithm</h2>

<p>To speed things up, we can simultaneously minimize our features and our parameters:</p>

<p>\(J(x,\theta) = \dfrac{1}{2} \displaystyle \sum_{(i,j):r(i,j)=1}((\theta^{(j)})^Tx^{(i)} - y^{(i,j)})^2 + \dfrac{\lambda}{2}\sum_{i=1}^{n_m} \sum_{k=1}^{n} (x_k^{(i)})^2 + \dfrac{\lambda}{2}\sum_{j=1}^{n_u} \sum_{k=1}^{n} (\theta_k^{(j)})^2\)</p>

<p>It looks very complicated, but we&#39;ve only combined the cost function for theta and the cost function for x.</p>

<p>Because the algorithm can learn them itself, the bias units where x0=1 have been removed, therefore x∈ℝn and θ∈ℝn.</p>

<p>These are the steps in the algorithm:</p>

<ol>
<li><p>Initialize \(x^{(i)},...,x^{(n_m)},\theta^{(1)},...,\theta^{(n_u)}\) to small random values. This serves to break symmetry and ensures that the algorithm learns features \(x^{(i)},...,x^{(n_m)}\) that are different from each other.</p></li>
<li><p>Minimize \(J(x^{(i)},...,x^{(n_m)},\theta^{(1)},...,\theta^{(n_u)})\) using gradient descent (or an advanced optimization algorithm).E.g. for every \(j=1,...,n_u,i=1,...n_m\):\(x_k^{(i)} := x_k^{(i)} - \alpha\left (\displaystyle \sum_{j:r(i,j)=1}{((\theta^{(j)})^T x^{(i)} - y^{(i,j)}) \theta_k^{(j)}} + \lambda x_k^{(i)} \right)\)\(\theta_k^{(j)} := \theta_k^{(j)} - \alpha\left (\displaystyle \sum_{i:r(i,j)=1}{((\theta^{(j)})^T x^{(i)} - y^{(i,j)}) x_k^{(i)}} + \lambda \theta_k^{(j)} \right)\)</p></li>
<li><p>For a user with parameters θ and a movie with (learned) features x, predict a star rating of \(\theta^Tx\).</p></li>
</ol>

<h2 id="toc_14">Vectorization: Low Rank Matrix Factorization</h2>

<p>Given matrices X (each row containing features of a particular movie) and Θ (each row containing the weights for those features for a given user), then the full matrix Y of all predicted ratings of all movies by all users is given simply by: \(Y = X\Theta^T\).</p>

<p>Predicting how similar two movies i and j are can be done using the distance between their respective feature vectors x. Specifically, we are looking for a small value of \(||x^{(i)} - x^{(j)}||\).</p>

<h2 id="toc_15">Implementation Detail: Mean Normalization</h2>

<p>If the ranking system for movies is used from the previous lectures, then new users (who have watched no movies), will be assigned new movies incorrectly. Specifically, they will be assigned θ with all components equal to zero due to the minimization of the regularization term. That is, we assume that the new user will rank all movies 0, which does not seem intuitively correct.</p>

<p>We rectify this problem by normalizing the data relative to the mean. First, we use a matrix Y to store the data from previous ratings, where the ith row of Y is the ratings for the ith movie and the jth column corresponds to the ratings for the jth user.</p>

<p>We can now define a vector</p>

<p>\(\mu = [\mu_1, \mu_2, \dots , \mu_{n_m}]\)</p>

<p>such that</p>

<p>\(\mu_i = \frac{\sum_{j:r(i,j)=1}{Y_{i,j}}}{\sum_{j}{r(i,j)}}\)</p>

<p>Which is effectively the mean of the previous ratings for the ith movie (where only movies that have been watched by users are counted). We now can normalize the data by subtracting u, the mean rating, from the actual ratings for each user (column in matrix Y):</p>

<p>As an example, consider the following matrix Y and mean ratings μ:</p>

<p>\(Y = \begin{bmatrix} 5 &amp; 5 &amp; 0 &amp; 0 \newline 4 &amp; ? &amp; ? &amp; 0 \newline 0 &amp; 0 &amp; 5 &amp; 4 \newline 0 &amp; 0 &amp; 5 &amp; 0 \newline \end{bmatrix}, \quad \mu = \begin{bmatrix} 2.5 \newline 2 \newline 2.25 \newline 1.25 \newline \end{bmatrix}\)</p>

<p>The resulting Y′ vector is:</p>

<p>\(Y&#39; = \begin{bmatrix} 2.5 &amp; 2.5 &amp; -2.5 &amp; -2.5 \newline 2 &amp; ? &amp; ? &amp; -2 \newline -.2.25 &amp; -2.25 &amp; 3.75 &amp; 1.25 \newline -1.25 &amp; -1.25 &amp; 3.75 &amp; -1.25 \end{bmatrix}\)</p>

<p>Now we must slightly modify the linear regression prediction to include the mean normalization term:</p>

<p>\((\theta^{(j)})^T x^{(i)} + \mu_i\)</p>

<p>Now, for a new user, the initial predicted values will be equal to the μ term instead of simply being initialized to zero, which is more accurate.</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/9/4</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html'>机器学习</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="virtualenv.html">
                
                  <h1>Python `virtualenv` on mac</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<h2 id="toc_0">Install</h2>

<p>Install <code>virtualenv</code> using <code>conda</code> instead of <code>pip</code>, because it might raise error (see on <a href="virtualenv%20--no-site-packages%20venv">StackOverflow</a>)</p>

<pre><code class="language-bash">conda install virtualenv
</code></pre>

<h2 id="toc_1">create your environment</h2>

<p>Now you can create your python environment for your particular programs. For example, under the folder <code>your project</code>, you create an environment called <code>.venv</code> by:</p>

<pre><code class="language-bash">virtualenv --no-site-packages .venv
</code></pre>

<p>The command <code>--no-site-packages</code> requires the environment should not access to global site-packages (as default now).</p>

<p>Before running your program in your created environment, you need to activate it:</p>

<pre><code class="language-python">source .venv/bin/activate
</code></pre>

<p>And remember to deactivate it whenever you are done.:</p>

<pre><code>deactivate
</code></pre>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/9/2</span>
                    
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="convolutional-neural-networks.html">
                
                  <h1>Convolutional Neural Networks</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">Architecture</a>
<ul>
<li>
<ul>
<li>
<a href="#toc_1">Conv layers</a>
</li>
<li>
<a href="#toc_2">Pooling</a>
</li>
<li>
<a href="#toc_3">Fully-connected layers</a>
</li>
<li>
<a href="#toc_4">Layer Patterns</a>
</li>
<li>
<a href="#toc_5">Layer Sizing Patterns</a>
</li>
</ul>
</li>
<li>
<a href="#toc_6">Computational Considerations</a>
</li>
</ul>
</li>
</ul>
</li>
<li>
<a href="#toc_7">The memory bottleneck is the largest bottleneck when constructing ConvNet architectures.</a>


<h2 id="toc_0">Architecture</h2>

<p>Three main types of layers to build ConvNet architectures: <strong>Convolutional Layer, Pooling Layer and Fully-Connected Layer</strong>.</p>

<p>The layers of a ConvNet have neurons arranged in 3 dimensions: <strong>width, height, depth</strong>.</p>

<p><img src="media/15042285789723/15043413926666.jpg" alt="convolutional"/></p>

<p>A simple ConvNet for <a href="https://www.cs.toronto.edu/%7Ekriz/cifar.html">CIFAR-10</a> classification could have the architecture as follows:</p>

<ul>
<li><strong>INPUT</strong> [width x height x color channel] will hold the raw pixel values of the image, in this case an image of width, height, and with three color channels R,G,B.</li>
<li><strong>CONV layer</strong> will compute the output of neurons that are connected to local regions in the input, each computing a dot product between their weights and a small region they are connected to in the input volume.</li>
<li><strong>RELU layer</strong> will apply an elementwise activation function, such as the <code>max(0,x)</code> thresholding at zero. </li>
<li><strong>POOL layer</strong> will perform a downsampling operation along the spatial dimensions (width, height).</li>
<li><strong>FC</strong> (i.e. fully-connected) layer will compute the class scores, where each score corresponding to the 10 categories of CIFAR-10. As with ordinary Neural Networks and as the name implies, each neuron in this layer will be connected to all the numbers in the previous volume.</li>
</ul>

<h4 id="toc_1">Conv layers</h4>

<p>Four hyperparameters <strong>depth</strong>(K), <strong>spatial extent</strong>(\(F\)), <strong>stride</strong>(\(S\)) and <strong>zero-padding</strong>(\(P\)) control the size of the output volume from the input volume (\(W\)).</p>

<p><strong>Summary</strong>. To summarize, the Conv Layer:</p>

<ul>
<li>Accepts a volume of size \(W_1 \times H_1 \times D_1\)</li>
<li>Requires four hyperparameters: 

<ul>
<li>Number of filters \(K\), </li>
<li>their spatial extent \(F\), </li>
<li>the stride \(S\), </li>
<li>the amount of zero padding \(P\).</li>
</ul></li>
<li>Produces a volume of size \(W_2 \times H_2 \times D_2\) where:

<ul>
<li>\(W_2 = (W_1 - F + 2P)/S + 1\)</li>
<li>\(H_2 = (H_1 - F + 2P)/S + 1\) (i.e. width and height are computed equally by symmetry)</li>
<li>\(D_2 = K\)</li>
</ul></li>
<li>With parameter sharing, it introduces \(F \cdot F \cdot D_1\) weights per filter, for a total of \((F \cdot F \cdot D_1) \cdot K\) weights and \(K\) biases.</li>
<li>In the output volume, the \(d\)-th depth slice (of size \(W_2 \times H_2\)) is the result of performing a valid convolution of the \(d\)-th filter over the input volume with a stride of \(S\), and then offset by \(d\)-th bias.</li>
</ul>

<p><strong>parameter sharing</strong>: the neurons in each depth slice(i.e \(K\)) to use the same weights and bias. </p>

<h4 id="toc_2">Pooling</h4>

<p><code>max pooling</code> is the most common function performed on the pooling units, others like <code>average pooling</code> or <code>L2-norm pooling</code> is work worse in practice. And Many people dislike the pooling operation and think that we can get away without it.</p>

<h4 id="toc_3">Fully-connected layers</h4>

<p>Neurons in a fully connected layer have full connections to all activations in the previous layer.</p>

<h4 id="toc_4">Layer Patterns</h4>

<p>The most common form of a ConvNet architecture stacks a few CONV-RELU layers, follows them with POOL layers, and <strong>repeats this pattern until the image has been merged spatially to a small size</strong>. At some point, it is common to transition to fully-connected layers. The last fully-connected layer holds the output, such as the class scores. In other words, the most common ConvNet architecture follows the pattern:</p>

<p>\[INPUT \rightarrow  [[CONV \rightarrow  RELU]*N \rightarrow  POOL?]*M \rightarrow [FC -&gt; RELU]*K \rightarrow  FC\]</p>

<p>where the <code>*</code> indicates repetition, and the <code>POOL?</code> indicates an optional pooling layer. Moreover, <code>N &gt;= 0</code> (and usually <code>N &lt;= 3</code>), <code>M &gt;= 0</code>, <code>K &gt;= 0</code> (and usually <code>K &lt; 3</code>). For example, here are some common ConvNet architectures you may see that follow this pattern:</p>

<p><strong>Note</strong>: <code>INPUT -&gt; FC</code>, implements a linear classifier. Here <code>N = M = K = 0</code>.<br/>
<strong>Note</strong>: Prefer a stack of small filter CONV to one large receptive field CONV layer, because of few parameters needed and expressing more powerful features of the input.</p>

<h4 id="toc_5">Layer Sizing Patterns</h4>

<ul>
<li><strong>input layer</strong>:  Common numbers include 32 (e.g. CIFAR-10), 64, 96 (e.g. STL-10), or 224 (e.g. common ImageNet ConvNets), 384, and 512.</li>
<li><strong>conv layers</strong>: small filters(\(3\times3\) or \(5\times5\)), with \(S=1\). Crucially, <em>padding the input volume with zeros in such way that the conv layer does not alter the spatial dimensions of the input.</em> </li>
<li><strong>pool layers</strong>: use max-pooling with \(F=2, S=2\) or \(F=3, S=2\)</li>
</ul>

<h3 id="toc_6">Computational Considerations</h3>

<h1 id="toc_7">The memory bottleneck is the largest bottleneck when constructing ConvNet architectures.</h1>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/9/2</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Deep%20Learning.html'>Deep Learning</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="15042359761425.html">
                
                  <h1>Transfer Learning</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p><code>Transfer learning</code> focuses on storing knowledge gained while solving one problem and applying it to a different but related problem.</p>

<p>Generally, transfer learning will work only well if the inputs have similar low-level features.</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/9/1</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html'>机器学习</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="15041905263785.html">
                
                  <h1>cross-entropy</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>\[<br/>
L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right) \hspace{0.5in} \text{or equivalently} \hspace{0.5in} L_i = -f_{y_i} + \log\sum_j e^{f_j}<br/>
\]</p>

<p>where we are using the notation \(f_j\) to mean the j-th element of the vector of class scores \(f\). As before, the full loss for the dataset is the mean of \(L_i\) over all training examples together with a regularization term \(R(W)\). The function \(f_j(z) = \frac{e^{z_j}}{\sum_k e^{z_k}} \) is called the <strong>softmax function</strong>: It takes a vector of arbitrary real-valued scores (in \(z\)) and squashes it to a vector of values between zero and one that sum to one. The full cross-entropy loss that involves the softmax function might look scary if you&#39;re seeing it for the first time but it is relatively easy to motivate.</p>

<p><strong>Information theory view</strong>. The <em>cross-entropy</em> between a &quot;true&quot; distribution \(p\) and an estimated distribution \(q\) is defined as:</p>

<p>\[<br/>
H(p,q) = - \sum_x p(x) \log q(x)<br/>
\]</p>

<p>The Softmax classifier is hence minimizing the cross-entropy between the estimated class probabilities ( \(q = e^{f_{y_i}}  / \sum_j e^{f_j} \) as seen above) and the &quot;true&quot; distribution, which in this interpretation is the distribution where all probability mass is on the correct class (i.e. \(p = [0, \ldots 1, \ldots, 0]\) contains a single 1 at the \(y_i\) -th position.). Moreover, since the cross-entropy can be written in terms of entropy and the Kullback-Leibler divergence as \(H(p,q) = H(p) + D_{KL}(p\|\|q)\), and the entropy of the delta function \(p\) is zero, this is also equivalent to minimizing the KL divergence between the two distributions (a measure of distance). In other words, the cross-entropy objective <em>wants</em> the predicted distribution to have all of its mass on the correct answer.</p>

<p><strong>Probabilistic interpretation</strong>. Looking at the expression, we see that</p>

<p>\[<br/>
P(y_i \mid x_i; W) = \frac{e^{f_{y_i}}}{\sum_j e^{f_j} }<br/>
\]</p>

<p>can be interpreted as the (normalized) probability assigned to the correct label \(y_i\) given the image \(x_i\) and parameterized by \(W\). To see this, remember that the Softmax classifier interprets the scores inside the output vector \(f\) as the unnormalized log probabilities. Exponentiating these quantities therefore gives the (unnormalized) probabilities, and the division performs the normalization so that the probabilities sum to one. In the probabilistic interpretation, we are therefore minimizing the negative log likelihood of the correct class, which can be interpreted as performing <em>Maximum Likelihood Estimation</em> (MLE). A nice feature of this view is that we can now also interpret the regularization term \(R(W)\) in the full loss function as coming from a Gaussian prior over the weight matrix \(W\), where instead of MLE we are performing the <em>Maximum a posteriori</em> (MAP) estimation. We mention these interpretations to help your intuitions, but the full details of this derivation are beyond the scope of this class.</p>

<p><strong>Practical issues: Numeric stability</strong>. When you&#39;re writing code for computing the Softmax function in practice, the intermediate terms \(e^{f_{y_i}}\) and \(\sum_j e^{f_j}\) may be very large due to the exponentials. Dividing large numbers can be numerically unstable, so it is important to use a normalization trick. Notice that if we multiply the top and bottom of the fraction by a constant \(C\) and push it into the sum, we get the following (mathematically equivalent) expression:</p>

<p>\[<br/>
\frac{e^{f_{y_i}}}{\sum_j e^{f_j}}<br/>
= \frac{Ce^{f_{y_i}}}{C\sum_j e^{f_j}}<br/>
= \frac{e^{f_{y_i} + \log C}}{\sum_j e^{f_j + \log C}}<br/>
\]</p>

<p>We are free to choose the value of \(C\). This will not change any of the results, but we can use this value to improve the numerical stability of the computation. A common choice for \(C\) is to set \(\log C = -\max_j f_j \). This simply states that we should shift the values inside the vector \(f\) so that the highest value is zero. In code:</p>

<pre><code class="language-python">f = np.array([123, 456, 789]) # example with 3 classes and each having large scores
p = np.exp(f) / np.sum(np.exp(f)) # Bad: Numeric problem, potential blowup

# instead: first shift the values of f so that the highest number is 0:
f -= np.max(f) # f becomes [-666, -333, 0]
p = np.exp(f) / np.sum(np.exp(f)) # safe to do, gives the correct answer

</code></pre>

<p><strong>Possibly confusing naming conventions</strong>. To be precise, the <em>SVM classifier</em> uses the <em>hinge loss</em>, or also sometimes called the <em>max-margin loss</em>. The <em>Softmax classifier</em> uses the <em>cross-entropy loss</em>. The Softmax classifier gets its name from the <em>softmax function</em>, which is used to squash the raw class scores into normalized positive values that sum to one, so that the cross-entropy loss can be applied. In particular, note that technically it doesn&#39;t make sense to talk about the &quot;softmax loss&quot;, since softmax is just the squashing function, but it is a relatively commonly used shorthand.</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/8/31</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Deep%20Learning.html'>Deep Learning</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
              


			<div class="row">
			  <div class="large-6 columns">
			  <p class="text-left" style="padding-top:25px;">
			   <a href="all_4.html">&laquo; Prev Page</a>  
			  </p>
			  </div>
			  <div class="large-6 columns">
			<p class="text-right" style="padding-top:25px;">
			 <a href="all_6.html">&raquo; Next Page</a> 
			</p>
			  </div>
			</div>
		</div>
	</div><!-- large 8 -->

 <div class="large-4 medium-4 columns">
  <div class="hide-for-small">
    <div id="sidebar" class="sidebar">
          <div id="site-info" class="site-info">
            
                <div class="site-a-logo"><img src="http://or9a8nskt.bkt.clouddn.com/figure.jpeg" /></div>
            
                <h1>techlarry</h1>
                <div class="site-des">他山之石，可以攻玉</div>
                <div class="social">









<a target="_blank" class="github" target="_blank" href="https://github.com/techlarry" title="GitHub">GitHub</a>
<a target="_blank" class="email" href="mailto:wang.zhen.hua.larry@gmail.com" title="Email">Email</a>
  <a target="_blank" class="rss" href="atom.xml" title="RSS">RSS</a>
                
              	 </div>
          	</div>

             

              <div id="site-categories" class="side-item ">
                <div class="side-header">
                  <h2>Categories</h2>
                </div>
                <div class="side-content">

      	<p class="cat-list">
        
            <a href="Leetcode.html"><strong>Leetcode</strong></a>
        
            <a href="C/C++.html"><strong>C/C++</strong></a>
        
            <a href="Python%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95.html"><strong>Python数据结构与算法</strong></a>
        
            <a href="Python%E7%89%B9%E6%80%A7.html"><strong>Python特性</strong></a>
        
            <a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html"><strong>机器学习</strong></a>
        
            <a href="Python%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E4%B8%89%E7%BB%B4%E5%8F%AF%E8%A7%86%E5%8C%96.html"><strong>Python科学计算三维可视化</strong></a>
        
            <a href="English.html"><strong>English</strong></a>
        
            <a href="Computer%20System.html"><strong>Computer System</strong></a>
        
            <a href="Deep%20Learning.html"><strong>Deep Learning</strong></a>
        
            <a href="Latex.html"><strong>Latex</strong></a>
        
            <a href="%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.html"><strong>操作系统</strong></a>
        
            <a href="Linux%20%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B.html"><strong>Linux 系统编程</strong></a>
        
            <a href="%E6%95%B0%E6%8D%AE%E5%BA%93.html"><strong>数据库</strong></a>
        
            <a href="Tensorflow.html"><strong>Tensorflow</strong></a>
        
            <a href="Data%20Science.html"><strong>Data Science</strong></a>
        
            <a href="Big%20Data.html"><strong>Big Data</strong></a>
         
        </p>


                </div>
              </div>

              <div id="site-categories" class="side-item">
                <div class="side-header">
                  <h2>Recent Posts</h2>
                </div>
                <div class="side-content">
                <ul class="posts-list">
	      
		      
			      <li class="post">
			        <a href="15094507460676.html">Introduction to big data</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="Spark_introduction.html">Spark</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="hardware/software_interface.html">Hardware/Software Interface</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15089188725996.html">Linux 内存布局</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15089186263565.html">Valgrind</a>
			      </li>
		     
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		   
		  		</ul>
                </div>
              </div>
        </div><!-- sidebar -->
      </div><!-- hide for small -->
</div><!-- large 4 -->

</div><!-- row -->

 <div class="page-bottom clearfix">
  <div class="row">
   <p class="copyright">Copyright &copy; 2015
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
</div>

        </section>
      </div>
    </div>

  
    

    <script src="asset/js/foundation.min.js"></script>
    <script>
      $(document).foundation();
      function fixSidebarHeight(){
        var w1 = $('.markdown-body').height();
          var w2 = $('#sidebar').height();
          if (w1 > w2) { $('#sidebar').height(w1); };
      }
      $(function(){
        fixSidebarHeight();
      })
      $(window).load(function(){
          fixSidebarHeight();
      });
     
    </script>

    <script src="asset/chart/all-min.js"></script><script type="text/javascript">$(function(){    var mwebii=0;    var mwebChartEleId = 'mweb-chart-ele-';    $('pre>code').each(function(){        mwebii++;        var eleiid = mwebChartEleId+mwebii;        if($(this).hasClass('language-sequence')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = Diagram.parse($(this).text());            diagram.drawSVG(eleiid,{theme: 'simple'});        }else if($(this).hasClass('language-flow')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = flowchart.parse($(this).text());            diagram.drawSVG(eleiid);        }    });});</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>


  </body>
</html>
