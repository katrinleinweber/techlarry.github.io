<!doctype html>
<html class="no-js" lang="en">
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?fdc936c9f5a3b72177541183cdeb8cb3";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  techlarry
  
  </title>
  
  
  <link href="atom.xml" rel="alternate" title="techlarry" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />
    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/github.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
  function before_search(){
    var searchVal = 'site:larryim.cc ' + document.getElementById('search_input').value;
    document.getElementById('search_q').value = searchVal;
    return true;
  }
</script>
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>


  <section class="top-bar-section">
  <div class="row">
      <div style="position: relative;width:100%;"><div style="position: absolute; width:100%;">
        <ul id="main-menu" class="left">
        
        <li id=""><a target="_self" href="index.html">Home</a></li>
        
        <li id=""><a target="_self" href="archives.html">Archives</a></li>
        
        <li id=""><a target="_self" href="about.html">About</a></li>
        
        <li id=""><a target="_self" href="category.html">Category</a></li>
        
        <li id=""><a target="_self" href="notebook.html">NoteBook</a></li>
        
        </ul>

        <ul class="right" id="search-wrap">
          <li>
<form target="_blank" onsubmit="return before_search();" action="http://google.com/search" method="get">
    <input type="hidden" id="search_q" name="q" value="" />
    <input tabindex="1" type="search" id="search_input"  placeholder="Search"/>
</form>
</li>
          </ul>
      </div></div>
  </div>
  </section>

</nav>

        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; techlarry</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
        
        <li><a target="_self" href="index.html">Home</a></li>
        
        <li><a target="_self" href="archives.html">Archives</a></li>
        
        <li><a target="_self" href="about.html">About</a></li>
        
        <li><a target="_self" href="category.html">Category</a></li>
        
        <li><a target="_self" href="notebook.html">NoteBook</a></li>
        

    <li><label>Categories</label></li>

        
            <li><a href="Leetcode.html">Leetcode</a></li>
        
            <li><a href="C/C++.html">C/C++</a></li>
        
            <li><a href="Python%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95.html">Python数据结构与算法</a></li>
        
            <li><a href="Python%E7%89%B9%E6%80%A7.html">Python特性</a></li>
        
            <li><a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html">机器学习</a></li>
        
            <li><a href="Python%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E4%B8%89%E7%BB%B4%E5%8F%AF%E8%A7%86%E5%8C%96.html">Python科学计算三维可视化</a></li>
        
            <li><a href="English.html">English</a></li>
        
            <li><a href="Computer%20System.html">Computer System</a></li>
        
            <li><a href="Deep%20Learning.html">Deep Learning</a></li>
        
            <li><a href="Latex.html">Latex</a></li>
        
            <li><a href="%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.html">操作系统</a></li>
        
            <li><a href="Linux%20%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B.html">Linux 系统编程</a></li>
        
            <li><a href="%E6%95%B0%E6%8D%AE%E5%BA%93.html">数据库</a></li>
        
            <li><a href="Tensorflow.html">Tensorflow</a></li>
        
            <li><a href="Big%20Data.html">Big Data</a></li>
         

      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>


        <section id="main-content" role="main" class="scroll-container">
        
       

 <script type="text/javascript">
	$(function(){
		$('#menu_item_index').addClass('is_active');
	});
</script>
<div class="row">
	<div class="large-8 medium-8 columns">
		<div class="markdown-body home-categories">
		
			<div class="article">
                <a class="clearlink" href="anomaly-detection.html">
                
                  <h1>Machine Learning(9): Anomaly Detection AND Recommender System</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">ML:Anomaly Detection</a>
<ul>
<li>
<a href="#toc_1">Problem Motivation</a>
</li>
<li>
<a href="#toc_2">Gaussian Distribution</a>
</li>
<li>
<a href="#toc_3">Algorithm</a>
</li>
<li>
<a href="#toc_4">Developing and Evaluating an Anomaly Detection System</a>
</li>
<li>
<a href="#toc_5">Anomaly Detection vs. Supervised Learning</a>
</li>
<li>
<a href="#toc_6">Choosing What Features to Use</a>
</li>
<li>
<a href="#toc_7">Multivariate Gaussian Distribution</a>
</li>
<li>
<a href="#toc_8">Anomaly Detection using the Multivariate Gaussian Distribution</a>
</li>
<li>
<a href="#toc_9">ML:Recommender Systems</a>
</li>
<li>
<a href="#toc_10">Problem Formulation</a>
</li>
<li>
<a href="#toc_11">Content Based Recommendations</a>
</li>
<li>
<a href="#toc_12">Collaborative Filtering</a>
</li>
<li>
<a href="#toc_13">Collaborative Filtering Algorithm</a>
</li>
<li>
<a href="#toc_14">Vectorization: Low Rank Matrix Factorization</a>
</li>
<li>
<a href="#toc_15">Implementation Detail: Mean Normalization</a>
</li>
</ul>
</li>
</ul>


<h1 id="toc_0">ML:Anomaly Detection</h1>

<h2 id="toc_1">Problem Motivation</h2>

<p>Just like in other learning problems, we are given a dataset \({x^{(1)}, x^{(2)},\dots,x^{(m)}}\).</p>

<p>We are then given a new example, \(x_{test}\), and we want to know whether this new example is abnormal/anomalous.</p>

<p>We define a &quot;model&quot; p(x) that tells us the probability the example is not anomalous. We also use a threshold ϵ (epsilon) as a dividing line so we can say which examples are anomalous and which are not.</p>

<p>A very common application of anomaly detection is detecting fraud:</p>

<ul>
<li><p>\(x^{(i)} =\) features of user i&#39;s activities</p></li>
<li><p>Model p(x) from the data.</p></li>
<li><p>Identify unusual users by checking which have p(x)&lt;ϵ.</p></li>
</ul>

<p>If our anomaly detector is flagging <strong>too many</strong> anomalous examples, then we need to <strong>decrease</strong> our threshold ϵ</p>

<h2 id="toc_2">Gaussian Distribution</h2>

<p>The Gaussian Distribution is a familiar bell-shaped curve that can be described by a function \(\mathcal{N}(\mu,\sigma^2)\)</p>

<p>Let x∈ℝ. If the probability distribution of x is Gaussian with mean μ, variance \(\sigma^2\), then:</p>

<p>\(x \sim \mathcal{N}(\mu, \sigma^2)\)</p>

<p>The little ∼ or &#39;tilde&#39; can be read as &quot;distributed as.&quot;</p>

<p>The Gaussian Distribution is parameterized by a mean and a variance.</p>

<p>Mu, or μ, describes the center of the curve, called the mean. The width of the curve is described by sigma, or σ, called the standard deviation.</p>

<p>The full function is as follows:</p>

<p>\(\large p(x;\mu,\sigma^2) = \dfrac{1}{\sigma\sqrt{(2\pi)}}e^{-\dfrac{1}{2}(\dfrac{x - \mu}{\sigma})^2}\)</p>

<p>We can estimate the parameter μ from a given dataset by simply taking the average of all the examples:</p>

<p>\(\mu = \dfrac{1}{m}\displaystyle \sum_{i=1}^m x^{(i)}\)</p>

<p>We can estimate the other parameter, \(\sigma^2\), with our familiar squared error formula:</p>

<p>\(\sigma^2 = \dfrac{1}{m}\displaystyle \sum_{i=1}^m(x^{(i)} - \mu)^2\)</p>

<p><img src="media/15045011015887/15045018120355.png" alt="Gaussian distribution"/></p>

<h2 id="toc_3">Algorithm</h2>

<p>Given a training set of examples, \(\lbrace x^{(1)},\dots,x^{(m)}\rbrace\) where each example is a vector, \(x \in \mathbb{R}^n\).</p>

<p>\(p(x) = p(x_1;\mu_1,\sigma_1^2)p(x_2;\mu_2,\sigma^2_2)\cdots p(x_n;\mu_n,\sigma^2_n)\)</p>

<p>In statistics, this is called an &quot;independence assumption&quot; on the values of the features inside training example x.</p>

<p>More compactly, the above expression can be written as follows:</p>

<p>\(= \displaystyle \prod^n_{j=1} p(x_j;\mu_j,\sigma_j^2)\)</p>

<p><strong>The algorithm</strong></p>

<p>Choose features \(x_i\) that you think might be indicative of anomalous examples.</p>

<p>Fit parameters \(\mu_1,\dots,\mu_n,\sigma_1^2,\dots,\sigma_n^2\)</p>

<p>Calculate \(\mu_j = \dfrac{1}{m}\displaystyle \sum_{i=1}^m x_j^{(i)}\)</p>

<p>Calculate \(\sigma^2_j = \dfrac{1}{m}\displaystyle \sum_{i=1}^m(x_j^{(i)} - \mu_j)^2\)</p>

<p>Given a new example x, compute p(x):</p>

<p>\(p(x) = \displaystyle \prod^n_{j=1} p(x_j;\mu_j,\sigma_j^2) = \prod\limits^n_{j=1} \dfrac{1}{\sqrt{2\pi}\sigma_j}exp(-\dfrac{(x_j - \mu_j)^2}{2\sigma^2_j})\)</p>

<p>Anomaly if p(x)&lt;ϵ</p>

<p>A vectorized version of the calculation for μ is \(\mu = \dfrac{1}{m}\displaystyle \sum_{i=1}^m x^{(i)}\). You can vectorize \(\sigma^2\) similarly.</p>

<h2 id="toc_4">Developing and Evaluating an Anomaly Detection System</h2>

<p>To evaluate our learning algorithm, we take some labeled data, categorized into anomalous and non-anomalous examples ( y = 0 if normal, y = 1 if anomalous).</p>

<p>Among that data, take a large proportion of <strong>good</strong> , non-anomalous data for the training set on which to train p(x).</p>

<p>Then, take a smaller proportion of mixed anomalous and non-anomalous examples (you will usually have many more non-anomalous examples) for your cross-validation and test sets.</p>

<p>For example, we may have a set where 0.2% of the data is anomalous. We take 60% of those examples, all of which are good (y=0) for the training set. We then take 20% of the examples for the cross-validation set (with 0.1% of the anomalous examples) and another 20% from the test set (with another 0.1% of the anomalous).</p>

<p>In other words, we split the data 60/20/20 training/CV/test and then split the anomalous examples 50/50 between the CV and test sets.</p>

<p><strong>Algorithm evaluation:</strong></p>

<p>Fit model p(x) on training set \(\lbrace x^{(1)},\dots,x^{(m)} \rbrace\)</p>

<p>On a cross validation/test example x, predict:</p>

<p>If p(x) &lt; ϵ ( <strong>anomaly</strong> ), then y=1</p>

<p>If p(x) ≥ ϵ ( <strong>normal</strong> ), then y=0</p>

<p>Possible evaluation metrics (see &quot;Machine Learning System Design&quot; section):</p>

<ul>
<li><p>True positive, false positive, false negative, true negative.</p></li>
<li><p>Precision/recall</p></li>
<li><p>\(F_1\) score</p></li>
</ul>

<p>Note that we use the cross-validation set to choose parameter ϵ</p>

<h2 id="toc_5">Anomaly Detection vs. Supervised Learning</h2>

<p>When do we use anomaly detection and when do we use supervised learning?</p>

<p>Use anomaly detection when...</p>

<ul>
<li><p>We have a very small number of positive examples (y=1 ... 0-20 examples is common) and a large number of negative (y=0) examples.</p></li>
<li><p>We have many different &quot;types&quot; of anomalies and it is hard for any algorithm to learn from positive examples what the anomalies look like; future anomalies may look nothing like any of the anomalous examples we&#39;ve seen so far.</p></li>
</ul>

<p>Use supervised learning when...</p>

<ul>
<li><p>We have a large number of both positive and negative examples. In other words, the training set is more evenly divided into classes.</p></li>
<li><p>We have enough positive examples for the algorithm to get a sense of what new positives examples look like. The future positive examples are likely similar to the ones in the training set.</p></li>
</ul>

<h2 id="toc_6">Choosing What Features to Use</h2>

<p>The features will greatly affect how well your anomaly detection algorithm works.</p>

<p>We can check that our features are <strong>gaussian</strong> by plotting a histogram of our data and checking for the bell-shaped curve.</p>

<p>Some <strong>transforms</strong> we can try on an example feature x that does not have the bell-shaped curve are:</p>

<ul>
<li><p>log(x)</p></li>
<li><p>log(x+1)</p></li>
<li><p>log(x+c) for some constant</p></li>
<li><p>\(\sqrt{x}\)</p></li>
<li><p>\(x^{1/3}\)</p></li>
</ul>

<p>We can play with each of these to try and achieve the gaussian shape in our data.</p>

<p>There is an <strong>error analysis procedure</strong> for anomaly detection that is very similar to the one in supervised learning.</p>

<p>Our goal is for p(x) to be large for normal examples and small for anomalous examples.</p>

<p>One common problem is when p(x) is similar for both types of examples. In this case, you need to examine the anomalous examples that are giving high probability in detail and try to figure out new features that will better distinguish the data.</p>

<p>In general, choose features that might take on unusually large or small values in the event of an anomaly.</p>

<h2 id="toc_7">Multivariate Gaussian Distribution</h2>

<p>The multivariate gaussian distribution is an extension of anomaly detection and may (or may not) catch more anomalies.</p>

<p>Instead of modeling \(p(x_1),p(x_2),\dots\) separately, we will model p(x) all in one go. Our parameters will be: \(\mu \in \mathbb{R}^n\) and \(\Sigma \in \mathbb{R}^{n \times n}\)</p>

<p>\(p(x;\mu,\Sigma) = \dfrac{1}{(2\pi)^{n/2} |\Sigma|^{1/2}} exp(-1/2(x-\mu)^T\Sigma^{-1}(x-\mu))\)</p>

<p>The important effect is that we can model oblong gaussian contours, allowing us to better fit data that might not fit into the normal circular contours.</p>

<p>Varying Σ changes the shape, width, and orientation of the contours. Changing μ will move the center of the distribution.</p>

<p>Check also:</p>

<ul>
<li>  <a href="http://cs229.stanford.edu/section/gaussians.pdf">The Multivariate Gaussian Distribution</a> <a href="http://cs229.stanford.edu/section/gaussians.pdf">http://cs229.stanford.edu/section/gaussians.pdf</a> Chuong B. Do, October 10, 2008.</li>
</ul>

<p>Following examples illustrate the basic meaning of parameters in multivariable gaussian distribution:</p>

<pre><code>mean = [0, 0]
cov = [[1, 0], [0, 1]]  # diagonal covariance
# Draw random samples from a multivariate normal distribution
x, y = np.random.multivariate_normal(mean, cov, 5000).T
plt.plot(x, y, &#39;x&#39;,color=&#39;y&#39;)
plt.axis(&#39;equal&#39;)
plt.hold

# change mean
mean = [0, 10]
x, y = np.random.multivariate_normal(mean, cov, 5000).T
plt.plot(x, y, &#39;.&#39;, color=&#39;b&#39;)

# change variances
mean = [10, 10]
cov = [[1, 0], [0, 10]]  # diagonal covariance
x, y = np.random.multivariate_normal(mean, cov, 5000).T
plt.plot(x, y, &#39;-&#39;, color=&#39;r&#39;)
plt.show()
</code></pre>

<p><img src="media/15045011015887/15045091983941.jpg" alt="demo of multivarible gaussian"/></p>

<h2 id="toc_8">Anomaly Detection using the Multivariate Gaussian Distribution</h2>

<p>When doing anomaly detection with multivariate gaussian distribution, we compute μ and Σ normally. We then compute p(x) using the new formula in the previous section and flag an anomaly if p(x) &lt; ϵ.</p>

<p>The original model for p(x) corresponds to a multivariate Gaussian where the contours of \(p(x;\mu,\Sigma)\) are axis-aligned.</p>

<p>The multivariate Gaussian model can automatically capture correlations between different features of x.</p>

<p>However, the original model maintains some advantages: it is computationally cheaper (no matrix to invert, which is costly for large number of features) and it performs well even with small training set size (in multivariate Gaussian model, it should be greater than the number of features for Σ to be invertible).</p>

<h2 id="toc_9">ML:Recommender Systems</h2>

<h2 id="toc_10">Problem Formulation</h2>

<p>Recommendation is currently a very popular application of machine learning.</p>

<p>Say we are trying to recommend movies to customers. We can use the following definitions</p>

<ul>
<li><p>\(n_u =\) number of users</p></li>
<li><p>\(n_m =\) number of movies</p></li>
<li><p>\(r(i,j) = 1\) if user j has rated movie i</p></li>
<li><p>\(y(i,j) =\) rating given by user j to movie i (defined only if r(i,j)=1)</p></li>
</ul>

<h2 id="toc_11">Content Based Recommendations</h2>

<p>We can introduce two features, \(x_1\) and \(x_2\) which represents how much romance or how much action a movie may have (on a scale of 0−1).</p>

<p>One approach is that we could do linear regression for every single user. For each user j, learn a parameter \(\theta^{(j)} \in \mathbb{R}^3\). Predict user j as rating movie i with \((\theta^{(j)})^Tx^{(i)}\) stars.</p>

<ul>
<li><p>\(\theta^{(j)} =\) parameter vector for user j</p></li>
<li><p>\(x^{(i)} =\) feature vector for movie i</p></li>
</ul>

<p>For user j, movie i, predicted rating: \((\theta^{(j)})^T(x^{(i)})\)</p>

<ul>
<li>  \(m^{(j)} =\) number of movies rated by user j</li>
</ul>

<p>To learn \(\theta^{(j)}\), we do the following</p>

<p>\(min_{\theta^{(j)}} = \dfrac{1}{2}\displaystyle \sum_{i:r(i,j)=1} ((\theta^{(j)})^T(x^{(i)}) - y^{(i,j)})^2 + \dfrac{\lambda}{2} \sum_{k=1}^n(\theta_k^{(j)})^2\)</p>

<p>This is our familiar linear regression. The base of the first summation is choosing all i such that \(r(i,j) = 1\).</p>

<p>To get the parameters for all our users, we do the following:</p>

<p>\(min_{\theta^{(1)},\dots,\theta^{(n_u)}} = \dfrac{1}{2}\displaystyle \sum_{j=1}^{n_u} \sum_{i:r(i,j)=1} ((\theta^{(j)})^T(x^{(i)}) - y^{(i,j)})^2 + \dfrac{\lambda}{2} \sum_{j=1}^{n_u} \sum_{k=1}^n(\theta_k^{(j)})^2\)</p>

<p>We can apply our linear regression gradient descent update using the above cost function.</p>

<p>The only real difference is that we <strong>eliminate the constant</strong> \(\dfrac{1}{m}\).</p>

<h2 id="toc_12">Collaborative Filtering</h2>

<p>It can be very difficult to find features such as &quot;amount of romance&quot; or &quot;amount of action&quot; in a movie. To figure this out, we can use <u>feature finders</u> .</p>

<p>We can let the users tell us how much they like the different genres, providing their parameter vector immediately for us.</p>

<p>To infer the features from given parameters, we use the squared error function with regularization over all the users:</p>

<p>\(min_{x^{(1)},\dots,x^{(n_m)}} \dfrac{1}{2} \displaystyle \sum_{i=1}^{n_m} \sum_{j:r(i,j)=1} ((\theta^{(j)})^T x^{(i)} - y^{(i,j)})^2 + \dfrac{\lambda}{2}\sum_{i=1}^{n_m} \sum_{k=1}^{n} (x_k^{(i)})^2\)</p>

<p>You can also <strong>randomly guess</strong> the values for theta to guess the features repeatedly. You will actually converge to a good set of features.</p>

<h2 id="toc_13">Collaborative Filtering Algorithm</h2>

<p>To speed things up, we can simultaneously minimize our features and our parameters:</p>

<p>\(J(x,\theta) = \dfrac{1}{2} \displaystyle \sum_{(i,j):r(i,j)=1}((\theta^{(j)})^Tx^{(i)} - y^{(i,j)})^2 + \dfrac{\lambda}{2}\sum_{i=1}^{n_m} \sum_{k=1}^{n} (x_k^{(i)})^2 + \dfrac{\lambda}{2}\sum_{j=1}^{n_u} \sum_{k=1}^{n} (\theta_k^{(j)})^2\)</p>

<p>It looks very complicated, but we&#39;ve only combined the cost function for theta and the cost function for x.</p>

<p>Because the algorithm can learn them itself, the bias units where x0=1 have been removed, therefore x∈ℝn and θ∈ℝn.</p>

<p>These are the steps in the algorithm:</p>

<ol>
<li><p>Initialize \(x^{(i)},...,x^{(n_m)},\theta^{(1)},...,\theta^{(n_u)}\) to small random values. This serves to break symmetry and ensures that the algorithm learns features \(x^{(i)},...,x^{(n_m)}\) that are different from each other.</p></li>
<li><p>Minimize \(J(x^{(i)},...,x^{(n_m)},\theta^{(1)},...,\theta^{(n_u)})\) using gradient descent (or an advanced optimization algorithm).E.g. for every \(j=1,...,n_u,i=1,...n_m\):\(x_k^{(i)} := x_k^{(i)} - \alpha\left (\displaystyle \sum_{j:r(i,j)=1}{((\theta^{(j)})^T x^{(i)} - y^{(i,j)}) \theta_k^{(j)}} + \lambda x_k^{(i)} \right)\)\(\theta_k^{(j)} := \theta_k^{(j)} - \alpha\left (\displaystyle \sum_{i:r(i,j)=1}{((\theta^{(j)})^T x^{(i)} - y^{(i,j)}) x_k^{(i)}} + \lambda \theta_k^{(j)} \right)\)</p></li>
<li><p>For a user with parameters θ and a movie with (learned) features x, predict a star rating of \(\theta^Tx\).</p></li>
</ol>

<h2 id="toc_14">Vectorization: Low Rank Matrix Factorization</h2>

<p>Given matrices X (each row containing features of a particular movie) and Θ (each row containing the weights for those features for a given user), then the full matrix Y of all predicted ratings of all movies by all users is given simply by: \(Y = X\Theta^T\).</p>

<p>Predicting how similar two movies i and j are can be done using the distance between their respective feature vectors x. Specifically, we are looking for a small value of \(||x^{(i)} - x^{(j)}||\).</p>

<h2 id="toc_15">Implementation Detail: Mean Normalization</h2>

<p>If the ranking system for movies is used from the previous lectures, then new users (who have watched no movies), will be assigned new movies incorrectly. Specifically, they will be assigned θ with all components equal to zero due to the minimization of the regularization term. That is, we assume that the new user will rank all movies 0, which does not seem intuitively correct.</p>

<p>We rectify this problem by normalizing the data relative to the mean. First, we use a matrix Y to store the data from previous ratings, where the ith row of Y is the ratings for the ith movie and the jth column corresponds to the ratings for the jth user.</p>

<p>We can now define a vector</p>

<p>\(\mu = [\mu_1, \mu_2, \dots , \mu_{n_m}]\)</p>

<p>such that</p>

<p>\(\mu_i = \frac{\sum_{j:r(i,j)=1}{Y_{i,j}}}{\sum_{j}{r(i,j)}}\)</p>

<p>Which is effectively the mean of the previous ratings for the ith movie (where only movies that have been watched by users are counted). We now can normalize the data by subtracting u, the mean rating, from the actual ratings for each user (column in matrix Y):</p>

<p>As an example, consider the following matrix Y and mean ratings μ:</p>

<p>\(Y = \begin{bmatrix} 5 &amp; 5 &amp; 0 &amp; 0 \newline 4 &amp; ? &amp; ? &amp; 0 \newline 0 &amp; 0 &amp; 5 &amp; 4 \newline 0 &amp; 0 &amp; 5 &amp; 0 \newline \end{bmatrix}, \quad \mu = \begin{bmatrix} 2.5 \newline 2 \newline 2.25 \newline 1.25 \newline \end{bmatrix}\)</p>

<p>The resulting Y′ vector is:</p>

<p>\(Y&#39; = \begin{bmatrix} 2.5 &amp; 2.5 &amp; -2.5 &amp; -2.5 \newline 2 &amp; ? &amp; ? &amp; -2 \newline -.2.25 &amp; -2.25 &amp; 3.75 &amp; 1.25 \newline -1.25 &amp; -1.25 &amp; 3.75 &amp; -1.25 \end{bmatrix}\)</p>

<p>Now we must slightly modify the linear regression prediction to include the mean normalization term:</p>

<p>\((\theta^{(j)})^T x^{(i)} + \mu_i\)</p>

<p>Now, for a new user, the initial predicted values will be equal to the μ term instead of simply being initialized to zero, which is more accurate.</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/9/9</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html'>机器学习</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="unsupervised_learning.html">
                
                  <h1>Machine Learning(8): Unsupervised Learning</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<h2 id="toc_0">K-means Clustering</h2>

<p>The K-Means Algorithm is the most popular and widely used algorithm for automatically grouping data into coherent subsets.</p>

<ol>
<li><p>Randomly initialize two points in the dataset called the <u>cluster centroids</u> .</p></li>
<li><p><u>Cluster assignment</u>: assign all examples into one of two groups based on which cluster centroid the example is closest to.</p></li>
<li><p><u>Move centroid</u>: compute the averages for all the points inside each of the two cluster centroid groups, then move the cluster centroid points to those averages.</p></li>
<li><p>Re-run (2) and (3) until we have found our clusters.</p></li>
</ol>

<p>Main variables are:</p>

<ul>
<li><p>K (number of clusters)</p></li>
<li><p>Training set \({x^{(1)}, x^{(2)}, \dots,x^{(m)}}\)</p></li>
<li><p>Where \(x^{(i)} \in \mathbb{R}^n\)</p></li>
</ul>

<p><strong>The algorithm:</strong></p>

<pre><code>Randomly initialize K cluster centroids mu(1), mu(2), ..., mu(K)
Repeat:
   for i = 1 to m:
      c(i):= index (from 1 to K) of cluster centroid closest to x(i)
   for k = 1 to K:
      mu(k):= average (mean) of points assigned to cluster k
</code></pre>

<h3 id="toc_1">Explaination</h3>

<p>The <strong>first for-loop</strong> is the <u>Cluster Assignment</u> step. We make a vector <u>c</u> where <u>c(i)</u> represents the centroid assigned to example <u>x(i)</u> .</p>

<p>We can write the operation of the Cluster Assignment step more mathematically as follows:</p>

<p>\(c^{(i)} = \arg \min_k\ ||x^{(i)} - \mu_k||^2\)</p>

<p>That is, each \(c^{(i)}\) contains the index of the centroid that has minimal distance to \(x^{(i)}\).</p>

<p>By convention, we square the right-hand-side, which makes the function we are trying to minimize more sharply increasing. It is mostly just a convention. But a convention that helps reduce the computation load because the Euclidean distance requires a square root but it is canceled.</p>

<p>Without the square:</p>

<p>\(||x^{(i)} - \mu_k|| = ||\quad\sqrt{(x_1^i - \mu_{1(k)})^2 + (x_2^i - \mu_{2(k)})^2 + (x_3^i - \mu_{3(k)})^2 + ...}\quad||\)</p>

<p>With the square:</p>

<p>\(||x^{(i)} - \mu_k||^2 = ||\quad(x_1^i - \mu_{1(k)})^2 + (x_2^i - \mu_{2(k)})^2 + (x_3^i - \mu_{3(k)})^2 + ...\quad||\)</p>

<p>...so the square convention serves two purposes, minimize more sharply and less computation.</p>

<p>The <strong>second for-loop</strong> is the &#39;Move Centroid&#39; step where we move each centroid to the average of its group.</p>

<p>More formally, the equation for this loop is as follows:</p>

<p>\(\mu_k = \dfrac{1}{n}[x^{(k_1)} + x^{(k_2)} + \dots + x^{(k_n)}] \in \mathbb{R}^n\)</p>

<p>Where each of \(x^{(k_1)}, x^{(k_2)}, \dots, x^{(k_n)}\) are the training examples assigned to group \(mμ_k\).</p>

<p>If you have a cluster centroid with <strong>0 points</strong> assigned to it, you can randomly <strong>re-initialize</strong> that centroid to a new point. You can also simply <strong>eliminate</strong> that cluster group.</p>

<p>After a number of iterations the algorithm will <u><strong>converge</strong></u> , where new iterations do not affect the clusters.</p>

<p>Note on non-separated clusters: some datasets have no real inner separation or natural structure. K-means can still evenly segment your data into K subsets, so can still be useful in this case.</p>

<h3 id="toc_2">Choosing the Number of Clusters</h3>

<p>Choosing K can be quite arbitrary and ambiguous.</p>

<p><strong>The elbow method</strong>: plot the cost J and the number of clusters K. The cost function should reduce as we increase the number of clusters, and then flatten out. Choose K at the point where the cost function starts to flatten out.</p>

<p>However, fairly often, the curve is <strong>very gradual</strong> , so there&#39;s no clear elbow.</p>

<p><strong>Note:</strong> J will <strong>always</strong> decrease as K is increased. The one exception is if k-means gets stuck at a bad local optimum.</p>

<p>Another way to choose K is to observe how well k-means performs on a <strong>downstream purpose</strong> . In other words, you choose K that proves to be most useful for some goal you&#39;re trying to achieve from using these clusters.</p>

<p><img src="media/15101207367271/elbow%20method.png" alt="elbow method"/></p>

<h2 id="toc_3">Principal Component Analysis</h2>

<p>The most popular dimensionality reduction algorithm is <u>Principal Component Analysis</u> (PCA)</p>

<p>Before applying PCA, there is a data pre-processing step we must perform:</p>

<p><strong>Data preprocessing</strong></p>

<ul>
<li>  Given training set: \(x(1),x(2),…,x(m)\)</li>
<li><p>Preprocess (feature scaling/mean normalization): \(\mu_j = \dfrac{1}{m}\sum^m_{i=1}x_j^{(i)}\)</p></li>
<li><p>Replace each \(x_j^{(i)}\) with \(x_j^{(i)} - \mu_j\)</p></li>
<li><p>If different features on different scales (e.g., \(x_1\) = size of house, \(x_2\) = number of bedrooms), scale features to have comparable range of values.</p></li>
</ul>

<p>Above, we first subtract the mean of each feature from the original feature. Then we scale all the features \(x_j^{(i)} = \dfrac{x_j^{(i)} - \mu_j}{s_j}\).</p>

<p>We can define specifically what it means to reduce from 2d to 1d data as follows:</p>

<p>\(\Sigma = \dfrac{1}{m}\sum^m_{i=1}(x^{(i)})(x^{(i)})^T\)</p>

<p>The z values are all real numbers and are the projections of our features onto \(u^{(1)}\).</p>

<p>So, PCA has two tasks: figure out \(u^{(1)},\dots,u^{(k)}\) and also to find \(z_1, z_2, \dots, z_m\).</p>

<p>The mathematical proof for the following procedure is complicated and beyond the scope of this course.</p>

<p><strong>1. Compute &quot;covariance matrix&quot;</strong></p>

<p>\(\Sigma = \dfrac{1}{m}\sum^m_{i=1}(x^{(i)})(x^{(i)})^T\)</p>

<p>This can be vectorized in Octave as:</p>

<p>\[Sigma = (1/m) * X&#39; * X;\]</p>

<p>We denote the covariance matrix with a capital sigma (which happens to be the same symbol for summation, confusingly---they represent entirely different things).</p>

<p>Note that \(x^{(i)}\) is an n×1 vector, \((x^{(i)})^T\) is an 1×n vector and X is a m×n matrix (row-wise stored examples). The product of those will be an n×n matrix, which are the dimensions of Σ.</p>

<p><strong>2. Compute &quot;eigenvectors&quot; of covariance matrix Σ</strong></p>

<pre><code class="language-octave">[U,S,V] = svd(Sigma);
</code></pre>

<p>svd() is the &#39;singular value decomposition&#39;, a built-in Octave function.</p>

<p>What we actually want out of svd() is the &#39;U&#39; matrix of the Sigma covariance matrix: \(U \in \mathbb{R}^{n \times n}\). U contains \(u^{(1)},\dots,u^{(n)}\), which is exactly what we want.</p>

<p><strong>3. Take the first k columns of the U matrix and compute z</strong></p>

<p>We&#39;ll assign the first k columns of U to a variable called <code>Ureduce</code>. This will be an n×k matrix. We compute z with:</p>

<p>\(z^{(i)} = \text{Ureduce}^T \cdot x^{(i)}\)</p>

<p>\(\text{Ureduce}Z^T\) will have dimensions k×n while x(i) will have dimensions n×1. The product \(\text{Ureduce}^T \cdot x^{(i)}\) will have dimensions k×1.</p>

<p>To summarize, the whole algorithm in <code>octave</code> is roughly:</p>

<pre><code class="language-octave">Sigma = (1/m) * X&#39; * X; % compute the covariance matrix
[U,S,V] = svd(Sigma);   % compute our projected directions
Ureduce = U(:,1:k);     % take the first k directions
Z = X * Ureduce;        % compute the projected data points
</code></pre>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/9/8</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html'>机器学习</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="batch_normalization.html">
                
                  <h1>Batch Normalization</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>The idea is relatively straightforward. Machine learning methods tend to work better when their input data consists of uncorrelated features with <strong>zero mean</strong> and <strong>unit variance</strong>. When training a neural network, we can preprocess the data before feeding it to the network to explicitly decorrelate its features; this will ensure that the first layer of the network sees data that follows a nice distribution. However even if we preprocess the input data, the activations at deeper layers of the network will likely no longer be decorrelated and will no longer have zero mean or unit variance since they are output from earlier layers in the network. Even worse, during the training process the distribution of features at each layer of the network will shift as the weights of each layer are updated.</p>

<h2 id="toc_0">Covariate Shift</h2>

<p>While stochastic gradient is simple and effective, it requires careful tuning of the model hyper-parameters, specifically the learning rate used in optimization, as well as the initial values for the model parameters. The train- ing is complicated by the fact that the inputs to each layer are affected by the parameters of all preceding layers – so that small changes to the network parameters amplify as the network becomes deeper.</p>

<p>The change in the distribution of layers&#39; inputs presents a problem because the layers need to continuously adapt to the new distribution. When the input distribution to a learning system changes, it is said to experience <code>covariate shift</code>. This is typically handled via domain adaption.</p>

<p><code>Internal Covariate Shift</code> refers to the change in the distribution of internal nodes of a deep network due to change in network parameters, in the course of training. <code>Batch Normalization</code>, that takes a step towards reducing internal covariance shift, and in doing so dramatically accelerates the training of deep neural nets. It accomplishes this via a normalization step that fixes the means and variances of layer inputs. It also has a beneficial effect on the dependence of gradients on the scale of the parameters or of their initial values. This allows us to use much higher learning rates without the risk of divergence. </p>

<h2 id="toc_1">Algorithm</h2>

<h3 id="toc_2">Forward</h3>

<p>Given some input values \(\mathcal{B} = x^{(1)},...,x^{(m)}\) over a mini-batch in the layer \(l\) of neural network; </p>

<p>\[\mu_{\mathcal{B}} = \frac{1}{m}\sum_i^m x_i  \text{     (mini-batch mean)}\\<br/>
\sigma^2=\frac{1}{m}\sum_i(x_i-\mu_{\mathcal{B}})^2  \text{     (mini-batch variance)}\\<br/>
\hat{x}_i=\frac{x_i-\mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{\mathcal{B}}}^2+\varepsilon}} \text{     (normalize)}\\<br/>
y_i=\gamma \hat{x}_i+\beta \text{     (scale and shift)}<br/>
\]</p>

<p>At each iteration, we update the running averages for mean and variance using an exponential decay based on the momentum parameter:<br/>
<code><br/>
running_mean = momentum * running_mean + (1 - momentum) * sample_mean<br/>
running_var = momentum * running_var + (1 - momentum) * sample_var<br/>
</code></p>

<h2 id="toc_3">Backward</h2>

<p><img src="media/15048405877529/Screen%20Shot%202017-09-08%20at%203.11.45%20PM.png" alt="batch-normalization-backward"/></p>

<h3 id="toc_4">Test time</h3>

<p>Using trained <code>runing_mean</code> and <code>running_var</code> to take forward step.</p>

<h2 id="toc_5">TensorFlow</h2>

<p>Applying <code>Batch Normalization</code> in TensorFlow Model is very convenient. Just add one line of code in TensorFlow: <code>tf.nn.batch-normalization</code>.</p>

<h2 id="toc_6">Reference</h2>

<p>Sergey Ioffe, Christian Szegedy. 2015. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. arXiv:<a href="https://arxiv.org/abs/1502.03167v3">1501.02167v3</a></p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/9/8</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Deep%20Learning.html'>Deep Learning</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="15046658203135.html">
                
                  <h1>TensorFlow Q</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<h3 id="toc_0">Q1: what&#39;s the difference between <code>tf.placeholder</code> and <code>tf.Variable</code></h3>

<p>In general, we use <code>tf.placeholder</code> to feed actual training examples, while using <code>tf.Variable</code> for parameters such as weights (\(W\)) and biases (\(b\)) for models.</p>

<p>With <code>tf.Variable</code>, we have to provide an initial value when declaring it. And we don&#39;t have to provide an initial value until running time with a <code>feed_dict</code>.</p>

<h3 id="toc_1">Q2: what&#39;s the difference between <code>tf.random_normal</code> and <code>tf.trucated_normal</code></h3>

<p><code>tf.trucated_normal</code> generates values following a normal distribution with specified mean and standard deviation, except that values whose magnitude is more than 2 standard deviations from the mean are dropped and re-picked.</p>

<pre><code>import tensorflow as tf
import matplotlib.pyplot as plt
%matplotlib inline
Session = tf.InteractiveSession()
</code></pre>

<pre><code>mean = 0.0
std = 1.0
shape = (10000,)
hist_range = (-5, 5)

A = tf.truncated_normal(shape, mean, std)
B = tf.random_normal(shape, mean, std)
a, b = Session.run([A, B])
</code></pre>

<pre><code>f, (ax1, ax2) = plt.subplots(2,1)
f1 = ax1.hist(a, bins= 100, range=hist_range)
f2 = ax2.hist(b, bins= 100, range=hist_range)
</code></pre>

<p><img src="media/15046658203135/output_2_0.png" alt="output_2_0"/></p>

<h3 id="toc_2">Q3: What&#39;s the difference between <code>tf.Variable</code> and <code>tf.get_variable</code></h3>

<p><code>tf.get_variable</code> will make it way easier to refactor code if you need to share variables at any time.</p>

<pre><code class="language-python">import tensorflow as tf

with tf.variable_scope(&quot;scope1&quot;):
    w1 = tf.get_variable(&quot;w1&quot;, shape=[2,3])
    w2 = tf.Variable(1.0, name=&quot;w2&quot;)
with tf.variable_scope(&quot;scope1&quot;, reuse=True):
    w1_p = tf.get_variable(&quot;w1&quot;, shape=[2,3])
    w2_p = tf.Variable(1.0, name=&quot;w2&quot;)

print(w1 is w1_p, w2 is w2_p)

#Output: True  False
</code></pre>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/9/6</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Tensorflow.html'>Tensorflow</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="15046649572570.html">
                
                  <h1>Pandas</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>To convert a pandas <code>dataframe</code> (df) to a numpy <code>ndarray</code>, use this code:</p>

<pre><code>df=df.values
</code></pre>

<p>df now becomes a numpy <code>ndarray</code>.</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/9/6</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Python%E7%89%B9%E6%80%A7.html'>Python特性</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
              


			<div class="row">
			  <div class="large-6 columns">
			  <p class="text-left" style="padding-top:25px;">
			   <a href="all_3.html">&laquo; Prev Page</a>  
			  </p>
			  </div>
			  <div class="large-6 columns">
			<p class="text-right" style="padding-top:25px;">
			 <a href="all_5.html">&raquo; Next Page</a> 
			</p>
			  </div>
			</div>
		</div>
	</div><!-- large 8 -->

 <div class="large-4 medium-4 columns">
  <div class="hide-for-small">
    <div id="sidebar" class="sidebar">
          <div id="site-info" class="site-info">
            
                <div class="site-a-logo"><img src="http://or9a8nskt.bkt.clouddn.com/figure.jpeg" /></div>
            
                <h1>techlarry</h1>
                <div class="site-des">他山之石，可以攻玉</div>
                <div class="social">









<a target="_blank" class="github" target="_blank" href="https://github.com/techlarry" title="GitHub">GitHub</a>
<a target="_blank" class="email" href="mailto:wang.zhen.hua.larry@gmail.com" title="Email">Email</a>
  <a target="_blank" class="rss" href="atom.xml" title="RSS">RSS</a>
                
              	 </div>
          	</div>

             

              <div id="site-categories" class="side-item ">
                <div class="side-header">
                  <h2>Categories</h2>
                </div>
                <div class="side-content">

      	<p class="cat-list">
        
            <a href="Leetcode.html"><strong>Leetcode</strong></a>
        
            <a href="C/C++.html"><strong>C/C++</strong></a>
        
            <a href="Python%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95.html"><strong>Python数据结构与算法</strong></a>
        
            <a href="Python%E7%89%B9%E6%80%A7.html"><strong>Python特性</strong></a>
        
            <a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html"><strong>机器学习</strong></a>
        
            <a href="Python%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E4%B8%89%E7%BB%B4%E5%8F%AF%E8%A7%86%E5%8C%96.html"><strong>Python科学计算三维可视化</strong></a>
        
            <a href="English.html"><strong>English</strong></a>
        
            <a href="Computer%20System.html"><strong>Computer System</strong></a>
        
            <a href="Deep%20Learning.html"><strong>Deep Learning</strong></a>
        
            <a href="Latex.html"><strong>Latex</strong></a>
        
            <a href="%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.html"><strong>操作系统</strong></a>
        
            <a href="Linux%20%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B.html"><strong>Linux 系统编程</strong></a>
        
            <a href="%E6%95%B0%E6%8D%AE%E5%BA%93.html"><strong>数据库</strong></a>
        
            <a href="Tensorflow.html"><strong>Tensorflow</strong></a>
        
            <a href="Big%20Data.html"><strong>Big Data</strong></a>
         
        </p>


                </div>
              </div>

              <div id="site-categories" class="side-item">
                <div class="side-header">
                  <h2>Recent Posts</h2>
                </div>
                <div class="side-content">
                <ul class="posts-list">
	      
		      
			      <li class="post">
			        <a href="introdution_to_big_data.html">Introduction to big data</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="Spark_introduction.html">Spark</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="hardware/software_interface.html">Hardware/Software Interface</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15089188725996.html">Linux 内存布局</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="Valgrind.html">Valgrind</a>
			      </li>
		     
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		   
		  		</ul>
                </div>
              </div>
        </div><!-- sidebar -->
      </div><!-- hide for small -->
</div><!-- large 4 -->

</div><!-- row -->

 <div class="page-bottom clearfix">
  <div class="row">
   <p class="copyright">Copyright &copy; 2015
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
</div>

        </section>
      </div>
    </div>

  
    

    <script src="asset/js/foundation.min.js"></script>
    <script>
      $(document).foundation();
      function fixSidebarHeight(){
        var w1 = $('.markdown-body').height();
          var w2 = $('#sidebar').height();
          if (w1 > w2) { $('#sidebar').height(w1); };
      }
      $(function(){
        fixSidebarHeight();
      })
      $(window).load(function(){
          fixSidebarHeight();
      });
     
    </script>

    <script src="asset/chart/all-min.js"></script><script type="text/javascript">$(function(){    var mwebii=0;    var mwebChartEleId = 'mweb-chart-ele-';    $('pre>code').each(function(){        mwebii++;        var eleiid = mwebChartEleId+mwebii;        if($(this).hasClass('language-sequence')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = Diagram.parse($(this).text());            diagram.drawSVG(eleiid,{theme: 'simple'});        }else if($(this).hasClass('language-flow')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = flowchart.parse($(this).text());            diagram.drawSVG(eleiid);        }    });});</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>


  </body>
</html>
