<!doctype html>
<html class="no-js" lang="en">
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?fdc936c9f5a3b72177541183cdeb8cb3";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  techlarry
  
  </title>
  
  
  <link href="atom.xml" rel="alternate" title="techlarry" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />
    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/github.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
  function before_search(){
    var searchVal = 'site:larryim.cc ' + document.getElementById('search_input').value;
    document.getElementById('search_q').value = searchVal;
    return true;
  }
</script>
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>


  <section class="top-bar-section">
  <div class="row">
      <div style="position: relative;width:100%;"><div style="position: absolute; width:100%;">
        <ul id="main-menu" class="left">
        
        <li id=""><a target="_self" href="index.html">Home</a></li>
        
        <li id=""><a target="_self" href="archives.html">Archives</a></li>
        
        <li id=""><a target="_self" href="about.html">About</a></li>
        
        <li id=""><a target="_self" href="category.html">Category</a></li>
        
        </ul>

        <ul class="right" id="search-wrap">
          <li>
<form target="_blank" onsubmit="return before_search();" action="http://google.com/search" method="get">
    <input type="hidden" id="search_q" name="q" value="" />
    <input tabindex="1" type="search" id="search_input"  placeholder="Search"/>
</form>
</li>
          </ul>
      </div></div>
  </div>
  </section>

</nav>

        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; techlarry</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
        
        <li><a target="_self" href="index.html">Home</a></li>
        
        <li><a target="_self" href="archives.html">Archives</a></li>
        
        <li><a target="_self" href="about.html">About</a></li>
        
        <li><a target="_self" href="category.html">Category</a></li>
        

    <li><label>Categories</label></li>

        
            <li><a href="Leetcode.html">Leetcode</a></li>
        
            <li><a href="Python%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95.html">Python数据结构与算法</a></li>
        
            <li><a href="Python%E7%89%B9%E6%80%A7.html">Python特性</a></li>
        
            <li><a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html">机器学习</a></li>
        
            <li><a href="Python%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E4%B8%89%E7%BB%B4%E5%8F%AF%E8%A7%86%E5%8C%96.html">Python科学计算三维可视化</a></li>
        
            <li><a href="English.html">English</a></li>
        
            <li><a href="%20%E7%90%86%E8%A7%A3%E8%AE%A1%E7%AE%97%E6%9C%BA.html"> 理解计算机</a></li>
        
            <li><a href="Deep%20Learning.html">Deep Learning</a></li>
        
            <li><a href="Latex.html">Latex</a></li>
        
            <li><a href="%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.html">操作系统</a></li>
        
            <li><a href="Linux%20%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B.html">Linux 系统编程</a></li>
        
            <li><a href="%E6%95%B0%E6%8D%AE%E5%BA%93.html">数据库</a></li>
        
            <li><a href="Tensorflow.html">Tensorflow</a></li>
        
            <li><a href="Data%20Science.html">Data Science</a></li>
        
            <li><a href="C/C++.html">C/C++</a></li>
         

      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>


        <section id="main-content" role="main" class="scroll-container">
        
       

 <script type="text/javascript">
	$(function(){
		$('#menu_item_index').addClass('is_active');
	});
</script>
<div class="row">
	<div class="large-8 medium-8 columns">
		<div class="markdown-body home-categories">
		
			<div class="article">
                <a class="clearlink" href="hash_table_1_introduction.html">
                
                  <h1>Hash Table (1): Introduction</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>One of the most useful Python collections is the dictionary, which is an associative data type where you can store key-data pairs. It is implemented using <code>hash tables</code>(哈希表).</p>

<p><code>Hash table</code> (哈希表) is a collection of items which are stored in such a way as to make it easy to find item. Each position of the hash table, often called a <code>slot</code>, can hold an item and is named by an integer value starting at 0. The mapping between an item and the slot where that item belongs in the hash table is called the <code>hash function</code>(哈希函数). A <code>perfect hash function</code> maps every items into a unique slot. When two items hash to the same slot, <code>collision</code> happens. And a systematic method, which called <code>collision resolution</code>, for placing the second item in the hash table must be put forward.</p>

<h2 id="toc_0">Direct-address tables</h2>

<p>If the number of possible keys is small and they are unique, <code>direct-address tables</code> (直接寻址表) can be used. Each slot corresponds to a unique key.</p>

<pre><code class="language-python">
Insert(T,x)
    T[key(x)] = x
    
Search(T,x)
    return(T[key(x)])
    
Delete(T,x)
    T[key(x)] = NIL
</code></pre>

<p><img src="media/14973464173252/14973484465039.gif" alt="nameasdfasfas"/></p>

<p>The range of the key determines the size of the direct address table and may be too large to be practical. For instance, it&#39;s not likely that you&#39;ll be able to use a direct address table to store elements which have arbitary 32-but integers as their keys for a few years yet.</p>

<h2 id="toc_1">Hash function</h2>

<h3 id="toc_2">What is a good hash function</h3>

<p>Hash functions should have the following properties:</p>

<ul>
<li>Fast computation of the hash value (\(O(1)\)) </li>
<li>Hash values should be distributed (nearly) uniformly</li>
</ul>

<p>The goal of a hash function is</p>

<ul>
<li>disperse the keys in an apparently random way</li>
</ul>

<p><code>Remainder method</code> simply takes an item and divides it by the table size, returning the remainder as its hash value, i.e. \(h(item)=item\%11\)</p>

<p><code>Folding method</code> begins by dividing the item into equal size pieces (the last piece may not be of equal size), and then added together to give the resulting hash value by extra step of dividing by the table size and keeping the remainder.</p>

<h2 id="toc_3">Collision Resolution</h2>

<p>Various techniques are used to manage collision:</p>

<ul>
<li>Chaining (链接法)</li>
<li>Open addressing (开放寻址法)

<ul>
<li>linear probing (线性探查)</li>
<li>quadratic probing (二次探查)</li>
</ul></li>
</ul>

<h3 id="toc_4">chaining</h3>

<p>Chaining(链接法) allows each slot to hold a reference to a collection of items. It allows many items to exist at the same location in the hash table.</p>

<p><img src="media/14973464173252/Screen%20Shot%202017-06-13%20at%206.16.40%20PM.png" alt="Screen Shot 2017-06-13 at 6.16.40 P"/></p>

<h3 id="toc_5">Open addressing</h3>

<p>In <strong>open addressing</strong>, all elements occupy the hash table itself. That is, each table entry contains either an element of the dynamic set or NIL. When searching for an element, we systematically examine table slots until either we find the desired element or we have ascertained that the element is not in the table. No lists and no elements are stored outside the table, unlike in chaining.</p>

<h4 id="toc_6">Linear probing</h4>

<p>One of the simplest re-hashing functions is +1(or -1) on a collision, i.e. look in the neighbouring slot in the table. It calculates new address extremely quickly.</p>

<p>A disadvantage to linear probing is the tendency for clustering: items become clustered in the table.</p>

<h4 id="toc_7">Quadratic probing</h4>

<p>Instead of using a constant &#39;skip&#39; value, quadratic probing using a rehash function that increments the hash value by 1,3,5,7,9 and so on. This means that if the first hash value is \(h\), the successive values are \(h+1\), \(h+4\),\(h+9\), \(h+16\) and so on.</p>

<h3 id="toc_8">Load factor</h3>

<p>Load factor (装载因子) is commonly denoted by \(\lambda = \frac{number\_of\_items}{table\_size}\).</p>

<p>The most important piece of information we need to analyze the use of a hash table is the load factor, \(\lambda\). Conceptually, if the load factor is small, then there is a lower chance of collisions.</p>

<p>For a successful search, using open addressing with linear probing, the average number of comparisons is approximately \(\frac{1}{2}(1+\frac{1}{1-\lambda})\) and an unsuccessful search gives \(\frac{1}{2}(1+(\frac{1}{1-\lambda})^2)\).</p>

<p>Using chaining, the average number of comparisons is \(1+\lambda/2\) for the successful case, and simply \(\lambda\) comparisons if the search is unsuccessful.</p>

<p>High load factor \(\alpha \ge 0.85\) has negative effect on efficiency:</p>

<ul>
<li>lots of collisions
efficiency due to collision overhead</li>
</ul>

<p>其中出现的魔法方法可以参见<a href="http://larryim.cc/14973620419454.html">Link</a></p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/6/13</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Hash%20Table.html'>Hash Table</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="14973149842140.html">
                
                  <h1>搜索</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>搜索是在一个项目集合中找到一个特定项目的算法过程。返回值一般是<code>bool</code>:<code>True</code> or <code>False</code>. 搜索的几种常见方法：顺序查找、二分法查找、二叉树查找、哈希查找</p>

<h2 id="toc_0">在python中查找</h2>

<p>在python中，查找一个元素是否在列表中非常简单，可以用<code>in</code>操作符</p>

<pre><code class="language-python">&gt;&gt;&gt; 15 in [3,5,2,4,1]
False
&gt;&gt;&gt; 3 in [3,5,2,4,1]
True
&gt;&gt;&gt;
</code></pre>

<h2 id="toc_1">顺序查找</h2>

<p>顺序查找从列表中的第一个项目开始，我们按照顺序次序，简单地从一个项移动到另一个项，直到找到我们正在查找的项或遍历完整个列表。如果我们遍历完整个列表，则说明正在搜索的项不存在。</p>

<p><img src="media/14973149842140/14973392316280.png" alt=""/></p>

<pre><code class="language-python">def sequential_search(alist, item):
    &quot;&quot;&quot;
    顺序查找
    &quot;&quot;&quot;

    for i in range(len(alist)):
        if alist[i] == item:
            return True

    return False
</code></pre>

<h2 id="toc_2">二分查找</h2>

<p>二分查找过程类似于查字典。首先，假设表中元素是按升序排列，将表中间位置记录的关键字与查找关键字比较，如果两者相等，则查找成功；否则利用中间位置记录将表分成前、后两个子表，如果中间位置记录的关键字大于查找关键字，则进一步查找前一子表，否则进一步查找后一子表。重复以上过程，直到找到满足条件的记录，使查找成功，或直到子表不存在为止，此时查找不成功。</p>

<ul>
<li>优点是比较次数少，查找速度快，平均性能好</li>
<li>缺点是要求待查表为有序表，且插入删除困难</li>
<li>因此，折半查找方法适用于不经常变动而查找频繁的有序列表。</li>
</ul>

<p><img src="media/14973149842140/14973392434825.png" alt=""/></p>

<pre><code class="language-python">
def binary_search(alist, item):
    &quot;&quot;&quot;
    二分查找, 递归
    &quot;&quot;&quot;

    n = len(alist)
    if n &gt;0:
        mid = n//2

        if alist[mid] == item:
            return True
        elif alist[mid] &gt; item:
            return binary_search(alist[:mid], item)
        else:
            return binary_search(alist[mid+1:], item)
    return False

def binary_search2(alist, item):
    &quot;&quot;&quot;
    二分查找，非递归
    &quot;&quot;&quot;
    n = len(alist)
    first = 0
    last = n-1
    while first &lt;= last:
        mid = (first+last)//2
        if alist[mid] == item:
            return True
        elif alist[mid] &gt; item:
            last = mid-1
        else:
            first = mid+1

    return False

</code></pre>

<h2 id="toc_3">时间复杂度</h2>

<ul>
<li>最优时间复杂度: \(O(1)\)</li>
<li>最坏时间复杂度: \(O(\log n)\)</li>
</ul>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/6/13</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Python%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95.html'>Python数据结构与算法</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="14972614146835.html">
                
                  <h1>Machine Learning (2): Linear Regression with Multiple Variables</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">Multiple Features (variables)</a>
</li>
<li>
<a href="#toc_1">Gradient descent</a>
<ul>
<li>
<a href="#toc_2">Feature Scaling</a>
</li>
<li>
<a href="#toc_3">Learning rate</a>
</li>
</ul>
</li>
<li>
<a href="#toc_4">Computing Parameters Analytically</a>
<ul>
<li>
<a href="#toc_5">Normal equation:</a>
</li>
<li>
<a href="#toc_6">Gradient descent v.s. Normal equation</a>
</li>
<li>
<a href="#toc_7">Normal Equation Non-invertible</a>
</li>
</ul>
</li>
</ul>


<h2 id="toc_0">Multiple Features (variables)</h2>

<p>Notation:<br/>
\(m\) = the number of training examples<br/>
\(n\) = the number of features<br/>
\(x^{(i)}\)  = the input (feature) of \(i^{th}\) training example<br/>
\(x^{(i)}_j\) =  value of feature \(j\) of \(i^{th}\) training example</p>

<p>The multivariable form of the hypothesis function accommodating these multiple features is as follows:</p>

<p>\[h_θ(x)=θ_0+θ_1x_1+θ_2x_2+θ_3x_3+⋯+θ_nx_n=\theta^Tx\] (\(n+1\)- dimensional vector)</p>

<p>For convenience of notation, define \(x_0=1\)</p>

<h2 id="toc_1">Gradient descent</h2>

<p><strong>Hypothesis</strong>: <br/>
\[h_\theta(x) = \theta^Tx\]</p>

<p><strong>Parameters</strong>:<br/>
\[\theta\]</p>

<p><strong>Cost Function</strong>:<br/>
\[J(\theta)=\frac{1}{2m}\Sigma^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2\]</p>

<p><strong>Gradient descent</strong>:<br/>
Repeat until converge{</p>

<p>\(\theta_j := \theta_j -\alpha\frac{\partial}{\partial \theta_j}J(\theta)=\theta_j -\alpha\frac{1}{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}\)  (simultaneously update for every j= 0,1,...,n)</p>

<h3 id="toc_2">Feature Scaling</h3>

<p>We can speed up gradient descent by having each of our input values in roughly the same range. This is because \(\theta\) will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.</p>

<p><strong>Idea: Make sure features are on a similar scale.</strong></p>

<p>Two techniques to help with this are <strong>feature scaling</strong> and <strong>mean normalization</strong>.</p>

<ul>
<li><p><strong>Feature scaling</strong> involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1. </p></li>
<li><p><strong>Mean normalization</strong> involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero. </p></li>
</ul>

<p>Replace \(x_i\) with \( x_i -\mu_i\) to make features have approximately zero mean (Do not apply to \(x_0=1\))</p>

<p>\[x_i =\frac{ x_i - \mu_i}{S_i}\]</p>

<p>where  \(\mu_i\) is average value of \(x_i\) in training set, \(S_i\) is the range (max-min) or standard deviation of \(x_i\).</p>

<p>E.g. \[x_1=\frac{size-1000}{2000}\]<br/>
\[x_2=\frac{\#bedrooms-2}{5}\]</p>

<h3 id="toc_3">Learning rate</h3>

<p><strong>Debugging</strong>: How to make sure gradient descent is working correctly.</p>

<p>-- How to choose learning rate \(\alpha\).</p>

<p>Gradient descent is working correctly if \(J(\theta)\) decreases after every iteration.</p>

<p>Use smaller \(\alpha\). For sufficiently small \(\alpha\), \(J(\theta)\) should decrease on every iteration.</p>

<p><strong>Automatic convergence test</strong>. Declare convergence if \(J(\theta)\) decreases by less than \(E\) in one iteration, where \(E\) is some small value such as \(10^{−3}\). However in practice it&#39;s difficult to choose this threshold value.</p>

<p><strong>Summary</strong>:</p>

<ul>
<li>If  \(\alpha\)  is too small: slow convergence.</li>
<li>If \(\alpha\) is too large: may not decrease on every iteration; may not converge.</li>
</ul>

<h2 id="toc_4">Computing Parameters Analytically</h2>

<h3 id="toc_5">Normal equation:</h3>

<p>Gradient descent gives one way of minimizing \(J\). The &quot;Normal Equation&quot; method minimizes \(J\) by explicitly taking its derivatives with respect to the \(θj\) ’s, and setting them to zero. This allows us to find the optimum theta without iteration. The normal equation formula is given below:</p>

<p>\[\theta = (X^TX)^{-1}X^Ty\]</p>

<p>Matlab command: </p>

<pre><code class="language-Matlab">pinv(X&#39;*X)*X&#39;*y
</code></pre>

<p>where <code>pinv</code> is <code>peudoinversion</code> of matrix. It is different to <code>inv</code>.</p>

<h3 id="toc_6">Gradient descent v.s. Normal equation</h3>

<p><img src="media/14972614146835/Screen%20Shot%202017-06-27%20at%202.38.45%20PM.png" alt="Screen Shot 2017-06-27 at 2.38.45 P"/></p>

<h3 id="toc_7">Normal Equation Non-invertible</h3>

<p>The common reason causes non-invertible:</p>

<ul>
<li>Redundant features(linearly dependent)
E.g. \(x_1\) = size in feet\(^2\), \(x_2\) = size in m\(^2\)</li>
<li>Too many features(e.g. \(m&lt;=n\)).
-- Delete some features, or use regularization.</li>
</ul>

<p>where \(m\) is the number of training examples, \(n\) is the number of features.</p>

<p>Solutions to the above problems include deleting a feature that is linearly dependent with another or deleting one or more features when there are too many features.</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/6/12</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html'>机器学习</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="14970886401689.html">
                
                  <h1>Machine Learning (1): Introduction to Machine Learning</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">Definition</a>
</li>
<li>
<a href="#toc_1">Examples</a>
</li>
<li>
<a href="#toc_2">Machine learning algorithms</a>
</li>
<li>
<a href="#toc_3">Supervised learning</a>
<ul>
<li>
<a href="#toc_4">Example</a>
</li>
</ul>
</li>
<li>
<a href="#toc_5">Unsupervised Learning</a>
</li>
<li>
<a href="#toc_6">Cost Function</a>
</li>
<li>
<a href="#toc_7">Gradient descent</a>
</li>
</ul>


<h2 id="toc_0">Definition</h2>

<p>Arthur Samuel(1959). Machine Learning: A Field of study that gives computers the ability to learn without being explicitly programmed.</p>

<p>Tom Mitchell(1998) Well-posed Learning Problem: A computer program is said to learn from experience <code>E</code> with respect to some task <code>T</code> and some performance measure <code>P</code>, if its performance on <code>T</code>, as measured by <code>P</code>, improves with experience <code>E</code>.</p>

<p>In general, any machine learning problem can be assigned to one of two broad classifications:</p>

<p><strong>Supervised learning</strong> and <strong>Unsupervised learning</strong>.</p>

<h2 id="toc_1">Examples</h2>

<ul>
<li><p>Database mining</p>

<p>Large datasets from growth of automation/web<br/>
E.g, Web click data, medical records, biology, engineering</p></li>
<li><p>Applications can&#39;t program by hand</p>

<p>E.g., Autonomous helicopter, handwriting recognition, most of Natural Language Processing( NLP), Computer Vision</p></li>
<li><p>Self-customizing programs</p>

<p>E.g. Amazon, Netflix product recommendations</p></li>
<li><p>Understanding human learning (brain, real AI)</p></li>
</ul>

<h2 id="toc_2">Machine learning algorithms</h2>

<ul>
<li>Supervised learning</li>
<li>Unsupervised learning</li>
<li>Reinforcement learning</li>
<li>Recommender systems</li>
</ul>

<h2 id="toc_3">Supervised learning</h2>

<p>In  supervised learning, we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output.</p>

<ul>
<li><strong>Regression problem</strong>: Predict continuous valued output (eg.price)</li>
<li><strong>classification problem</strong>: Predict discrete valued output (eg. tumor or not)</li>
</ul>

<h3 id="toc_4">Example</h3>

<p>Given data about the size of houses on the real estate market, try to predict their price. Price as a function of size is a continuous output, so this is a regression problem.</p>

<p>We could turn this example into a classification problem by instead making our output about whether the house &quot;sells for more or less than the asking price.&quot; Here we are classifying the houses based on price into two discrete categories.</p>

<p><img src="media/14970886401689/Screen%20Shot%202017-06-12%20at%2010.15.14%20AM.png" alt="Screen Shot 2017-06-12 at 10.15.14 A"/></p>

<h2 id="toc_5">Unsupervised Learning</h2>

<p>Unsupervised learning is the task of making an inference from data without the “correct answers” given (unlabeled data). With unsupervised learning there is no feedback based on the prediction results.</p>

<h2 id="toc_6">Cost Function</h2>

<p>We can measure the accuracy of our hypothesis function by using a <strong>cost function</strong>. </p>

<p><strong>Idea</strong>: Choose \(\theta_0, \theta_1\) so that \(h_\theta(x)\) is close to \(y\) for our training examples (x,y)</p>

<p><strong>Hypothesis</strong>: <br/>
\[h_\theta(x) = \theta_0+\theta_1x \]</p>

<p><strong>Parameters</strong>:<br/>
\[\theta_0, \theta_1\]</p>

<p><strong>Cost Function</strong>:<br/>
\[J(\theta_0, \theta_1)=\frac{1}{2m}\Sigma^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2\]</p>

<p>This function is otherwise called the &quot;Squared error function&quot;, or &quot;Mean squared error&quot;. </p>

<p><strong>Goal</strong>:<br/>
\[ \min \limits_{\theta_0,\theta_1} J(\theta_0, \theta_1) \]</p>

<h2 id="toc_7">Gradient descent</h2>

<p>Have some function Goal: \(J(\theta_0, \theta_1) \)</p>

<p>Want \( \min \limits_{\theta_0,\theta_1} J(\theta_0, \theta_1) \)</p>

<p><strong>Outline</strong>:</p>

<ul>
<li>Start with some \(\theta_0,\theta_1\)</li>
<li>Keep changing \(\theta_0,\theta_1\) to reduce \(J(\theta_0,\theta_1)\) until we hopefully end up at a minimum</li>
</ul>

<p><strong>Gradient descent algorithm</strong>:<br/>
repeat until converge{</p>

<p>\(\theta_j := \theta_j -\alpha\frac{\partial}{\partial \theta_j}J(\theta_0,\theta_1)\)  (for \(j=0\) and \(j=1\))</p>

<p>注：</p>

<ol>
<li><code>:=</code> is assignment, not truth assertion</li>
</ol>

<p>2.At each iteration j, one should simultaneously update the parameters \(\theta_0,\theta_1\). Updating a specific parameter prior to calculating another one on the j(th) iteration would yield to a wrong implementation.</p>

<p>\(temp0 := \theta_0 -\alpha\frac{\partial}{\partial \theta_0}J(\theta_0,\theta_1)\)</p>

<p>\(temp1 := \theta_1 -\alpha\frac{\partial}{\partial \theta_1}J(\theta_0,\theta_1)\)</p>

<p>\(\theta_0:=temp0\)<br/>
\(\theta_1:=temp1\)</p>

<ol>
<li>\(\alpha\) is learning rate: if \(\alpha\) is too small, gradient descent can be slow. If \(\alpha\)  is too large, gradient descent can overshoot the minimum. It may fail to converge, or even diverge.</li>
<li>As we approach a local minimum, gradient descent will automatically take smaller steps. So, no need to decrease \(\alpha\) over time.</li>
</ol>

<p>convex function(often Bowl shaped function)</p>

<p><strong>Batch Gradient Descent</strong>: Each step of gradient descent uses all the training examples.</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/6/10</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html'>机器学习</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="14970832553234.html">
                
                  <h1>调试</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<h2 id="toc_0">pdb</h2>

<p>pdb是基于命令行的调试工具，是Python的调试器，非常类似GNU的GDB（调试c/c++）</p>

<pre><code class="language-python">python -m pdb some.py # -m是模块的意思, pdb模块
</code></pre>

<h3 id="toc_1">命令</h3>

<ul>
<li>输入命令<code>l</code> (list) 查看代码</li>
<li>输入命令<code>n</code> (next) 可以单步执行代码</li>
<li>输入命令<code>c</code> (continue) 继续执行代码</li>
<li>输入命令<code>b</code> (break) 添加断点</li>
<li>输入命令<code>clear</code> 清除断点</li>
<li>输入命令<code>p</code> (print) 打印一个变量的值</li>
<li>输入命令<code>s</code> (step) 进入到一个函数</li>
<li>输入命令<code>a</code> (args) 打印所有的形参数据</li>
<li>输入命令<code>r</code> (return) 快速执行到函数的最后一行</li>
<li>输入命令<code>q</code> quit 退出调试</li>
</ul>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/6/10</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Linux%20%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B.html'>Linux 系统编程</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
              


			<div class="row">
			  <div class="large-6 columns">
			  <p class="text-left" style="padding-top:25px;">
			   <a href="all_17.html">&laquo; Prev Page</a>  
			  </p>
			  </div>
			  <div class="large-6 columns">
			<p class="text-right" style="padding-top:25px;">
			 <a href="all_19.html">&raquo; Next Page</a> 
			</p>
			  </div>
			</div>
		</div>
	</div><!-- large 8 -->

 <div class="large-4 medium-4 columns">
  <div class="hide-for-small">
    <div id="sidebar" class="sidebar">
          <div id="site-info" class="site-info">
            
                <div class="site-a-logo"><img src="http://or9a8nskt.bkt.clouddn.com/figure.jpeg" /></div>
            
                <h1>techlarry</h1>
                <div class="site-des">他山之石，可以攻玉</div>
                <div class="social">









<a target="_blank" class="github" target="_blank" href="https://github.com/techlarry" title="GitHub">GitHub</a>
<a target="_blank" class="email" href="mailto:wang.zhen.hua.larry@gmail.com" title="Email">Email</a>
  <a target="_blank" class="rss" href="atom.xml" title="RSS">RSS</a>
                
              	 </div>
          	</div>

             

              <div id="site-categories" class="side-item ">
                <div class="side-header">
                  <h2>Categories</h2>
                </div>
                <div class="side-content">

      	<p class="cat-list">
        
            <a href="Leetcode.html"><strong>Leetcode</strong></a>
        
            <a href="Python%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95.html"><strong>Python数据结构与算法</strong></a>
        
            <a href="Python%E7%89%B9%E6%80%A7.html"><strong>Python特性</strong></a>
        
            <a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html"><strong>机器学习</strong></a>
        
            <a href="Python%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E4%B8%89%E7%BB%B4%E5%8F%AF%E8%A7%86%E5%8C%96.html"><strong>Python科学计算三维可视化</strong></a>
        
            <a href="English.html"><strong>English</strong></a>
        
            <a href="%20%E7%90%86%E8%A7%A3%E8%AE%A1%E7%AE%97%E6%9C%BA.html"><strong> 理解计算机</strong></a>
        
            <a href="Deep%20Learning.html"><strong>Deep Learning</strong></a>
        
            <a href="Latex.html"><strong>Latex</strong></a>
        
            <a href="%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.html"><strong>操作系统</strong></a>
        
            <a href="Linux%20%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B.html"><strong>Linux 系统编程</strong></a>
        
            <a href="%E6%95%B0%E6%8D%AE%E5%BA%93.html"><strong>数据库</strong></a>
        
            <a href="Tensorflow.html"><strong>Tensorflow</strong></a>
        
            <a href="Data%20Science.html"><strong>Data Science</strong></a>
        
            <a href="C/C++.html"><strong>C/C++</strong></a>
         
        </p>


                </div>
              </div>

              <div id="site-categories" class="side-item">
                <div class="side-header">
                  <h2>Recent Posts</h2>
                </div>
                <div class="side-content">
                <ul class="posts-list">
	      
		      
			      <li class="post">
			        <a href="15089188725996.html">Linux 内存布局</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15089186263565.html">Valgrind</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="machine_learning_techniques_SVM.html">Machine Learning Techniques: Support Vector Machine (SVM)</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15088244818278.html">Machine Learning Foundations - Mathematical Foundations</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="active_learning_intro.html">主动学习</a>
			      </li>
		     
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		   
		  		</ul>
                </div>
              </div>
        </div><!-- sidebar -->
      </div><!-- hide for small -->
</div><!-- large 4 -->

</div><!-- row -->

 <div class="page-bottom clearfix">
  <div class="row">
   <p class="copyright">Copyright &copy; 2015
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
</div>

        </section>
      </div>
    </div>

  
    

    <script src="asset/js/foundation.min.js"></script>
    <script>
      $(document).foundation();
      function fixSidebarHeight(){
        var w1 = $('.markdown-body').height();
          var w2 = $('#sidebar').height();
          if (w1 > w2) { $('#sidebar').height(w1); };
      }
      $(function(){
        fixSidebarHeight();
      })
      $(window).load(function(){
          fixSidebarHeight();
      });
     
    </script>

    <script src="asset/chart/all-min.js"></script><script type="text/javascript">$(function(){    var mwebii=0;    var mwebChartEleId = 'mweb-chart-ele-';    $('pre>code').each(function(){        mwebii++;        var eleiid = mwebChartEleId+mwebii;        if($(this).hasClass('language-sequence')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = Diagram.parse($(this).text());            diagram.drawSVG(eleiid,{theme: 'simple'});        }else if($(this).hasClass('language-flow')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = flowchart.parse($(this).text());            diagram.drawSVG(eleiid);        }    });});</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>


  </body>
</html>
