<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  techlarry
  
  </title>
  <meta name="keywords" content="C++; Constructor; Destructor" />
  
  <link href="atom.xml" rel="alternate" title="techlarry" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />
    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/github.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
  function before_search(){
    var searchVal = 'site:larryim.cc ' + document.getElementById('search_input').value;
    document.getElementById('search_q').value = searchVal;
    return true;
  }
</script>
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>


  <section class="top-bar-section">
  <div class="row">
      <div style="position: relative;width:100%;"><div style="position: absolute; width:100%;">
        <ul id="main-menu" class="left">
        
        <li id=""><a target="_self" href="index.html">HomePage</a></li>
        
        <li id=""><a target="_self" href="archives.html">Archives</a></li>
        
        <li id=""><a target="_blank" href="wiki">WIKI</a></li>
        
        <li id=""><a target="_self" href="notebook.html">NOTEBOOK</a></li>
        
        <li id=""><a target="_self" href="about.html">About</a></li>
        
        </ul>

        <ul class="right" id="search-wrap">
          <li>
<form target="_blank" onsubmit="return before_search();" action="http://google.com/search" method="get">
    <input type="hidden" id="search_q" name="q" value="" />
    <input tabindex="1" type="search" id="search_input"  placeholder="Search"/>
</form>
</li>
          </ul>
      </div></div>
  </div>
  </section>

</nav>

        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; techlarry</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
        
        <li><a target="_self" href="index.html">HomePage</a></li>
        
        <li><a target="_self" href="archives.html">Archives</a></li>
        
        <li><a target="_blank" href="wiki">WIKI</a></li>
        
        <li><a target="_self" href="notebook.html">NOTEBOOK</a></li>
        
        <li><a target="_self" href="about.html">About</a></li>
        

    <li><label>Categories</label></li>

        
            <li><a href="Leetcode.html">Leetcode</a></li>
        
            <li><a href="programming_language.html">编程语言</a></li>
        
            <li><a href="data_structure_and_algorithm.html">数据结构和算法</a></li>
        
            <li><a href="Course.html">Course</a></li>
        
            <li><a href="Python%E7%89%B9%E6%80%A7.html">Python特性</a></li>
        
            <li><a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html">机器学习</a></li>
        
            <li><a href="Python%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E4%B8%89%E7%BB%B4%E5%8F%AF%E8%A7%86%E5%8C%96.html">Python科学计算三维可视化</a></li>
        
            <li><a href="English.html">English</a></li>
        
            <li><a href="Computer%20System.html">Computer System</a></li>
        
            <li><a href="Deep%20Learning.html">Deep Learning</a></li>
        
            <li><a href="Linux%20%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B.html">Linux 系统编程</a></li>
        
            <li><a href="%E6%95%B0%E6%8D%AE%E5%BA%93.html">数据库</a></li>
        
            <li><a href="Tensorflow.html">Tensorflow</a></li>
        
            <li><a href="Big%20Data.html">Big Data</a></li>
        
            <li><a href="%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB.html">文献阅读</a></li>
        
            <li><a href="Tools.html">Tools</a></li>
         

      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>


        <section id="main-content" role="main" class="scroll-container">
        
       

 <script type="text/javascript">
	$(function(){
		$('#menu_item_index').addClass('is_active');
	});
</script>
<div class="row">
	<div class="large-8 medium-8 columns">
		<div class="markdown-body home-categories">
		
			<div class="article">
                <a class="clearlink" href="machine_learning_system_design.html">
                
                  <h1>Machine Learning (6): Advice for applying machine learning and Machine Learning System Design</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">[1] Advice for applying machine learning</a>
<ul>
<li>
<a href="#toc_1">Deciding What to Try Next</a>
</li>
<li>
<a href="#toc_2">Evaluating a Hypothesis</a>
<ul>
<li>
<a href="#toc_3">The test set error</a>
</li>
</ul>
</li>
<li>
<a href="#toc_4">Model Selection and Train/Validation/Test Sets</a>
</li>
<li>
<a href="#toc_5">Diagnosing Bias vs. Variance</a>
</li>
<li>
<a href="#toc_6">Regularization and Bias/Variance</a>
</li>
<li>
<a href="#toc_7">Learning Curves</a>
</li>
<li>
<a href="#toc_8">Deciding What to Do Next Revisited</a>
<ul>
<li>
<a href="#toc_9">Diagnosing Neural Networks</a>
</li>
<li>
<a href="#toc_10">Model Selection:</a>
</li>
</ul>
</li>
</ul>
</li>
<li>
<a href="#toc_11">[2] Machine Learning System Design</a>
<ul>
<li>
<a href="#toc_12">Prioritizing What to Work On</a>
</li>
<li>
<a href="#toc_13">Error Analysis</a>
</li>
<li>
<a href="#toc_14">Error Metrics for Skewed Classes</a>
</li>
<li>
<a href="#toc_15">Trading Off Precision and Recall</a>
</li>
<li>
<a href="#toc_16">Data for Machine Learning</a>
</li>
</ul>
</li>
</ul>


<h2 id="toc_0">[1] Advice for applying machine learning</h2>

<h3 id="toc_1">Deciding What to Try Next</h3>

<p>Errors in your predictions can be troubleshooted by:</p>

<ul>
<li>  Getting more training examples</li>
<li>  Trying smaller sets of features</li>
<li>  Trying additional features</li>
<li>  Trying polynomial features</li>
<li>  Increasing or decreasing λ</li>
</ul>

<p>Don&#39;t just pick one of these avenues at random. We&#39;ll explore diagnostic techniques for choosing one of the above solutions in the following sections.</p>

<h3 id="toc_2">Evaluating a Hypothesis</h3>

<p>A hypothesis may have low error for the training examples but still be inaccurate (because of overfitting).</p>

<p>With a given dataset of training examples, we can split up the data into two sets: a <strong>training set</strong> and a <strong>test set</strong> .</p>

<p>The new procedure using these two sets is then:</p>

<ol>
<li> Learn \(\Theta\) and minimize \(J_{train}(\Theta)\) using the training set</li>
<li> Compute the test set error \(J_{test}(\Theta)\)</li>
</ol>

<h4 id="toc_3">The test set error</h4>

<ol>
<li> For linear regression: \(J_{test}(\Theta) = \dfrac{1}{2m_{test}} \sum_{i=1}^{m_{test}}(h_\Theta(x^{(i)}_{test}) - y^{(i)}_{test})^2\)</li>
<li> For classification ~ Misclassification error (aka 0/1 misclassification error):</li>
</ol>

<p>\(err(h_\Theta(x),y) = \{\begin{matrix} 1 &amp; \mbox{if } h_\Theta(x) \geq 0.5\ and\ y = 0\ or\ h_\Theta(x) &lt; 0.5\ and\ y = 1\newline 0 &amp; \mbox otherwise \end{matrix}\)</p>

<p>This gives us a binary 0 or 1 error result based on a misclassification.</p>

<p>The average test error for the test set is</p>

<p>\(\text{Test Error} = \dfrac{1}{m_{test}} \sum^{m_{test}}_{i=1} err(h_\Theta(x^{(i)}_{test}), y^{(i)}_{test})\)</p>

<p>This gives us the proportion of the test data that was misclassified.</p>

<h3 id="toc_4">Model Selection and Train/Validation/Test Sets</h3>

<ul>
<li>  Just because a learning algorithm fits a training set well, that does not mean it is a good hypothesis.</li>
<li>  The error of your hypothesis as measured on the data set with which you trained the parameters will be lower than any other data set.</li>
</ul>

<p>In order to choose the model of your hypothesis, you can test each degree of polynomial and look at the error result.</p>

<p><strong>Without the Validation Set (note: this is a bad method - do not use it)</strong></p>

<ol>
<li> Optimize the parameters in Θ using the training set for each polynomial degree.</li>
<li> Find the polynomial degree d with the least error using the test set.</li>
<li> Estimate the generalization error also using the test set with \(J_{test}(\Theta^{(d)})\), (d = theta from polynomial with lower error);</li>
</ol>

<p>In this case, we have trained one variable, d, or the degree of the polynomial, using the test set. This will cause our error value to be greater for any other set of data.</p>

<p><strong>Use of the CV set</strong></p>

<p>To solve this, we can introduce a third set, the <strong>Cross Validation Set</strong> , to serve as an intermediate set that we can train d with. Then our test set will give us an accurate, non-optimistic error.</p>

<p>One example way to break down our dataset into the three sets is:</p>

<ul>
<li>  Training set: 60%</li>
<li>  Cross validation set: 20%</li>
<li>  Test set: 20%</li>
</ul>

<p>We can now calculate three separate error values for the three different sets.</p>

<p><strong>With the Validation Set (note: this method presumes we do not also use the CV set for regularization)</strong></p>

<ol>
<li><p>Optimize the parameters in Θ using the training set for each polynomial degree.</p></li>
<li><p>Find the polynomial degree d with the least error using the cross validation set.</p></li>
<li><p>Estimate the generalization error using the test set with \(J_{test}(\Theta^{(d)})\), (d = theta from polynomial with lower error);</p></li>
</ol>

<p>This way, the degree of the polynomial d has not been trained using the test set.</p>

<p>(Mentor note: be aware that using the CV set to select &#39;d&#39; means that we cannot also use it for the validation curve process of setting the lambda value).</p>

<h3 id="toc_5">Diagnosing Bias vs. Variance</h3>

<p>In this section we examine the relationship between the degree of the polynomial d and the underfitting or overfitting of our hypothesis.</p>

<ul>
<li><p>We need to distinguish whether <strong>bias</strong> or <strong>variance</strong> is the problem contributing to bad predictions.</p></li>
<li><p>High bias is underfitting and high variance is overfitting. We need to find a golden mean between these two.</p></li>
</ul>

<p>The training error will tend to <strong>decrease</strong> as we increase the degree d of the polynomial.</p>

<p>At the same time, the cross validation error will tend to <strong>decrease</strong> as we increase d up to a point, and then it will <strong>increase</strong> as d is increased, forming a convex curve.</p>

<p><strong>High bias (underfitting)</strong> : both \(J_{train}(\Theta)\) and \(J_{CV}(\Theta)\) will be high. Also, \(J_{CV}(\Theta) \approx J_{train}(\Theta)\).</p>

<p><strong>High variance (overfitting)</strong> : \(J_{train}(\Theta)\) will be low and \(J_{CV}(\Theta)\) will be much greater than\(J_{train}(\Theta)\).</p>

<p>The is represented in the figure below:</p>

<p><img src="media/15045143826264/15049369778320.jpg" alt=""/></p>

<h3 id="toc_6">Regularization and Bias/Variance</h3>

<p>Instead of looking at the degree d contributing to bias/variance, now we will look at the regularization parameter λ.</p>

<ul>
<li>  Large λ: High bias (underfitting)</li>
<li>  Intermediate λ: just right</li>
<li>  Small λ: High variance (overfitting)</li>
</ul>

<p>A large lambda heavily penalizes all the Θ parameters, which greatly simplifies the line of our resulting function, so causes underfitting.</p>

<p>The relationship of λ to the training set and the variance set is as follows:</p>

<p><strong>Low λ</strong> : \(J_{train}(\Theta)\) is low and \(J_{CV}(\Theta)\) is high (high variance/overfitting).</p>

<p><strong>Intermediate λ</strong> : \(J_{train}(\Theta)\) and \(J_{CV}(\Theta)\) are somewhat low and \(J_{train}(\Theta) \approx J_{CV}(\Theta)\).</p>

<p><strong>Large λ</strong> : both \(J_{train}(\Theta)\) and \(J_{CV}(\Theta)\) will be high (underfitting /high bias)</p>

<p>The figure below illustrates the relationship between lambda and the hypothesis:</p>

<p><img src="media/15045143826264/15049370422319.jpg" alt=""/></p>

<p>In order to choose the model and the regularization λ, we need:</p>

<ol>
<li> Create a list of lambdas (i.e. λ∈{0,0.01,0.02,0.04,0.08,0.16,0.32,0.64,1.28,2.56,5.12,10.24});<br/>
2. Create a set of models with different degrees or any other variants.<br/>
3. Iterate through the \(\lambda\)s and for each \(\lambda\) go through all the models to learn some \(\Theta\).<br/>
4. Compute the cross validation error using the learned Θ (computed with λ) on the \(J_{CV}(\Theta)\) without regularization or λ = 0.<br/>
5. Select the best combo that produces the lowest error on the cross validation set.<br/>
6. Using the best combo Θ and λ, apply it on \(J_{test}(\Theta)\) to see if it has a good generalization of the problem.</li>
</ol>

<h3 id="toc_7">Learning Curves</h3>

<p>Training 3 examples will easily have 0 errors because we can always find a quadratic curve that exactly touches 3 points.</p>

<ul>
<li>  As the training set gets larger, the error for a quadratic function increases.</li>
<li>  The error value will plateau out after a certain m, or training set size.</li>
</ul>

<p><strong>With high bias</strong></p>

<p><strong>Low training set size</strong> : causes \(J_{train}(\Theta)\) to be low and \(J_{CV}(\Theta)\) to be high.</p>

<p><strong>Large training set size</strong> : causes both \(J_{train}(\Theta)\) and \(J_{CV}(\Theta)\) to be high with \(J_{train}(\Theta)\)≈\(J_{CV}(\Theta)\).</p>

<p>If a learning algorithm is suffering from <strong>high bias</strong> , getting more training data <strong>will not (by itself) help much</strong> .</p>

<p>For high variance, we have the following relationships in terms of the training set size:</p>

<p><strong>With high variance</strong></p>

<p><strong>Low training set size</strong> : \(J_{train}(\Theta)\) will be low and \(J_{CV}(\Theta)\) will be high.</p>

<p><strong>Large training set size</strong> : \(J_{train}(\Theta)\) increases with training set size and \(J_{CV}(\Theta)\) continues to decrease without leveling off. Also, \(J_{train}(\Theta)\)&lt;\(J_{CV}(\Theta)\) but the difference between them remains significant.</p>

<p>If a learning algorithm is suffering from <strong>high variance</strong> , getting more training data is <strong>likely to help.</strong></p>

<p><img src="media/15045143826264/15049370738559.jpg" alt=""/></p>

<p><img src="media/15045143826264/15049370848695.jpg" alt=""/></p>

<h3 id="toc_8">Deciding What to Do Next Revisited</h3>

<p>Our decision process can be broken down as follows:</p>

<ul>
<li>  Getting more training examples</li>
</ul>

<p>Fixes high variance</p>

<ul>
<li>  Trying smaller sets of features</li>
</ul>

<p>Fixes high variance</p>

<ul>
<li>  Adding features</li>
</ul>

<p>Fixes high bias</p>

<ul>
<li>  Adding polynomial features</li>
</ul>

<p>Fixes high bias</p>

<ul>
<li>  Decreasing λ</li>
</ul>

<p>Fixes high bias</p>

<ul>
<li>  Increasing λ</li>
</ul>

<p>Fixes high variance</p>

<h4 id="toc_9">Diagnosing Neural Networks</h4>

<ul>
<li>  A neural network with fewer parameters is <strong>prone to underfitting</strong> . It is also <strong>computationally cheaper</strong> .</li>
<li>  A large neural network with more parameters is <strong>prone to overfitting</strong> . It is also <strong>computationally expensive</strong> . In this case you can use regularization (increase λ) to address the overfitting.</li>
</ul>

<p>Using a single hidden layer is a good starting default. You can train your neural network on a number of hidden layers using your cross validation set.</p>

<h4 id="toc_10">Model Selection:</h4>

<p>Choosing M the order of polynomials.</p>

<p>How can we tell which parameters Θ to leave in the model (known as &quot;model selection&quot;)?</p>

<p>There are several ways to solve this problem:</p>

<ul>
<li>  Get more data (very difficult).</li>
<li>  Choose the model which best fits the data without overfitting (very difficult).</li>
<li>  Reduce the opportunity for overfitting through regularization.</li>
</ul>

<p><strong>Bias: approximation error (Difference between expected value and optimal value)</strong></p>

<ul>
<li>  High Bias = UnderFitting (BU)</li>
<li>  \(J_{train}(\Theta)\) and \(J_{CV}(\Theta)\) both will be high and \(J_{train}(\Theta)\) ≈ \(J_{CV}(\Theta)\)</li>
</ul>

<p><strong>Variance: estimation error due to finite data</strong></p>

<ul>
<li>  High Variance = OverFitting (VO)</li>
<li>  \(J_{train}(\Theta)\) is low and \(J_{CV}(\Theta)\) ≫\(J_{train}(\Theta)\)</li>
</ul>

<p><strong>Intuition for the bias-variance trade-off:</strong></p>

<ul>
<li>  Complex model =&gt; sensitive to data =&gt; much affected by changes in X =&gt; high variance, low bias.</li>
<li>  Simple model =&gt; more rigid =&gt; does not change as much with changes in X =&gt; low variance, high bias.</li>
</ul>

<p>One of the most important goals in learning: finding a model that is just right in the bias-variance trade-off.</p>

<p><strong>Regularization Effects:</strong></p>

<ul>
<li>  Small values of λ allow model to become finely tuned to noise leading to large variance =&gt; overfitting.</li>
<li>  Large values of λ pull weight parameters to zero leading to large bias =&gt; underfitting.</li>
</ul>

<p><strong>Model Complexity Effects:</strong></p>

<ul>
<li>  Lower-order polynomials (low model complexity) have high bias and low variance. In this case, the model fits poorly consistently.</li>
<li>  Higher-order polynomials (high model complexity) fit the training data extremely well and the test data extremely poorly. These have low bias on the training data, but very high variance.</li>
<li>  In reality, we would want to choose a model somewhere in between, that can generalize well but also fits the data reasonably well.</li>
</ul>

<p><strong>A typical rule of thumb when running diagnostics is:</strong></p>

<ul>
<li>  More training examples fixes high variance but not high bias.</li>
<li>  Fewer features fixes high variance but not high bias.</li>
<li>  Additional features fixes high bias but not high variance.</li>
<li>  The addition of polynomial and interaction features fixes high bias but not high variance.</li>
<li>  When using gradient descent, decreasing lambda can fix high bias and increasing lambda can fix high variance (lambda is the regularization parameter).</li>
<li>  When using neural networks, small neural networks are more prone to under-fitting and big neural networks are prone to over-fitting. Cross-validation of network size is a way to choose alternatives.</li>
</ul>

<h2 id="toc_11">[2] Machine Learning System Design</h2>

<h3 id="toc_12">Prioritizing What to Work On</h3>

<p>Different ways we can approach a machine learning problem:</p>

<ul>
<li>  Collect lots of data (for example &quot;honeypot&quot; project but doesn&#39;t always work)</li>
<li>  Develop sophisticated features (for example: using email header data in spam emails)</li>
<li>  Develop algorithms to process your input in different ways (recognizing misspellings in spam).</li>
</ul>

<p>It is difficult to tell which of the options will be helpful.</p>

<h3 id="toc_13">Error Analysis</h3>

<p>The recommended approach to solving machine learning problems is:</p>

<ul>
<li>  Start with a simple algorithm, implement it quickly, and test it early.</li>
<li>  Plot learning curves to decide if more data, more features, etc. will help</li>
<li>  Error analysis: manually examine the errors on examples in the cross validation set and try to spot a trend.</li>
</ul>

<p>It&#39;s important to get error results as a single, numerical value. Otherwise it is difficult to assess your algorithm&#39;s performance.</p>

<p>You may need to process your input before it is useful. For example, if your input is a set of words, you may want to treat the same word with different forms (fail/failing/failed) as one word, so must use &quot;stemming software&quot; to recognize them all as one.</p>

<h3 id="toc_14">Error Metrics for Skewed Classes</h3>

<p>It is sometimes difficult to tell whether a reduction in error is actually an improvement of the algorithm.</p>

<ul>
<li>  For example: In predicting a cancer diagnoses where 0.5% of the examples have cancer, we find our learning algorithm has a 1% error. However, if we were to simply classify every single example as a 0, then our error would reduce to 0.5% even though we did not improve the algorithm.</li>
</ul>

<p>This usually happens with <strong>skewed classes</strong> ; that is, when our class is very rare in the entire data set.</p>

<p>Or to say it another way, when we have lot more examples from one class than from the other class.</p>

<p>For this we can use <strong>Precision/Recall</strong> .</p>

<ul>
<li>  Predicted: 1, Actual: 1 --- True positive</li>
<li>  Predicted: 0, Actual: 0 --- True negative</li>
<li>  Predicted: 0, Actual, 1 --- False negative</li>
<li>  Predicted: 1, Actual: 0 --- False positive</li>
</ul>

<p><strong>Precision</strong> : of all patients we predicted where y=1, what fraction actually has cancer?</p>

<p>\(\dfrac{\text{True Positives}}{\text{Total number of predicted positives}} = \dfrac{\text{True Positives}}{\text{True Positives}+\text{False positives}}\)</p>

<p><strong>Recall</strong> : Of all the patients that actually have cancer, what fraction did we correctly detect as having cancer?</p>

<p>\(\dfrac{\text{True Positives}}{\text{Total number of actual positives}}= \dfrac{\text{True Positives}}{\text{True Positives}+\text{False negatives}}\)</p>

<p>These two metrics give us a better sense of how our classifier is doing. We want both precision and recall to be high.</p>

<p>In the example at the beginning of the section, if we classify all patients as 0, then our <strong>recall</strong> will be \(\dfrac{0}{0 + f} = 0\), so despite having a lower error percentage, we can quickly see it has worse recall.</p>

<p>Accuracy = \(\frac {\text{true positive + true negative}} {\text{total population}}\)</p>

<p>Note 1: if an algorithm predicts only negatives like it does in one of exercises, the precision is not defined, it is impossible to divide by 0. F1 score will not be defined too.</p>

<h3 id="toc_15">Trading Off Precision and Recall</h3>

<p>We might want a <strong>confident</strong> prediction of two classes using logistic regression. One way is to increase our threshold:</p>

<ul>
<li><p>Predict 1 if: \(h_\theta(x) \geq 0.7\)</p></li>
<li><p>Predict 0 if: \(h_\theta(x) &lt; 0.7\)</p></li>
</ul>

<p>This way, we only predict cancer if the patient has a 70% chance.</p>

<p>Doing this, we will have <strong>higher precision</strong> but <strong>lower recall</strong> (refer to the definitions in the previous section).</p>

<p>In the opposite example, we can lower our threshold:</p>

<ul>
<li><p>Predict 1 if: \(h_\theta(x) \geq 0.3\)</p></li>
<li><p>Predict 0 if: \(h_\theta(x) &lt; 0.3\)</p></li>
</ul>

<p>That way, we get a very <strong>safe</strong> prediction. This will cause <strong>higher recall</strong> but <strong>lower precision</strong> .</p>

<p>The greater the threshold, the greater the precision and the lower the recall.</p>

<p>The lower the threshold, the greater the recall and the lower the precision.</p>

<p>In order to turn these two metrics into one single number, we can take the <strong>F value</strong> .</p>

<p>One way is to take the <strong>average</strong> :</p>

<p>\(\dfrac{P+R}{2}\)</p>

<p>This does not work well. If we predict all y=0 then that will bring the average up despite having 0 recall. If we predict all examples as y=1, then the very high recall will bring up the average despite having 0 precision.</p>

<p>A better way is to compute the <strong>F Score</strong> (or F1 score):</p>

<p>\(\text{F Score} = 2\dfrac{PR}{P + R}\)</p>

<p>In order for the F Score to be large, both precision and recall must be large.</p>

<p>We want to train precision and recall on the <strong>cross validation set</strong> so as not to bias our test set.</p>

<h3 id="toc_16">Data for Machine Learning</h3>

<p>How much data should we train on?</p>

<p>In certain cases, an &quot;inferior algorithm,&quot; if given enough data, can outperform a superior algorithm with less data.</p>

<p>We must choose our features to have <strong>enough</strong> information. A useful test is: Given input x, would a human expert be able to confidently predict y?</p>

<p><strong>Rationale for large data</strong> : if we have a <strong>low bias</strong> algorithm (many features or hidden units making a very complex function), then the larger the training set we use, the less we will have overfitting (and the more accurate the algorithm will be on the test set).</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/9/2</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html'>机器学习</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="logistic_regression_classification.html">
                
                  <h1>Machine Learning (3): Logistic Regression - Classification</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">Binary Classification</a>
<ul>
<li>
<a href="#toc_1">Examples</a>
</li>
<li>
<a href="#toc_2">Hypothesis Representation</a>
</li>
<li>
<a href="#toc_3">Decision boundary</a>
</li>
<li>
<a href="#toc_4">Cost Function</a>
</li>
<li>
<a href="#toc_5">Gradient Descent</a>
</li>
<li>
<a href="#toc_6">Advanced Optimization</a>
</li>
</ul>
</li>
<li>
<a href="#toc_7">Multiclass classification</a>
<ul>
<li>
<a href="#toc_8">Example</a>
</li>
<li>
<a href="#toc_9">Method</a>
</li>
</ul>
</li>
</ul>


<p>The classification problem is just like the regression problem, except that the values \(y\) we now want to predict take on only a small number of discrete values.</p>

<h2 id="toc_0">Binary Classification</h2>

<p><strong>Binary classification problem</strong> : \(y\) can take on only two values, 0 and 1.</p>

<h3 id="toc_1">Examples</h3>

<ul>
<li>Email: Spam/Not Spam?</li>
<li>Online Transaction: Fraudulent(Yes/NO)?</li>
<li>Tumor: Malignant/ Benign?</li>
</ul>

<p>Given \(x(i)\), the corresponding \(y(i)\) is also called the <strong>label</strong> for the training example.</p>

<h3 id="toc_2">Hypothesis Representation</h3>

<p>It doesn&#39;t make sense for \(h_\theta(x)\) to take values larger than 1 or smaller than0, when we know that \( y\in \{0,1\}\). To fix this, let&#39;s change the form for our hypotheses \(h_\theta(x)\) to satisfy \(0\leq_\theta(x)\leq 1\). This is accomplished by plugging \(\theta^Tx\) into the Logistic Function.</p>

<p><strong>Logistic Function</strong>:<br/>
also called <code>sigmoid</code> (/&#39;sɪgmɔɪd/) function<br/>
   \[\begin{align*}&amp; h_\theta (x) = g ( \theta^T x ) \newline \newline&amp; z = \theta^T x \newline&amp; g(z) = \dfrac{1}{1 + e^{-z}}\end{align*}\]</p>

<ol>
<li><p>The sigmoid function g(z), maps any real number to the (0, 1) interval, making it useful for transforming an arbitrary-valued function into a function better suited for classification.</p></li>
<li><p>\(h_\theta(x)\) gives us the probability that our output is 1. Probability that prediction is 0 is just the complement of probability that it is 1:</p></li>
</ol>

<p>\[\begin{align*}&amp; h_\theta(x) = P(y=1 | x ; \theta) = 1 - P(y=0 | x ; \theta) \newline&amp; P(y = 0 | x;\theta) + P(y = 1 | x ; \theta) = 1\end{align*}\]</p>

<h3 id="toc_3">Decision boundary</h3>

<p>The <strong>decision boundary</strong> is the line that separates the area where \(y\) = 0 and where \(y = 1\). It is defined by hypothesis function.</p>

<h3 id="toc_4">Cost Function</h3>

<p>We cannot use the same cost function that we use for linear regression because the Logistic Function will cause the output to be wary, causing many local optima.</p>

<p>Instead, our cost function for logistic regression looks like:</p>

<p>\[\begin{align*}&amp; J(\theta) = \dfrac{1}{m} \sum_{i=1}^m \mathrm{Cost}(h_\theta(x^{(i)}),y^{(i)}) \newline &amp; \mathrm{Cost}(h_\theta(x),y) = -\log(h_\theta(x)) \; &amp; \text{if y = 1} \newline &amp; \mathrm{Cost}(h_\theta(x),y) = -\log(1-h_\theta(x)) \; &amp; \text{if y = 0}\end{align*}\]</p>

<p>Cost function&#39;s two conditional cases can be compressed into one case:<br/>
\[Cost(h_\theta(x),y)= -y \log(h_\theta(x))-(1-y) \log(1-h_\theta(x))\]</p>

<p>The entire cost function can be written as follows:</p>

<p>\[ J(\theta) = -\dfrac{1}{m} \sum_{i=1}^m[ y ^{(i)}\log(h_\theta(x^{(i)}))+(1-y^{(i)}) \log(1-h_\theta(x^{(i)}))]\]</p>

<p>A vectorized implementation is:<br/>
\[h=g(X\theta)\]<br/>
\[J(\theta)=\frac{1}{m}(-y^T\log(h)-(1-y)^T\log(1-h))\]</p>

<p>And the gradient of the cost is a vector of the same length as \(\theta\) where the \(j^{th}\) element is defined as follows:</p>

<p>\[\frac{\partial J(\theta)}{\partial \theta_j}=\frac{1}{m}\Sigma_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j\]</p>

<p>A vectorized implementation is:<br/>
\[\frac{\partial J(\theta)}{\partial \theta_j}=\frac{1}{m}(h-y)^TX\]</p>

<h3 id="toc_5">Gradient Descent</h3>

<p>Repeat until {</p>

<p>\[\theta_j:=\theta_j-\frac{\alpha}{m}\Sigma^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j\]<br/>
}</p>

<p>A vectorized implementation is:</p>

<p>\[\theta:=\theta-\frac{\alpha}{m}X^T(g(X\theta)-y)\]</p>

<h3 id="toc_6">Advanced Optimization</h3>

<p>There are more sophisticated, faster ways to optimize \(\theta\) that can be used instead of gradient descent:</p>

<ul>
<li>Conjugate gradient</li>
<li>BFGS</li>
<li>L-BFGS</li>
</ul>

<p>You should not write these algorithms yourself, but use the libraries provided by Octave and Matlab instead.</p>

<h2 id="toc_7">Multiclass classification</h2>

<h3 id="toc_8">Example</h3>

<ul>
<li>Email foldering/tagging: Work, friends, family, hobby</li>
<li>Medical diagrams: Not ill, Cold, Flu</li>
<li>Weather: Sunny, Cloudy, Rain, Snow</li>
</ul>

<h3 id="toc_9">Method</h3>

<p>Since \(y = \{0,1...n\}\), we divide our problem into \(n+1\) (+1 because the index starts at 0) binary classification problems; in each one, we predict the probability that &#39;y&#39; is a member of one of our classes.</p>

<p>We are basically choosing one class and then lumping all the others into a single second class. We do this repeatedly, applying binary logistic regression to each case, and then use the hypothesis that returned the highest value as our prediction.</p>

<p>\[\begin{align*}&amp; y \in \lbrace0, 1 ... n\rbrace \newline&amp; h_\theta^{(0)}(x) = P(y = 0 | x ; \theta) \newline&amp; h_\theta^{(1)}(x) = P(y = 1 | x ; \theta) \newline&amp; \cdots \newline&amp; h_\theta^{(n)}(x) = P(y = n | x ; \theta) \newline&amp; \mathrm{prediction} = \max_i( h_\theta ^{(i)}(x) )\newline\end{align*}\]</p>

<p><strong>To summarize</strong>:</p>

<ul>
<li>Train a logistic regression classifier hθ(x) for each class to predict the probability that  \(y = i\).</li>
<li>To make a prediction on a new x, pick the class that maximizes \(h_\theta(x)\).</li>
</ul>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/6/27</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html'>机器学习</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="15025903235887.html">
                
                  <h1>Latex 技巧</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<h2 id="toc_0">Latex 查询</h2>

<p>可以在<code>Dash</code>中下载Latex Cheat Sheet以便随时查询。</p>

<h2 id="toc_1">Latex 如何将下标放在正上方／正下方</h2>

<p>在 <code>\sum</code>，<code>\max</code>等之后添加<code>\limits^{upper}_{lower}</code>：\(\sum\limits^{upper}_{lower}\)</p>

<p>例如：</p>

<pre><code class="language-latex">c(i,j) = \sum\limits^{j}_{k=i}f[k] + \min\limits_{r=i}^j \prod\limits^{m}_np(x^i)
</code></pre>

<p>\(c(i,j) = \sum \limits^{j}_{k=i}f[k] + \min \limits_{r=i}^j \prod \limits^{m}_np(x^i)\)</p>

<h2 id="toc_2">字体</h2>

<p>花式字体<code>math calligraphy</code>，常用于随机变量，例如: \(\mathcal{X, Y}\)</p>

<pre><code class="language-latex">\mathcal{X, Y}
</code></pre>

<p>空心加粗字体<code>blackboard bold</code>，常用于实数集R: \(\mathbb{R}\)</p>

<pre><code class="language-text">\mathbb{R}
</code></pre>

<h2 id="toc_3">上下</h2>

<p>使用<code>\stackrel{}{}</code>，效果如下：</p>

<p>\(x \stackrel{i.i.d}{\backsim} P(x), y \stackrel{p}{apple} P(y|x)\)</p>

<pre><code class="language-latex">x \stackrel{i.i.d}{\backsim} P(x), y \stackrel{p}{apple} P(y|x)
</code></pre>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/8/13</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Tools.html'>Tools</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="introdution_to_big_data.html">
                
                  <h1>Introduction to big data</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<h2 id="toc_0">Characteristics of Big data</h2>

<p>3 V: Volume, Variety, Velocity</p>

<p>Most now agree with the characterization of big data using the 3 V’s coined by Doug Laney of Gartner:</p>

<p>·<code>Volume</code>: This refers to the vast amounts of data that is generated every second/minute/hour/day in our digitized world.</p>

<p>· <code>Velocity</code>: This refers to the speed at which data is being generated and the pace at which data moves from one point to the next.</p>

<p>· <code>Variety</code>: This refers to the ever-increasing different forms that data can come in, e.g., text, images, voice, geospatial.</p>

<p>A fourth V is now also sometimes added:</p>

<p>· <code>Veracity</code>: This refers to the quality of the data, which can vary greatly.</p>

<p>There are many other V&#39;s that gets added to these depending on the context. For our specialization, we will add:</p>

<p>· <code>Valence</code>: This refers to how big data can bond with each other, forming connections between otherwise disparate datasets.</p>

<p>The above V’s are the dimensions that characterize big data, and also embody its challenges: We have huge amounts of data, in different formats and varying quality, that must be processed quickly.</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/10/31</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Big%20Data.html'>Big Data</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="C++baics.html">
                
                  <h1>C++ Basics</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>The article summarizes the key points, concepts of C++, with detailed examples, from a C Programmer standpoint.</p>

<ul>
<li>
<a href="#toc_0">C++ Advantage</a>
</li>
<li>
<a href="#toc_1">C++ is better than C</a>
</li>
<li>
<a href="#toc_2">Overloading</a>
</li>
<li>
<a href="#toc_3">Generics</a>
<ul>
<li>
<a href="#toc_4">Multiple template arguments</a>
</li>
<li>
<a href="#toc_5">Overloading Operator</a>
</li>
</ul>
</li>
<li>
<a href="#toc_6">Natural way to build widgets: <code>class</code></a>
</li>
<li>
<a href="#toc_7">Constructor and destructor</a>
<ul>
<li>
<a href="#toc_8">Copy Constructor</a>
</li>
<li>
<a href="#toc_9">Destructor</a>
</li>
</ul>
</li>
<li>
<a href="#toc_10">Scope and storage class</a>
</li>
<li>
<a href="#toc_11">Memory Management</a>
</li>
<li>
<a href="#toc_12">Reference</a>
</li>
</ul>


<h2 id="toc_0">C++ Advantage</h2>

<ul>
<li>safe cast
<ul>
<li><code>static cast &lt;type&gt;</code></li>
</ul></li>
<li><code>for</code> statement can include declaration initializer</li>
<li><code>endl</code> io-manipulator can be placed in an iostream.</li>
</ul>

<h2 id="toc_1">C++ is better than C</h2>

<ul>
<li>More <code>type safety</code>(the extent to which a programming language discourages or prevents type errors)
<ul>
<li>The new operator returns a pointer of a specific type based on the operand, versus the void pointer from C&#39;s <code>malloc</code>.</li>
<li>Certain code written in C that uses void pointers can be rewritten using C++ templates to give a type to an argument whose type is variable.</li>
<li>Using <code>static_cast</code> and <code>dynamic_cast</code></li>
</ul></li>
<li>More Libraries
<ul>
<li>STL(Standard Template Library)</li>
</ul></li>
<li>Less reliance on preprocessor
<ul>
<li><code># define</code>  goes away, easy to use <code>const &lt;type&gt;</code> and <code>inline</code> (substitute macros in C)instead.</li>
</ul></li>
<li>OO vs imperative
<ul>
<li>OO is better</li>
</ul></li>
<li><code>namespace</code></li>
<li>safe cast 
<ul>
<li>using <code>static_cast &lt;type&gt;</code></li>
</ul></li>
<li><code>iostream</code></li>
<li>Declarations anywhere including
<ul>
<li>for-statement initialization</li>
</ul></li>
</ul>

<h2 id="toc_2">Overloading</h2>

<p><code>Overloaded functions</code> are an important polymorphic mechanism in C++. <code>signature matching algorithm</code> decides the way to pick the appropriate functions.</p>

<p><strong><code>Basic Signature Matching Algorithm</code></strong>:</p>

<ul>
<li>Use an exact match if found.</li>
<li>Try standard type promotions.</li>
<li>Try standard type conversions.</li>
<li>Try user-defined conversions.</li>
<li>...</li>
</ul>

<p><strong>Example: Swap in C</strong> </p>

<pre><code class="language-c"> //first swap function
 //Call by reference simulated with pointers
 void swap(int* i, int* j){
    int temp=*i;
    *i=*j;
    *j=temp;
 }

//second swap function
//Call by reference simulated with pointers
void swap_double(double* i, double* j){ 
    double temp=*i;
    *i=*j;
    *j=temp;
 }
 
 int main()
 {
    int m=5, n=10;
    double x=5.3, y=10.6;
    printf(&quot;inputs:%d, %d\n&quot;, m,n);
    swap_double(&amp;m, &amp;n);
    printf(&quot;outputs:%d, %d\n&quot;, m,n);
    printf(&quot;double inputs:%lf, %lf\n&quot;, x,y);
    swap_double(&amp;x, &amp;y);
    printf(&quot;double outputs: %lf, %lf\n&quot;, x, y);
}
</code></pre>

<p>In C each function in a given scope most have a unique name(no function overloading).</p>

<ul>
<li>C: Call by reference simulated with pointers (the previous example)</li>
<li>C++: Call by reference argument passing (see the example below)</li>
</ul>

<p><strong>Example: Swap in C++</strong>:</p>

<pre><code class="language-cpp">#include &lt;iostream&gt;
using namespace std;

// iostream type safe and intuitive convinient.

inline void swap(int &amp;i, int &amp;j){ // call by reference, reference to int
        int temp = i;
        i = j;
        j = temp;
}

inline void swap(double &amp;i, double &amp;j){
        double temp = i;
        i = j;
        j = temp;
}

int main()
{
        int m=5, n=10;
        double x = 5.3, y=10.6;
        cout &lt;&lt; &quot;inputs: &quot; &lt;&lt; m &lt;&lt; &quot;,&quot; &lt;&lt; n &lt;&lt; endl;
        swap(m,n);
        cout &lt;&lt; &quot;outputs: &quot; &lt;&lt; m &lt;&lt; &quot;,&quot; &lt;&lt; n &lt;&lt; endl;

        cout &lt;&lt; &quot;double inputs: &quot; &lt;&lt; x &lt;&lt; &quot;,&quot; &lt;&lt; y &lt;&lt; endl;
        swap(x,y);
        cout &lt;&lt; &quot;outputs: &quot; &lt;&lt; x &lt;&lt; &quot;,&quot; &lt;&lt; y &lt;&lt; endl;
}
</code></pre>

<ul>
<li>Having the same name for conceptually the same activity promotes readable code.</li>
<li>Overloading based on signature and the use of generics(<code>template</code>) is a powerful reuse mechanism.</li>
</ul>

<h2 id="toc_3">Generics</h2>

<p>Generics in C++: programing using templates.</p>

<ul>
<li>The compiler uses <code>template</code> to write code appropriate to each set of parameters.</li>
<li>The \(m\), \(n\) parameters are ints - so the compiled code is <code>swap(int&amp;, int&amp;)</code> signature.</li>
<li>Similarly, for the other two signatures. So the compiler is compiling code appropriate to each distinct signature.</li>
</ul>

<p><strong>Example: Swap Using templates</strong>:</p>

<pre><code class="language-cpp">#include &lt;iostream&gt;
#include &lt;complex&gt;

using namespace std;

template &lt;class T&gt;
inline void swaps(T&amp; d, T&amp; s)
{
        T temp=d;
        d = s;
        s = temp;
}

int main()
{
        int m=5, n=10;
        double x=5.3, y=10.6;
        complex&lt;double&gt; r(2.4, 3.5), s(3.4, 6.7);
        cout &lt;&lt; &quot;inputs: &quot; &lt;&lt; m &lt;&lt; &quot;,&quot; &lt;&lt; n &lt;&lt; endl;
        swaps(m,n);
        cout &lt;&lt; &quot;outputs:&quot; &lt;&lt; m &lt;&lt; &quot;,&quot; &lt;&lt; n &lt;&lt; endl;

        cout &lt;&lt; &quot;complex double inputs: &quot; &lt;&lt; r &lt;&lt; &quot;,&quot; &lt;&lt; s &lt;&lt; endl;
        swaps(r,s);
        cout &lt;&lt; &quot;complex double outputs:&quot; &lt;&lt; r &lt;&lt; &quot;,&quot; &lt;&lt; s &lt;&lt; endl;
}
</code></pre>

<p>Another example, how in C do you write a function to sum a array in doubles?</p>

<p><strong>Example: Sum a array in C</strong>:</p>

<pre><code class="language-c">double sum(double data[], int size)
{
    double s =0.0;
    int i;
    for (i=0; i&lt;size; i++)
        s += data[i];
    return s;        
</code></pre>

<p>Generic programming:</p>

<ul>
<li>writing code that can use an arbitrary type or types; in C++ this is done with template.</li>
</ul>

<p><strong>Example: Sum a array in C++ Using template</strong>:</p>

<pre><code class="language-cpp">template &lt;class T&gt; //T is generic type
T sum(const T data[], int size, T s=0)
{
    for (int i=0; i&lt;size; ++i)
        s += data[i]; // += must work for T
    return s;
}

int main()
{
    cout &lt;&lt; &quot;template for sum()&quot; &lt;&lt; endl;
    int a[] = {1,2,3};
    double b[] = {2.1, 2.2, 2.3};
    cout &lt;&lt; sum(a,3) &lt;&lt; endl;
    cout &lt;&lt; sum(b,3) &lt;&lt; endl;
}
</code></pre>

<h3 id="toc_4">Multiple template arguments</h3>

<ul>
<li>We have used templates with one template parameter, but it is useful to have more that one distinct type in a template.</li>
<li>More genericity - but be careful!</li>
</ul>

<p><strong>Type safety</strong>:</p>

<ul>
<li>More types means worrying about conversions and more signatures</li>
<li>There <code>static_cast</code> operators are considered safe.</li>
<li>The old cast operator (type) is deprecated as a reminder the other casting operators are:
<ul>
<li><code>reinterpret_cast&lt;type&gt;</code> highly unsafe</li>
<li><code>dynamic_cast&lt;type&gt;</code> used with classes</li>
<li><code>const_cast&lt;type&gt;</code> cast away const-ness</li>
</ul></li>
</ul>

<h3 id="toc_5">Overloading Operator</h3>

<p>Unary and binary operators can be overloaded as nonstatic member functions. Implicitly, they are acting on a class value.</p>

<pre><code class="language-cpp">#include&lt;iostream&gt; 
#include&lt;cstdint&gt; 

using namespace std;

enum class days:std::int8_t 
{SUN,MON,TUE,WED,THU,FRI,SAT};

//unlike plain enums C++11 enum class is typesafe and does not silently 
//convert to int

ostream&amp; operator&lt;&lt;(ostream&amp; out, const days&amp; d)  // call by reference 
{
    switch (d){
        case days::SUN: out&lt;&lt; &quot;SUN&quot;; break;
        case days::MON: out&lt;&lt; &quot;MON&quot;; break;
        case days::TUE: out&lt;&lt; &quot;TUE&quot;; break;
        case days::WED: out&lt;&lt; &quot;WED&quot;; break;
        case days::THU: out&lt;&lt; &quot;THU&quot;; break;
        case days::FRI: out&lt;&lt; &quot;FRI&quot;; break;
        case days::SAT: out&lt;&lt; &quot;SAT&quot;; break;
    }
    return out;
}

days operator++(days&amp; d) //PREFIX OPERATOR 
{ 
    d = static_cast&lt;days&gt;((static_cast&lt;int&gt;(d) + 1) % 7); 
    return d;
}

days operator++(days&amp; d, int) //POSTFIX OPERATOR 
{
    days temp = d;
    d = static_cast&lt;days&gt;((static_cast&lt;int&gt;(d) + 1) % 7);
    return temp; 
}

int main(){
    days today{days::MON};
    std::cout &lt;&lt; &quot;Demonstrate class enum&quot; &lt;&lt; std::endl; 
    std::cout &lt;&lt; &quot;MONDAY VALUE IS &quot; &lt;&lt; today &lt;&lt; std::endl;
    std::cout &lt;&lt; &quot;INCREMENT VALUE IS &quot; &lt;&lt; ++today &lt;&lt; std::endl; 
    std::cout &lt;&lt; &quot;INCREMENT VALUE IS &quot; &lt;&lt; today++ &lt;&lt; std::endl; 
    std::cout &lt;&lt; &quot;INCREMENT VALUE IS &quot; &lt;&lt; today &lt;&lt; std::endl; 
    return 0;
}

</code></pre>

<p>OO Principle:</p>

<ul>
<li>User defined types should be indistinguishable from native types
<ul>
<li>Operator overloading and conversion allows us to implement this principle</li>
</ul></li>
</ul>

<h2 id="toc_6">Natural way to build widgets: <code>class</code></h2>

<p><strong>C type extension</strong>:</p>

<ul>
<li>In C you can &#39;add&#39; a type using <code>struct</code></li>
<li>In C++ <code>struct</code> is different - <code>struct</code> is a named scope that can include functions (methods) and have different layers of opacity (data hiding). While C++ retains <code>struct</code> it has the new keyword <code>class</code>.</li>
<li><code>class</code> is almost equivalent to <code>struct</code> - but with different data hiding defaults.</li>
</ul>

<pre><code class="language-cpp">typedef struct point {double x,y;} point;
void add_points(point* p1, point* p2, point* sum)
{
    sum -&gt; x = p1 -&gt; x + p2 -&gt; x;
    sum -&gt; y = p1 -&gt; y + p2 -&gt; y;
}
</code></pre>

<ul>
<li>C struct has fields - data members;</li>
<li>File scope functions manipulate the struct using pointer operations</li>
</ul>

<p><strong>Naive C++ point</strong></p>

<pre><code class="language-cpp">class point{
    public:
        double x,y;
};
</code></pre>

<ul>
<li><code>public</code>, <code>private</code>, <code>protected</code> are access keywords.</li>
</ul>

<p><strong>Better C++ Point</strong>:</p>

<pre><code class="language-cpp">#include &lt;iostream&gt;
using namespace std;

class point{
    public:
        double getx(){return x;} // access methods
        void setx(double v){x = v;} // mutate methods 
        double x,y; 
};

point operator+(point&amp; p1, point&amp; p2){
    point sum={p1.x + p2.x, p1.y+p2.y};
    return sum;
}

ostream&amp; operator&lt;&lt;(ostream&amp; out, const point&amp;p){
    out &lt;&lt; &quot;(&quot; &lt;&lt; p.x &lt;&lt; &quot;,&quot; &lt;&lt; p.y &lt;&lt;&quot;)&quot;;
    return out;
}

int main()
{
    point a = {3.5, 2.5}, b={2.5, 4.5}, c;
    cout &lt;&lt; &quot;a=&quot; &lt;&lt; a &lt;&lt; &quot; b= &quot; &lt;&lt; b &lt;&lt; endl;
    cout &lt;&lt; &quot;sum= &quot; &lt;&lt; a+b &lt;&lt; endl;
}

// reuslt is 
//a=(3.5,2.5) b= (2.5,4.5)
//sum= (6,7)
</code></pre>

<p><strong>class - User defined types</strong>:</p>

<ul>
<li>Using point in <code>main()</code> looks very much like using a native type.</li>
<li>Indeed this is one of our key goal in Object Oriented Programming.</li>
<li>We accomplished this by having point be a class -- a user defined type. We overload the standard operators like + and &lt;&lt; to give them appropriate &quot;point&quot; semantics.</li>
</ul>

<p><strong>Methods</strong>:</p>

<ul>
<li>Class member functions have automatic access to private members.</li>
<li><code>p1.getx()</code>, <code>p1.setx(3.5)</code></li>
</ul>

<h2 id="toc_7">Constructor and destructor</h2>

<p>A <code>constructor</code>(构造函数) is a member function whose name is the same as the class name; it creates objects of the class type. This process involves initializing data members and, frequently, allocating storage from the heap by using <code>new</code>.</p>

<p><code>default constructor</code> does not requires a initializing value(no arguments),</p>

<pre><code class="language-cpp">class point{
    public:
        //constructor, initializer list
        point(double x=0.0, double y=0.0): x(x), y(y){} 
        double getx(){return x;}
        void setx(double v){x=v;}
    private:
        double x,y;
};
</code></pre>

<p><strong>A special method constructor</strong>:</p>

<ul>
<li><code>point(){x=y=0.0;}</code>: ordinary assignment</li>
<li><code>point(){this-&gt;x=0.0; this-&gt;y=0.0}</code> :initialized list</li>
<li><code>point():x(0.0), y(0.0){}</code> : best way</li>
<li>Default constructor - the constructor whose signature is void.</li>
</ul>

<p><strong>Constructor overloading</strong>:</p>

<ul>
<li>It is useful to have multiple ways to initialize an object like point.</li>
<li><code>point(double x, double y)</code></li>
<li><code>{this -&gt; x=x; this-&gt;y=y};</code></li>
</ul>

<p>It is also the case that usually there are several constructors per class. Each constructor signature represents a useful way to declare and initialize an object of that type.</p>

<p><strong>Example: Improved Points Using constructor</strong></p>

<pre><code class="language-cpp">#include &lt;iostream&gt;

using namespace std;

class point{
    public:
        point(): x(0), y(0) {} //default
        point(double u): x(u), y(0) {} //double to point
        point(double u, double v): x(u), y(v) {}
        void print() const { cout &lt;&lt; &quot;(&quot; &lt;&lt; x &lt;&lt; &quot;,&quot; &lt;&lt; y &lt;&lt; &quot;)&quot;; }
        void set(double u, double v) {x =u; y=v;}
        void plus(point c) {x += c.x; y+=c.y;}
    private:
        double x, y;
};


double parabola(double x, double p)
{
    return (x*x)/p;
}

void graph(double a, double b, double incr, 
    double f(double, double), double p, point gr[])
{
    double x = a;
    for (int i=0; x &lt;=b; ++i, x += incr)
        gr[i].set(x, f(x, p));
}


const int no_of_pts = 20;


int main()
{
    point g[no_of_pts]; // use default ctor

    graph(0, 2, 0.1, parabola, 5, g);
    cout &lt;&lt; &quot;First 20 samples:&quot; &lt;&lt; endl;
    for (int i=0; i&lt; no_of_pts; ++i)
    {
        g[i].print();
        if (i%5==4)
            cout &lt;&lt; endl;
        else
            cout &lt;&lt; &quot;  &quot;;
    }
}
</code></pre>

<h3 id="toc_8">Copy Constructor</h3>

<p>For class types, call-by-value requires a <code>copy constructor</code>. The compiler provides a copy constructor whose signature is <code>class_name:: class_name(const class_name &amp;)</code></p>

<h3 id="toc_9">Destructor</h3>

<p>A <code>destructor</code>(析构函数) is a member function whose name is the class preceded by the tilde character, <code>~</code>. The destructor is implicitly invoked on block and function exit to clean up storage no longer accessible.</p>

<p><strong>Example:List</strong>:</p>

<pre><code class="language-cpp">#include &lt;iostream&gt;

using namespace std;

struct list_element {
    list_element(int n=0, list_element* ptr=0): d(n), next(ptr){}
    int d;
    list_element* next;
};


class list{
    public:
        list(): head(0), cursor(0){}  // default constructor
        list(const int* arr, int n); // transfer data
        list(const list&amp; lst); //copy constructor
        void prepend(int n); //insert at front value n
        int get_element(){ return cursor -&gt; d;}
        void advance(){ cursor = cursor -&gt; next;}
        void print();
        ~list(){delete head; delete cursor;}; // destructor
    private:
        list_element* head;
        list_element* cursor;
};


void list::prepend(int n)
{
    if (head==0) // empty list case
        cursor = head = new list_element(n, head);
    else // add to front-chain
        head = new list_element(n, head);
}


void list::print(){
    list_element* h = head;
    while(h!=0){//idiom for chaining
        cout &lt;&lt; h-&gt;d &lt;&lt; &quot;,&quot;;
        h = h-&gt;next;
    }
    cout &lt;&lt; &quot;###&quot; &lt;&lt; endl;
}

int main()
{
    list a, b;
    a.prepend(9);
    a.prepend(8);
    cout &lt;&lt; &quot; list a&quot; &lt;&lt; endl;
    a.print();
    for (int i=0; i&lt;40; ++i)
        b.prepend(i*i);
    cout &lt;&lt; &quot; list b&quot; &lt;&lt; endl;
    b.print();
    return 0;
}
</code></pre>

<h2 id="toc_10">Scope and storage class</h2>

<p>The C++ has two principle forms of scope: <code>local scope</code> and <code>file scope</code>. <code>Local scope</code> is scoped to a block(e.g. compound statements, function bodies). <code>File scope</code> has names that are external(global).  </p>

<p>The four storage classes are <code>auto</code>, <code>extern</code>, <code>register</code>, and <code>static</code>.</p>

<p><strong><code>auto</code></strong>:</p>

<ul>
<li>allocated within a block, and its lifetime is limited to the execution of that block.</li>
</ul>

<p><strong><code>register</code></strong>:</p>

<ul>
<li>tells the compiler that the associated variables should be stored in high-speed memory registers</li>
</ul>

<p><strong><code>static</code></strong>:</p>

<ul>
<li><code>static</code> variables retain its previous value when the block is reentered.</li>
<li><code>static</code> functions are visible only within the file in which they are defined.</li>
</ul>

<h2 id="toc_11">Memory Management</h2>

<p>In C++:</p>

<ul>
<li><code>new</code> (allocator) instead of <code>malloc()</code> in C </li>
<li><code>delete</code> (deallocator) instead of <code>free()</code> in C</li>
<li>Both work with a heap -- heap is dynamically allocated memory - unlike Java not automatically garbage collected.</li>
</ul>

<p><strong>Simple use of <code>new</code> and <code>delete</code></strong>:</p>

<pre><code class="language-cpp">char* s = new char[size]; //get of heap
int* p = new int(9); //single int initialized
delete [] s; //delete an array
delete p; //delete single element

// These will get used with dynamic data structures 
// in constructors and destructors
</code></pre>

<h2 id="toc_12">Reference</h2>

<ul>
<li>Ira Pohl. C++ for C Programmers,University of California, Santa Cruz</li>
<li>Ira Pohl. <a href="https://users.soe.ucsc.edu/%7Epohl/">C++ by Dissection</a>.</li>
</ul>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2016/10/28</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='programming_language.html'>编程语言</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
              


			<div class="row">
			  <div class="large-6 columns">
			  <p class="text-left" style="padding-top:25px;">
			   <a href="all_10.html">&laquo; Prev Page</a>  
			  </p>
			  </div>
			  <div class="large-6 columns">
			<p class="text-right" style="padding-top:25px;">
			 <a href="all_12.html">&raquo; Next Page</a> 
			</p>
			  </div>
			</div>
		</div>
	</div><!-- large 8 -->

 <div class="large-4 medium-4 columns">
  <div class="hide-for-small">
    <div id="sidebar" class="sidebar">
          <div id="site-info" class="site-info">
            
                <div class="site-a-logo"><img src="http://or9a8nskt.bkt.clouddn.com/figure.jpeg" /></div>
            
                <h1>techlarry</h1>
                <div class="site-des">他山之石，可以攻玉</div>
                <div class="social">









<a target="_blank" class="github" target="_blank" href="https://github.com/techlarry" title="GitHub">GitHub</a>
<a target="_blank" class="email" href="mailto:wang.zhen.hua.larry@gmail.com" title="Email">Email</a>
  <a target="_blank" class="rss" href="atom.xml" title="RSS">RSS</a>
                
              	 </div>
          	</div>

             

              <div id="site-categories" class="side-item ">
                <div class="side-header">
                  <h2>Categories</h2>
                </div>
                <div class="side-content">

      	<p class="cat-list">
        
            <a href="Leetcode.html"><strong>Leetcode</strong></a>
        
            <a href="programming_language.html"><strong>编程语言</strong></a>
        
            <a href="data_structure_and_algorithm.html"><strong>数据结构和算法</strong></a>
        
            <a href="Course.html"><strong>Course</strong></a>
        
            <a href="Python%E7%89%B9%E6%80%A7.html"><strong>Python特性</strong></a>
        
            <a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html"><strong>机器学习</strong></a>
        
            <a href="Python%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E4%B8%89%E7%BB%B4%E5%8F%AF%E8%A7%86%E5%8C%96.html"><strong>Python科学计算三维可视化</strong></a>
        
            <a href="English.html"><strong>English</strong></a>
        
            <a href="Computer%20System.html"><strong>Computer System</strong></a>
        
            <a href="Deep%20Learning.html"><strong>Deep Learning</strong></a>
        
            <a href="Linux%20%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B.html"><strong>Linux 系统编程</strong></a>
        
            <a href="%E6%95%B0%E6%8D%AE%E5%BA%93.html"><strong>数据库</strong></a>
        
            <a href="Tensorflow.html"><strong>Tensorflow</strong></a>
        
            <a href="Big%20Data.html"><strong>Big Data</strong></a>
        
            <a href="%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB.html"><strong>文献阅读</strong></a>
        
            <a href="Tools.html"><strong>Tools</strong></a>
         
        </p>


                </div>
              </div>

              <div id="site-categories" class="side-item">
                <div class="side-header">
                  <h2>Recent Posts</h2>
                </div>
                <div class="side-content">
                <ul class="posts-list">
	      
		      
			      <li class="post">
			        <a href="csapp-internet-programming.html">CSAPP - 网络编程</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="os-concets-processes.html">Operating System Concepts 3 - Processes</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="os_concepts_threads_and_concurrency.html">Operating System Concepts 4 - Threads & Concurrency</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="head-first_java_note.html">Head first Java</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="cpp_by_diessection.html">[NOTE] C++ By Dissection</a>
			      </li>
		     
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		   
		  		</ul>
                </div>
              </div>
        </div><!-- sidebar -->
      </div><!-- hide for small -->
</div><!-- large 4 -->

</div><!-- row -->

 <div class="page-bottom clearfix">
  <div class="row">
   <p class="copyright">Copyright &copy; 2015
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
</div>

        </section>
      </div>
    </div>

  
    

    <script src="asset/js/foundation.min.js"></script>
    <script>
      $(document).foundation();
      function fixSidebarHeight(){
        var w1 = $('.markdown-body').height();
          var w2 = $('#sidebar').height();
          if (w1 > w2) { $('#sidebar').height(w1); };
      }
      $(function(){
        fixSidebarHeight();
      })
      $(window).load(function(){
          fixSidebarHeight();
      });
     
    </script>

    
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>


  </body>
</html>
