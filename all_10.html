<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  techlarry
  
  </title>
  
  
  <link href="atom.xml" rel="alternate" title="techlarry" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />
    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/github.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
  function before_search(){
    var searchVal = 'site:larryim.cc ' + document.getElementById('search_input').value;
    document.getElementById('search_q').value = searchVal;
    return true;
  }
</script>
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>


  <section class="top-bar-section">
  <div class="row">
      <div style="position: relative;width:100%;"><div style="position: absolute; width:100%;">
        <ul id="main-menu" class="left">
        
        <li id=""><a target="_self" href="index.html">HomePage</a></li>
        
        <li id=""><a target="_self" href="archives.html">Archives</a></li>
        
        <li id=""><a target="_blank" href="wiki">WIKI</a></li>
        
        <li id=""><a target="_self" href="notebook.html">NOTEBOOK</a></li>
        
        <li id=""><a target="_self" href="about.html">About</a></li>
        
        </ul>

        <ul class="right" id="search-wrap">
          <li>
<form target="_blank" onsubmit="return before_search();" action="http://google.com/search" method="get">
    <input type="hidden" id="search_q" name="q" value="" />
    <input tabindex="1" type="search" id="search_input"  placeholder="Search"/>
</form>
</li>
          </ul>
      </div></div>
  </div>
  </section>

</nav>

        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; techlarry</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
        
        <li><a target="_self" href="index.html">HomePage</a></li>
        
        <li><a target="_self" href="archives.html">Archives</a></li>
        
        <li><a target="_blank" href="wiki">WIKI</a></li>
        
        <li><a target="_self" href="notebook.html">NOTEBOOK</a></li>
        
        <li><a target="_self" href="about.html">About</a></li>
        

    <li><label>Categories</label></li>

        
            <li><a href="Leetcode.html">Leetcode</a></li>
        
            <li><a href="C/C++.html">C/C++</a></li>
        
            <li><a href="Python%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95.html">Python数据结构与算法</a></li>
        
            <li><a href="Course.html">Course</a></li>
        
            <li><a href="Python%E7%89%B9%E6%80%A7.html">Python特性</a></li>
        
            <li><a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html">机器学习</a></li>
        
            <li><a href="Python%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E4%B8%89%E7%BB%B4%E5%8F%AF%E8%A7%86%E5%8C%96.html">Python科学计算三维可视化</a></li>
        
            <li><a href="English.html">English</a></li>
        
            <li><a href="Computer%20System.html">Computer System</a></li>
        
            <li><a href="Deep%20Learning.html">Deep Learning</a></li>
        
            <li><a href="Linux%20%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B.html">Linux 系统编程</a></li>
        
            <li><a href="%E6%95%B0%E6%8D%AE%E5%BA%93.html">数据库</a></li>
        
            <li><a href="Tensorflow.html">Tensorflow</a></li>
        
            <li><a href="Big%20Data.html">Big Data</a></li>
        
            <li><a href="%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB.html">文献阅读</a></li>
        
            <li><a href="Tools.html">Tools</a></li>
         

      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>


        <section id="main-content" role="main" class="scroll-container">
        
       

 <script type="text/javascript">
	$(function(){
		$('#menu_item_index').addClass('is_active');
	});
</script>
<div class="row">
	<div class="large-8 medium-8 columns">
		<div class="markdown-body home-categories">
		
			<div class="article">
                <a class="clearlink" href="unsupervised_learning.html">
                
                  <h1>Machine Learning(8): Unsupervised Learning</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">K-means Clustering</a>
<ul>
<li>
<a href="#toc_1">Explaination</a>
</li>
<li>
<a href="#toc_2">Choosing the Number of Clusters</a>
</li>
</ul>
</li>
<li>
<a href="#toc_3">Principal Component Analysis</a>
<ul>
<li>
<a href="#toc_4">Choosing the Number of Principal Components</a>
</li>
<li>
<a href="#toc_5">Advice for Applying PCA</a>
</li>
</ul>
</li>
</ul>


<h2 id="toc_0">K-means Clustering</h2>

<p>The K-Means Algorithm is the most popular and widely used algorithm for automatically grouping data into coherent subsets.</p>

<ol>
<li><p>Randomly initialize two points in the dataset called the <u>cluster centroids</u> .</p></li>
<li><p>_Cluster assignment_: assign all examples into one of two groups based on which cluster centroid the example is closest to.</p></li>
<li><p>_Move centroid_: compute the averages for all the points inside each of the two cluster centroid groups, then move the cluster centroid points to those averages.</p></li>
<li><p>Re-run (2) and (3) until we have found our clusters.</p></li>
</ol>

<p>Main variables are:</p>

<ul>
<li><p>K (number of clusters)</p></li>
<li><p>Training set \({x^{(1)}, x^{(2)}, \dots,x^{(m)}}\)</p></li>
<li><p>Where \(x^{(i)} \in \mathbb{R}^n\)</p></li>
</ul>

<p><strong>The algorithm:</strong></p>

<pre><code class="language-text">Randomly initialize K cluster centroids mu(1), mu(2), ..., mu(K)
Repeat:
   for i = 1 to m:
      c(i):= index (from 1 to K) of cluster centroid closest to x(i)
   for k = 1 to K:
      mu(k):= average (mean) of points assigned to cluster k
</code></pre>

<h3 id="toc_1">Explaination</h3>

<p>The <strong>first for-loop</strong> is the <u>Cluster Assignment</u> step. We make a vector <u>c</u> where <u>c(i)</u> represents the centroid assigned to example <u>x(i)</u> .</p>

<p>We can write the operation of the Cluster Assignment step more mathematically as follows:</p>

<p>\(c^{(i)} = \arg \min_k\ ||x^{(i)} - \mu_k||^2\)</p>

<p>That is, each \(c^{(i)}\) contains the index of the centroid that has minimal distance to \(x^{(i)}\).</p>

<p>By convention, we square the right-hand-side, which makes the function we are trying to minimize more sharply increasing. It is mostly just a convention. But a convention that helps reduce the computation load because the Euclidean distance requires a square root but it is canceled.</p>

<p>Without the square:</p>

<p>\(||x^{(i)} - \mu_k|| = ||\quad\sqrt{(x_1^i - \mu_{1(k)})^2 + (x_2^i - \mu_{2(k)})^2 + (x_3^i - \mu_{3(k)})^2 + ...}\quad||\)</p>

<p>With the square:</p>

<p>\(||x^{(i)} - \mu_k||^2 = ||\quad(x_1^i - \mu_{1(k)})^2 + (x_2^i - \mu_{2(k)})^2 + (x_3^i - \mu_{3(k)})^2 + ...\quad||\)</p>

<p>...so the square convention serves two purposes, minimize more sharply and less computation.</p>

<p>The <strong>second for-loop</strong> is the &#39;Move Centroid&#39; step where we move each centroid to the average of its group.</p>

<p>More formally, the equation for this loop is as follows:</p>

<p>\(\mu_k = \dfrac{1}{n}[x^{(k_1)} + x^{(k_2)} + \dots + x^{(k_n)}] \in \mathbb{R}^n\)</p>

<p>Where each of \(x^{(k_1)}, x^{(k_2)}, \dots, x^{(k_n)}\) are the training examples assigned to group \(mμ_k\).</p>

<p>If you have a cluster centroid with <strong>0 points</strong> assigned to it, you can randomly <strong>re-initialize</strong> that centroid to a new point. You can also simply <strong>eliminate</strong> that cluster group.</p>

<p>After a number of iterations the algorithm will <u><strong>converge</strong></u> , where new iterations do not affect the clusters.</p>

<p>Note on non-separated clusters: some datasets have no real inner separation or natural structure. K-means can still evenly segment your data into K subsets, so can still be useful in this case.</p>

<h3 id="toc_2">Choosing the Number of Clusters</h3>

<p>Choosing K can be quite arbitrary and ambiguous.</p>

<p><strong>The elbow method</strong>: plot the cost J and the number of clusters K. The cost function should reduce as we increase the number of clusters, and then flatten out. Choose K at the point where the cost function starts to flatten out.</p>

<p>However, fairly often, the curve is <strong>very gradual</strong> , so there&#39;s no clear elbow.</p>

<p><strong>Note:</strong> J will <strong>always</strong> decrease as K is increased. The one exception is if k-means gets stuck at a bad local optimum.</p>

<p>Another way to choose K is to observe how well k-means performs on a <strong>downstream purpose</strong> . In other words, you choose K that proves to be most useful for some goal you&#39;re trying to achieve from using these clusters.</p>

<p><img src="media/15101207367271/elbow%20method.png" alt="elbow method"/></p>

<h2 id="toc_3">Principal Component Analysis</h2>

<p>The most popular dimensionality reduction algorithm is <u>Principal Component Analysis</u> (PCA)</p>

<p>Before applying PCA, there is a data pre-processing step we must perform:</p>

<p><strong>Data preprocessing</strong></p>

<ul>
<li>  Given training set: \(x(1),x(2),…,x(m)\)</li>
<li><p>Preprocess (feature scaling/mean normalization): \(\mu_j = \dfrac{1}{m}\sum^m_{i=1}x_j^{(i)}\)</p></li>
<li><p>Replace each \(x_j^{(i)}\) with \(x_j^{(i)} - \mu_j\)</p></li>
<li><p>If different features on different scales (e.g., \(x_1\) = size of house, \(x_2\) = number of bedrooms), scale features to have comparable range of values.</p></li>
</ul>

<p>Above, we first subtract the mean of each feature from the original feature. Then we scale all the features \(x_j^{(i)} = \dfrac{x_j^{(i)} - \mu_j}{s_j}\).</p>

<p>We can define specifically what it means to reduce from 2d to 1d data as follows:</p>

<p>\(\Sigma = \dfrac{1}{m}\sum^m_{i=1}(x^{(i)})(x^{(i)})^T\)</p>

<p>The z values are all real numbers and are the projections of our features onto \(u^{(1)}\).</p>

<p>So, PCA has two tasks: figure out \(u^{(1)},\dots,u^{(k)}\) and also to find \(z_1, z_2, \dots, z_m\).</p>

<p>The mathematical proof for the following procedure is complicated and beyond the scope of this course.</p>

<p><strong>1. Compute &quot;covariance matrix&quot;</strong></p>

<p>\(\Sigma = \dfrac{1}{m}\sum^m_{i=1}(x^{(i)})(x^{(i)})^T\)</p>

<p>This can be vectorized in Octave as:</p>

<p>\[Sigma = (1/m) * X&#39; * X;\]</p>

<p>We denote the covariance matrix with a capital sigma (which happens to be the same symbol for summation, confusingly---they represent entirely different things).</p>

<p>Note that \(x^{(i)}\) is an n×1 vector, \((x^{(i)})^T\) is an 1×n vector and X is a m×n matrix (row-wise stored examples). The product of those will be an n×n matrix, which are the dimensions of Σ.</p>

<p><strong>2. Compute &quot;eigenvectors&quot; of covariance matrix Σ</strong></p>

<pre><code class="language-octave">[U,S,V] = svd(Sigma);
</code></pre>

<p>svd() is the &#39;singular value decomposition&#39;, a built-in Octave function.</p>

<p>What we actually want out of svd() is the &#39;U&#39; matrix of the Sigma covariance matrix: \(U \in \mathbb{R}^{n \times n}\). U contains \(u^{(1)},\dots,u^{(n)}\), which is exactly what we want.</p>

<p><strong>3. Take the first k columns of the U matrix and compute z</strong></p>

<p>We&#39;ll assign the first k columns of U to a variable called <code>Ureduce</code>. This will be an n×k matrix. We compute z with:</p>

<p>\(z^{(i)} = \text{Ureduce}^T \cdot x^{(i)}\)</p>

<p>\(\text{Ureduce}Z^T\) will have dimensions k×n while x(i) will have dimensions n×1. The product \(\text{Ureduce}^T \cdot x^{(i)}\) will have dimensions k×1.</p>

<p>To summarize, the whole algorithm in <code>octave</code> is roughly:</p>

<pre><code class="language-octave">Sigma = (1/m) * X&#39; * X; % compute the covariance matrix
[U,S,V] = svd(Sigma);   % compute our projected directions
Ureduce = U(:,1:k);     % take the first k directions
Z = X * Ureduce;        % compute the projected data points
</code></pre>

<h3 id="toc_4">Choosing the Number of Principal Components</h3>

<p>How do we choose <u>number of principal components</u> \(k\)? </p>

<p>One way to choose k is by using the following formula:</p>

<ul>
<li><p>Given the average squared projection error: \(\dfrac{1}{m}\sum^m_{i=1}||x^{(i)} - x_{approx}^{(i)}||^2\)</p></li>
<li><p>Also given the total variation in the data: \(\dfrac{1}{m}\sum^m_{i=1}||x^{(i)}||^2\)</p></li>
<li><p>Choose k to be the smallest value such that: \(\dfrac{\dfrac{1}{m}\sum^m_{i=1}||x^{(i)} - x_{approx}^{(i)}||^2}{\dfrac{1}{m}\sum^m_{i=1}||x^{(i)}||^2} \leq 0.01\)</p></li>
</ul>

<p>In other words, the squared projection error divided by the total variation should be less than one percent, so that <strong>99% of the variance is retained</strong> .</p>

<p><strong>Algorithm for choosing k</strong></p>

<ol>
<li><p>Try PCA with k=1,2,…</p></li>
<li><p>Compute \(U_{reduce}, z, x\)</p></li>
<li><p>Check the formula given above that _99% of the variance is retained_. If not, go to step one and increase k.</p></li>
</ol>

<p>This procedure would actually be horribly inefficient. In Octave, we will call svd:</p>

<pre>[U,S,V] = svd(Sigma)
</pre>

<p>Which gives us a matrix S. We can actually check for 99% of retained variance using the S matrix as follows:</p>

<p>\(\dfrac{\sum_{i=1}^kS_{ii}}{\sum_{i=1}^nS_{ii}} \geq 0.99\)</p>

<h3 id="toc_5">Advice for Applying PCA</h3>

<p>The most common use of PCA is to speed up supervised learning.</p>

<p>Given a training set with a large number of features (e.g. \(x^{(1)},\dots,x^{(m)} \in \mathbb{R}^{10000}\) ) we can use PCA to reduce the number of features in each example of the training set (e.g. \(z^{(1)},\dots,z^{(m)} \in \mathbb{R}^{1000}\)).</p>

<p>Note that we should define the PCA reduction from \(x^{(i)}\) to \(z^{(i)}\) only on the training set and not on the cross-validation or test sets. You can apply the mapping z(i) to your cross-validation and test sets after it is defined on the training set.</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/9/8</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html'>机器学习</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="batch_normalization.html">
                
                  <h1>Batch Normalization</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>The idea is relatively straightforward. Machine learning methods tend to work better when their input data consists of uncorrelated features with <strong>zero mean</strong> and <strong>unit variance</strong>. When training a neural network, we can preprocess the data before feeding it to the network to explicitly decorrelate its features; this will ensure that the first layer of the network sees data that follows a nice distribution. However even if we preprocess the input data, the activations at deeper layers of the network will likely no longer be decorrelated and will no longer have zero mean or unit variance since they are output from earlier layers in the network. Even worse, during the training process the distribution of features at each layer of the network will shift as the weights of each layer are updated.</p>

<h2 id="toc_0">Covariate Shift</h2>

<p>While stochastic gradient is simple and effective, it requires careful tuning of the model hyper-parameters, specifically the learning rate used in optimization, as well as the initial values for the model parameters. The train- ing is complicated by the fact that the inputs to each layer are affected by the parameters of all preceding layers – so that small changes to the network parameters amplify as the network becomes deeper.</p>

<p>The change in the distribution of layers&#39; inputs presents a problem because the layers need to continuously adapt to the new distribution. When the input distribution to a learning system changes, it is said to experience <code>covariate shift</code>. This is typically handled via domain adaption.</p>

<p><code>Internal Covariate Shift</code> refers to the change in the distribution of internal nodes of a deep network due to change in network parameters, in the course of training. <code>Batch Normalization</code>, that takes a step towards reducing internal covariance shift, and in doing so dramatically accelerates the training of deep neural nets. It accomplishes this via a normalization step that fixes the means and variances of layer inputs. It also has a beneficial effect on the dependence of gradients on the scale of the parameters or of their initial values. This allows us to use much higher learning rates without the risk of divergence. </p>

<h2 id="toc_1">Algorithm</h2>

<h3 id="toc_2">Forward</h3>

<p>Given some input values \(\mathcal{B} = x^{(1)},...,x^{(m)}\) over a mini-batch in the layer \(l\) of neural network; </p>

<p>\[\mu_{\mathcal{B}} = \frac{1}{m}\sum_i^m x_i  \text{     (mini-batch mean)}\\<br/>
\sigma^2=\frac{1}{m}\sum_i(x_i-\mu_{\mathcal{B}})^2  \text{     (mini-batch variance)}\\<br/>
\hat{x}_i=\frac{x_i-\mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{\mathcal{B}}}^2+\varepsilon}} \text{     (normalize)}\\<br/>
y_i=\gamma \hat{x}_i+\beta \text{     (scale and shift)}<br/>
\]</p>

<p>At each iteration, we update the running averages for mean and variance using an exponential decay based on the momentum parameter:</p>

<pre><code class="language-text">running_mean = momentum * running_mean + (1 - momentum) * sample_mean
running_var = momentum * running_var + (1 - momentum) * sample_var
</code></pre>

<h2 id="toc_3">Backward</h2>

<p><img src="media/15048405877529/Screen%20Shot%202017-09-08%20at%203.11.45%20PM.png" alt="batch-normalization-backward"/></p>

<h3 id="toc_4">Test time</h3>

<p>Using trained <code>runing_mean</code> and <code>running_var</code> to take forward step.</p>

<h2 id="toc_5">TensorFlow</h2>

<p>Applying <code>Batch Normalization</code> in TensorFlow Model is very convenient. Just add one line of code in TensorFlow: <code>tf.nn.batch-normalization</code>.</p>

<h2 id="toc_6">Reference</h2>

<p>Sergey Ioffe, Christian Szegedy. 2015. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. arXiv:<a href="https://arxiv.org/abs/1502.03167v3">1501.02167v3</a></p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/9/8</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Deep%20Learning.html'>Deep Learning</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="15046658203135.html">
                
                  <h1>TensorFlow Q</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<h3 id="toc_0">Q1: what&#39;s the difference between <code>tf.placeholder</code> and <code>tf.Variable</code></h3>

<p>In general, we use <code>tf.placeholder</code> to feed actual training examples, while using <code>tf.Variable</code> for parameters such as weights (\(W\)) and biases (\(b\)) for models.</p>

<p>With <code>tf.Variable</code>, we have to provide an initial value when declaring it. And we don&#39;t have to provide an initial value until running time with a <code>feed_dict</code>.</p>

<h3 id="toc_1">Q2: what&#39;s the difference between <code>tf.random_normal</code> and <code>tf.trucated_normal</code></h3>

<p><code>tf.trucated_normal</code> generates values following a normal distribution with specified mean and standard deviation, except that values whose magnitude is more than 2 standard deviations from the mean are dropped and re-picked.</p>

<pre><code class="language-text">import tensorflow as tf
import matplotlib.pyplot as plt
%matplotlib inline
Session = tf.InteractiveSession()
</code></pre>

<pre><code class="language-text">mean = 0.0
std = 1.0
shape = (10000,)
hist_range = (-5, 5)

A = tf.truncated_normal(shape, mean, std)
B = tf.random_normal(shape, mean, std)
a, b = Session.run([A, B])
</code></pre>

<pre><code class="language-text">f, (ax1, ax2) = plt.subplots(2,1)
f1 = ax1.hist(a, bins= 100, range=hist_range)
f2 = ax2.hist(b, bins= 100, range=hist_range)
</code></pre>

<p><img src="media/15046658203135/output_2_0.png" alt="output_2_0"/></p>

<h3 id="toc_2">Q3: What&#39;s the difference between <code>tf.Variable</code> and <code>tf.get_variable</code></h3>

<p><code>tf.get_variable</code> will make it way easier to refactor code if you need to share variables at any time.</p>

<pre><code class="language-python">import tensorflow as tf

with tf.variable_scope(&quot;scope1&quot;):
    w1 = tf.get_variable(&quot;w1&quot;, shape=[2,3])
    w2 = tf.Variable(1.0, name=&quot;w2&quot;)
with tf.variable_scope(&quot;scope1&quot;, reuse=True):
    w1_p = tf.get_variable(&quot;w1&quot;, shape=[2,3])
    w2_p = tf.Variable(1.0, name=&quot;w2&quot;)

print(w1 is w1_p, w2 is w2_p)

#Output: True  False
</code></pre>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/9/6</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Tensorflow.html'>Tensorflow</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="15046649572570.html">
                
                  <h1>Pandas</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>To convert a pandas <code>dataframe</code> (df) to a numpy <code>ndarray</code>, use this code:</p>

<pre><code class="language-text">df=df.values
</code></pre>

<p>df now becomes a numpy <code>ndarray</code>.</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/9/6</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Python%E7%89%B9%E6%80%A7.html'>Python特性</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="google-cloud-platform.html">
                
                  <h1>Google Cloud Platform</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>Google Cloud Platform enables developers to <strong>build</strong>, <strong>test</strong> and <strong>deploy</strong> applications on Google&#39;s <em>highly-scalable</em>, <em>secure</em>, and <em>reliable</em> infrastructure.</p>

<p>All Google Cloud Platform services are associated with a project that is used to:</p>

<ul>
<li>Track resource and quota usage</li>
<li>Enable billing</li>
<li>Manage permissions and credentials</li>
<li>Enable services and APIs</li>
</ul>

<h2 id="toc_0">VM Instances</h2>

<p>Run the following code to start or stop an compute instance called <code>instance-name</code>.</p>

<pre><code class="language-bash">gcloud compute instances start/stop instance-name
</code></pre>

<p>After starting your compute instance, you might need to access the instance in the zone which is <code>zone-name</code> here, via the terminal by <code>ssh</code> </p>

<pre><code class="language-bash">gcloud compute ssh instance-name --zone zone-name
</code></pre>

<h2 id="toc_1">Setup Environment</h2>

<p>Following the guide of <a href="http://cs231n.github.io/gce-tutorial/">CS231n</a>, using an off-the-shelf script <code>setup_googlecloud.sh</code>, and activate the environment created by the script</p>

<pre><code class="language-bash">source .env/bin/activate
</code></pre>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/9/4</span>
                    
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
              


			<div class="row">
			  <div class="large-6 columns">
			  <p class="text-left" style="padding-top:25px;">
			   <a href="all_9.html">&laquo; Prev Page</a>  
			  </p>
			  </div>
			  <div class="large-6 columns">
			<p class="text-right" style="padding-top:25px;">
			 <a href="all_11.html">&raquo; Next Page</a> 
			</p>
			  </div>
			</div>
		</div>
	</div><!-- large 8 -->

 <div class="large-4 medium-4 columns">
  <div class="hide-for-small">
    <div id="sidebar" class="sidebar">
          <div id="site-info" class="site-info">
            
                <div class="site-a-logo"><img src="http://or9a8nskt.bkt.clouddn.com/figure.jpeg" /></div>
            
                <h1>techlarry</h1>
                <div class="site-des">他山之石，可以攻玉</div>
                <div class="social">









<a target="_blank" class="github" target="_blank" href="https://github.com/techlarry" title="GitHub">GitHub</a>
<a target="_blank" class="email" href="mailto:wang.zhen.hua.larry@gmail.com" title="Email">Email</a>
  <a target="_blank" class="rss" href="atom.xml" title="RSS">RSS</a>
                
              	 </div>
          	</div>

             

              <div id="site-categories" class="side-item ">
                <div class="side-header">
                  <h2>Categories</h2>
                </div>
                <div class="side-content">

      	<p class="cat-list">
        
            <a href="Leetcode.html"><strong>Leetcode</strong></a>
        
            <a href="C/C++.html"><strong>C/C++</strong></a>
        
            <a href="Python%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95.html"><strong>Python数据结构与算法</strong></a>
        
            <a href="Course.html"><strong>Course</strong></a>
        
            <a href="Python%E7%89%B9%E6%80%A7.html"><strong>Python特性</strong></a>
        
            <a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html"><strong>机器学习</strong></a>
        
            <a href="Python%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E4%B8%89%E7%BB%B4%E5%8F%AF%E8%A7%86%E5%8C%96.html"><strong>Python科学计算三维可视化</strong></a>
        
            <a href="English.html"><strong>English</strong></a>
        
            <a href="Computer%20System.html"><strong>Computer System</strong></a>
        
            <a href="Deep%20Learning.html"><strong>Deep Learning</strong></a>
        
            <a href="Linux%20%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B.html"><strong>Linux 系统编程</strong></a>
        
            <a href="%E6%95%B0%E6%8D%AE%E5%BA%93.html"><strong>数据库</strong></a>
        
            <a href="Tensorflow.html"><strong>Tensorflow</strong></a>
        
            <a href="Big%20Data.html"><strong>Big Data</strong></a>
        
            <a href="%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB.html"><strong>文献阅读</strong></a>
        
            <a href="Tools.html"><strong>Tools</strong></a>
         
        </p>


                </div>
              </div>

              <div id="site-categories" class="side-item">
                <div class="side-header">
                  <h2>Recent Posts</h2>
                </div>
                <div class="side-content">
                <ul class="posts-list">
	      
		      
			      <li class="post">
			        <a href="15312166852240.html">CSE 421/521 Operating Systems</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15272289883847.html">回溯法和深度优先遍历</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="Coursera_notebook_jupyter.html">技巧：Coursera下载Jupyter notebook相关文件</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15171345711813.html">用于训练的深度学习硬件</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15166008990128.html">网络编程</a>
			      </li>
		     
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		   
		  		</ul>
                </div>
              </div>
        </div><!-- sidebar -->
      </div><!-- hide for small -->
</div><!-- large 4 -->

</div><!-- row -->

 <div class="page-bottom clearfix">
  <div class="row">
   <p class="copyright">Copyright &copy; 2015
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
</div>

        </section>
      </div>
    </div>

  
    

    <script src="asset/js/foundation.min.js"></script>
    <script>
      $(document).foundation();
      function fixSidebarHeight(){
        var w1 = $('.markdown-body').height();
          var w2 = $('#sidebar').height();
          if (w1 > w2) { $('#sidebar').height(w1); };
      }
      $(function(){
        fixSidebarHeight();
      })
      $(window).load(function(){
          fixSidebarHeight();
      });
     
    </script>

    
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>


  </body>
</html>
