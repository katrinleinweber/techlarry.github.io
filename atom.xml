<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[techlarry]]></title>
  <link href="http://larryim.cc/atom.xml" rel="self"/>
  <link href="http://larryim.cc/"/>
  <updated>2017-08-22T12:10:15+08:00</updated>
  <id>http://larryim.cc/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im/">MWeb</generator>
  
  <entry>
    <title type="html"><![CDATA[Getting started with TensorFlow]]></title>
    <link href="http://larryim.cc/getting_started_with_tensorflow.html"/>
    <updated>2017-08-22T11:53:38+08:00</updated>
    <id>http://larryim.cc/getting_started_with_tensorflow.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">TensorFlow Core tutorial</a>
<ul>
<li>
<a href="#toc_1">importing TensorFlow</a>
</li>
<li>
<a href="#toc_2">The Computational Graph</a>
</li>
</ul>
</li>
<li>
<a href="#toc_3">tf.train API</a>
<ul>
<li>
<a href="#toc_4">Complete program</a>
</li>
</ul>
</li>
<li>
<a href="#toc_5">tf.estimator</a>
<ul>
<li>
<a href="#toc_6">Basic usage</a>
</li>
</ul>
</li>
</ul>


<h2 id="toc_0">TensorFlow Core tutorial</h2>

<h3 id="toc_1">importing TensorFlow</h3>

<pre><code class="language-python">import tensorflow as tf
</code></pre>

<h3 id="toc_2">The Computational Graph</h3>

<p>A <code>computational graph</code> is a series of TensorFlow operations arranged into a graph of nodes. Let&#39;s build a simple computational graph. Each node takes zero or more tensors as inputs and produces a tensor as an output. One type of node is a constant. Like all TensorFlow constant, it takes no inputs, and it outputs a value it stores internally. We can create two floating point Tensor <code>node1</code> and <code>node2</code> as follows:</p>

<pre><code class="language-python">node1 = tf.constant(3.0, dtype=tf.float32)
node2 = tf.constant(4.0) # also tf.float32 implicitly
print(node1, node2)
</code></pre>

<pre><code>Tensor(&quot;Const:0&quot;, shape=(), dtype=float32)
Tensor(&quot;Const_1:0&quot;, shape=(), dtype=float32)
</code></pre>

<p>To produce output values for <code>node1</code> and <code>nodes2</code>, evaluation is needed. To actually evaluate the nodes, we must run the computational graph within a <code>session</code>. A session encapsulates the control and state of the Tensorflow runtime.</p>

<p>The following code creates a <code>Session</code> object and then invokes its <code>run</code> method to run enough of the computational graph to evaluate <code>node1</code> and <code>node2</code>. By running the computational graph in a session as follows:</p>

<pre><code class="language-python">sess = tf.Session()
print(sess.run([node1, node2]))
</code></pre>

<pre><code>[3.0, 4.0]
</code></pre>

<p>We can build more complicated computations by combining <code>Tensor</code> nodes with operations(Operations are also nodes). For example, we can add our two constant nodes and produce a new graph as follows:</p>

<pre><code class="language-python">node3 = tf.add(node1, node2)
print(&#39;node3:&#39;, node3)
print(&#39;sess.run(node3):&#39;, sess.run(node3))
</code></pre>

<pre><code>node3: Tensor(&quot;Add:0&quot;, shape=(), dtype=float32)
sess.run(node3): 7.0
</code></pre>

<p>TensorFlow provides a utility called <code>TensorBoard</code> that can display a picture of the computational graph. Here is a screenshot showing how <code>TensorBoard</code> visualizes the graph:<br/>
<img src="media/15033740189206/getting_started_add.png" alt=""/></p>

<p>As it stands, this graph is not especially interesting because it always produces a constant result. A graph can be parameterized to accept external inputs, known as <code>placeholders</code>. A <code>placeholder</code> is a promise to provide a value later.</p>

<pre><code class="language-python">a = tf.placeholder(tf.float32)
b = tf.placeholder(tf.float32)
adder_node = a + b # + provides a shortcut for tf.add(a,b)
</code></pre>

<p>The preceding three lines are a bit like a function or a lambda in which we define two input parameters (a and b) and then an operation on them. We can evaluate this graph with multiple inputs by using the feed_dict argument to the <code>run method</code> to feed concrete values to the placeholder:</p>

<pre><code class="language-python">print(sess.run(adder_node, {a:3, b:4.5}))
print(sess.run(adder_node, {a:[1,3], b:[2,4]}))
</code></pre>

<pre><code>7.5
[ 3.  7.]
</code></pre>

<p>In <code>TensorBoard</code>, the graph looks like this:</p>

<p><img src="media/15033740189206/getting_started_adder.png" alt=""/></p>

<p>We can make the computational graph more complex by adding another operation. For example,</p>

<pre><code class="language-python">add_and_triple = adder_node * 3.
print(sess.run(add_and_triple,{a:3, b:4.5}))
</code></pre>

<pre><code>22.5
</code></pre>

<p>The preceding computational graph would look as follows in <code>TensorBoard</code>:<br/>
<img src="media/15033740189206/getting_started_triple.png" alt=""/></p>

<p>In machine learning we will typically want a model that can take arbitrary inputs, such as the one above. To make the model trainable, we need to be able to modify the graph to get new outputs with the same input. <code>variables</code> allow us to add trainable parameters to a graph. They are constructed with a type and initial value:</p>

<pre><code class="language-python">W = tf.Variable([.3], dtype=tf.float32)
b = tf.Variable([-.3], dtype=tf.float32)
x = tf.placeholder(tf.float32)
linear_model = W*x +b 
</code></pre>

<p>Constants are initialized when you call <code>tf.constant</code>, and their value can never change. By contrast, variables are not initialized when you call <code>tf.Variable</code>. To initialize all the variables in a TensorFlow program, you must explicitly call a special operation as follows:</p>

<pre><code class="language-python">init = tf.global_variables_initializer()
sess.run(init)
</code></pre>

<p>It is important to realize <code>init</code> is a handle to the TensorFlow sub-graph that initializes all the global variables. Until we call <code>sess.run</code>, the variables are uninitialized.</p>

<p>Since <code>x</code> is a placeholder, we can evaluate <code>linear_model</code> for several values of <code>x</code> simultaneously as follows:</p>

<pre><code class="language-python">print(sess.run(linear_model,{x:[1,2,3,4]}))
</code></pre>

<pre><code>[ 0.          0.30000001  0.60000002  0.90000004]
</code></pre>

<p>We&#39;ve create a model, but we don&#39;t know how good it is yet. To evaluate the model on training data, we need a <code>y</code> placeholder to provide the desired values, and we need to write a loss function.</p>

<p>A <code>loss function</code> measures how far apart the current model is from the provided data. We&#39;ll use a standard loss model for linear regression, which sums the squares of the deltas between the current model and the provided data. <code>linear_model -y</code> creates a vector where each element is the corresponding example&#39;s error delta. We call <code>tf.square</code> to square that error. Then, we sum all the squared errors to create a single scalar that abstacts the error of all examples using <code>tf.reduce_sum</code>:</p>

<pre><code class="language-python">y = tf.placeholder(tf.float32)
squared_deltas = tf.square(linear_model - y)
loss = tf.reduce_sum(squared_deltas)
print(sess.run(loss, {x:[1,2,3,4], y:[0,-1,-2,-3]}))
</code></pre>

<pre><code>23.66
</code></pre>

<p>We could improve this manually by reassigning the values of <code>W</code> and <code>b</code> to the perfect values of -1 and 1. A variable is initialized to the value provided to <code>tf.Variable</code> but can be changed using operations like <code>tf.assign</code>. FOr example, <code>W=-1</code> and <code>b=1</code> are the optimal parameters for our model. We can change <code>W</code> and <code>b</code> accordingly:</p>

<pre><code class="language-python">fixW = tf.assign(W, [-1.])
fixb = tf.assign(b, [1.])
sess.run([fixW, fixb])
print(sess.run(loss, {x:[1,2,3,4], y:[0,-1,-2,-3]}))
</code></pre>

<pre><code>0.0
</code></pre>

<p>We guessed the &quot;perfect&quot; values of <code>W</code> and <code>b</code>, but the whole point of machine learning is to find the correct model parameters automatically. We will show how to accomplish this in the next section.</p>

<h2 id="toc_3">tf.train API</h2>

<p>A complete discussion of machine learning is out of the scope of this tutorial. However, TensorFlow provides <code>optimizers</code> that slowly change each variable in order to minimize the loss function. The simplest optimizer is <code>gradient descent</code>. It modifies each variable according to the magnitude of the derivative of loss with respect to that variable. In general, computing symbolic derivatives manually is tedious and error-prone. Consequently, <code>TensorFlow</code> can automatically produce derivatives given only a description of the model using the function <code>tf.gradients</code>. For simplicity, <code>optimizers</code> typically do this for you. For example,</p>

<pre><code class="language-python">optimizer = tf.train.GradientDescentOptimizer(0.01)
train = optimizer.minimize(loss)

sess.run(init) # reset values to incorrect defaults
for i in range(1000):
    sess.run(train, {x:[1,2,3,4], y:[0,-1,-2,-3]})
print(sess.run([W,b]))
</code></pre>

<pre><code>[array([-0.9999969], dtype=float32), array([ 0.99999082], dtype=float32)]
</code></pre>

<p>Now we have done actual machine learning! Although doing this simple linear regression doesn&#39;t require much TensorFlow core code, more complicated models and methods to feed data into your model necessitate more code. Thus TensorFlow provides higher level abstractions for common patterns, structures, and functionality. We will learn how to use some of these abstractions in the next section.</p>

<h3 id="toc_4">Complete program</h3>

<pre><code class="language-python">import tensorflow as tf

# Model parameters
W = tf.Variable([.3], dtype=tf.float32)
b = tf.Variable([-.3], dtype=tf.float32)
# Model input and output
x = tf.placeholder(tf.float32)
linear_model = W * x + b
y = tf.placeholder(tf.float32)

# loss
loss = tf.reduce_sum(tf.square(linear_model - y)) # sum of the squares
# optimizer
optimizer = tf.train.GradientDescentOptimizer(0.01)
train = optimizer.minimize(loss)

# training data
x_train = [1, 2, 3, 4]
y_train = [0, -1, -2, -3]
# training loop
init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init) # reset values to wrong
for i in range(1000):
  sess.run(train, {x: x_train, y: y_train})

# evaluate training accuracy
curr_W, curr_b, curr_loss = sess.run([W, b, loss], {x: x_train, y: y_train})
print(&quot;W: %s b: %s loss: %s&quot;%(curr_W, curr_b, curr_loss))
</code></pre>

<pre><code>W: [-0.9999969] b: [ 0.99999082] loss: 5.69997e-11
</code></pre>

<p>This more complicated program can still be visualized in TensorBoard:</p>

<p><img src="media/15033740189206/getting_started_final.png" alt=""/></p>

<h2 id="toc_5">tf.estimator</h2>

<p><code>tf.estimator</code> is a high-level TensorFlow library that simplifies the mechanics of machine learning, including the following:</p>

<ul>
<li>running training loops</li>
<li>running evaluation loops</li>
<li>managing data sets</li>
<li><code>tf.estimator</code> defines many common models.</li>
</ul>

<h3 id="toc_6">Basic usage</h3>

<p>Notice how much simpler the linear regression program becomes with <code>tf.estimator</code>:</p>

<pre><code class="language-python"># NumPy is often used to load, manipulate and preprocess data.
import numpy as np

# Declare list of features. We only have one numeric feature. There are many
# other types of columns that are more complicated and useful.
feature_columns = [tf.feature_column.numeric_column(&quot;x&quot;, shape=[1])]

# An estimator is the front end to invoke training (fitting) and evaluation
# (inference). There are many predefined types like linear regression,
# linear classification, and many neural network classifiers and regressors.
# The following code provides an estimator that does linear regression.
estimator = tf.estimator.LinearRegressor(feature_columns=feature_columns)

# TensorFlow provides many helper methods to read and set up data sets.
# Here we use two data sets: one for training and one for evaluation
# We have to tell the function how many batches
# of data (num_epochs) we want and how big each batch should be.
x_train = np.array([1., 2., 3., 4.])
y_train = np.array([0., -1., -2., -3.])
x_eval = np.array([2., 5., 8., 1.])
y_eval = np.array([-1.01, -4.1, -7, 0.])
input_fn = tf.estimator.inputs.numpy_input_fn(
    {&quot;x&quot;: x_train}, y_train, batch_size=4, num_epochs=None, shuffle=True)
train_input_fn = tf.estimator.inputs.numpy_input_fn(
    {&quot;x&quot;: x_train}, y_train, batch_size=4, num_epochs=1000, shuffle=False)
eval_input_fn = tf.estimator.inputs.numpy_input_fn(
    {&quot;x&quot;: x_eval}, y_eval, batch_size=4, num_epochs=1000, shuffle=False)

# We can invoke 1000 training steps by invoking the  method and passing the
# training data set.
estimator.train(input_fn=input_fn, steps=1000)

# Here we evaluate how well our model did.
train_metrics = estimator.evaluate(input_fn=train_input_fn)
eval_metrics = estimator.evaluate(input_fn=eval_input_fn)
print(&quot;train metrics: %r&quot;% train_metrics)
print(&quot;eval metrics: %r&quot;% eval_metrics)
</code></pre>

<pre><code>INFO:tensorflow:Using default config.
WARNING:tensorflow:Using temporary folder as model directory: /var/folders/66/y1hc77j572v71r0gm2r39rfr0000gn/T/tmp2rnith_a
INFO:tensorflow:Using config: {&#39;_model_dir&#39;: &#39;/var/folders/66/y1hc77j572v71r0gm2r39rfr0000gn/T/tmp2rnith_a&#39;, &#39;_tf_random_seed&#39;: 1, &#39;_save_summary_steps&#39;: 100, &#39;_save_checkpoints_secs&#39;: 600, &#39;_save_checkpoints_steps&#39;: None, &#39;_session_config&#39;: None, &#39;_keep_checkpoint_max&#39;: 5, &#39;_keep_checkpoint_every_n_hours&#39;: 10000, &#39;_log_step_count_steps&#39;: 100}
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Saving checkpoints for 1 into /var/folders/66/y1hc77j572v71r0gm2r39rfr0000gn/T/tmp2rnith_a/model.ckpt.
INFO:tensorflow:loss = 19.0, step = 1
INFO:tensorflow:global_step/sec: 592.325
INFO:tensorflow:loss = 0.192443, step = 101 (0.171 sec)
INFO:tensorflow:global_step/sec: 710.314
INFO:tensorflow:loss = 0.0370785, step = 201 (0.141 sec)
INFO:tensorflow:global_step/sec: 669.791
INFO:tensorflow:loss = 0.0173565, step = 301 (0.150 sec)
INFO:tensorflow:global_step/sec: 729.609
INFO:tensorflow:loss = 0.00361388, step = 401 (0.136 sec)
INFO:tensorflow:global_step/sec: 814
INFO:tensorflow:loss = 0.000215951, step = 501 (0.123 sec)
INFO:tensorflow:global_step/sec: 793.172
INFO:tensorflow:loss = 0.0001734, step = 601 (0.127 sec)
INFO:tensorflow:global_step/sec: 776.415
INFO:tensorflow:loss = 3.66416e-05, step = 701 (0.128 sec)
INFO:tensorflow:global_step/sec: 845.781
INFO:tensorflow:loss = 3.03422e-06, step = 801 (0.118 sec)
INFO:tensorflow:global_step/sec: 849.689
INFO:tensorflow:loss = 1.18453e-06, step = 901 (0.118 sec)
INFO:tensorflow:Saving checkpoints for 1000 into /var/folders/66/y1hc77j572v71r0gm2r39rfr0000gn/T/tmp2rnith_a/model.ckpt.
INFO:tensorflow:Loss for final step: 3.72255e-07.
INFO:tensorflow:Starting evaluation at 2017-08-21-05:57:38
INFO:tensorflow:Restoring parameters from /var/folders/66/y1hc77j572v71r0gm2r39rfr0000gn/T/tmp2rnith_a/model.ckpt-1000
INFO:tensorflow:Finished evaluation at 2017-08-21-05:57:40
INFO:tensorflow:Saving dict for global step 1000: average_loss = 6.05797e-08, global_step = 1000, loss = 2.42319e-07
INFO:tensorflow:Starting evaluation at 2017-08-21-05:57:40
INFO:tensorflow:Restoring parameters from /var/folders/66/y1hc77j572v71r0gm2r39rfr0000gn/T/tmp2rnith_a/model.ckpt-1000
INFO:tensorflow:Finished evaluation at 2017-08-21-05:57:42
INFO:tensorflow:Saving dict for global step 1000: average_loss = 0.00254753, global_step = 1000, loss = 0.0101901
train metrics: {&#39;average_loss&#39;: 6.057968e-08, &#39;loss&#39;: 2.4231872e-07, &#39;global_step&#39;: 1000}
eval metrics: {&#39;average_loss&#39;: 0.0025475256, &#39;loss&#39;: 0.010190102, &#39;global_step&#39;: 1000}
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What is a tensorflow session?]]></title>
    <link href="http://larryim.cc/what_is_a_tensorflow_session.html"/>
    <updated>2017-08-22T11:52:07+08:00</updated>
    <id>http://larryim.cc/what_is_a_tensorflow_session.html</id>
    <content type="html"><![CDATA[
<p>by <a href="http://danijar.com/what-is-a-tensorflow-session/">Danijar Hafner</a>, modified by <a href="http://larryim.cc">larry</a></p>

<p>I’ve seen a lot of confusion over the rules of <code>tf.Graph</code> and <code>tf.Session</code> in TensorFlow. It’s simple:</p>

<ul>
<li>A <code>graph</code> defines the computation. It doesn’t compute anything, it doesn’t hold any values, it just defines the operations that you specified in your code.</li>
<li>A <code>session</code> allows you to execute graphs or part of graphs. It allocates resources (on one or more machines) for that and holds the actual values of intermediate results and variables.</li>
</ul>

<p>Let’s look at an example.</p>

<h3 id="toc_0">Defining the Graph</h3>

<p>We define a graph with a variable and three operations: <code>x</code> returns the current value of our variable. <code>init</code> assigns the initial value of 42 to that variable. <code>x_assign</code> assigns the new value of 13 to that variable.</p>

<pre><code class="language-python">import tensorflow as tf
graph = tf.Graph()
with graph.as_default():
  x = tf.Variable(42, name=&#39;foo&#39;)
  init = tf.global_variables_initializer()
  x_assign = x.assign(13)
</code></pre>

<p>On a side note: TensorFlow creates a default graph for you, so we don’t need the first two lines of the code above. The default graph is also what the sessions in the next section use when not manually specifying a graph.</p>

<h3 id="toc_1">Running Computations in a Session</h3>

<p>To run any of the three defined operations, we need to create a session for that graph. The session will also allocate memory to store the current value of the variable.</p>

<pre><code class="language-python">with tf.Session(graph=graph) as sess:
  sess.run(init)
  sess.run(x_assign)
  print(sess.run(x))
</code></pre>

<pre><code>13
</code></pre>

<p>As you can see, the value of our variable is only valid within one session. If we try to query the value afterwards in a second session, TensorFlow will raise an error because the variable is not initialized there.</p>

<pre><code class="language-python">with tf.Session(graph=graph) as sess:
  print(sess.run(x))
# Error: Attempting to use uninitialized value foo

</code></pre>

<p>Of course, we can use the graph in more than one session, we just have to initialize the variables again. The values in the new session will be completely independent from the first one:</p>

<pre><code class="language-python">with tf.Session(graph=graph) as sess:
  sess.run(init)
  print(sess.run(x))
</code></pre>

<pre><code>42
</code></pre>

<p>Hopefully this short workthrough helped you to better understand <code>tf.Session</code>. </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Bit manipulation (2)]]></title>
    <link href="http://larryim.cc/15029400727762.html"/>
    <updated>2017-08-17T11:21:12+08:00</updated>
    <id>http://larryim.cc/15029400727762.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">下面列举了一些常见的二进制位的变换操作。</h2>

<table>
<thead>
<tr>
<th>功能</th>
<th>示例</th>
<th>位运算</th>
</tr>
</thead>

<tbody>
<tr>
<td>去掉最后一位</td>
<td>(101101-&gt;10110)</td>
<td>x &gt;&gt; 1</td>
</tr>
<tr>
<td>在最后加一个0</td>
<td>(101101-&gt;1011010)</td>
<td>x &gt;&gt; 1</td>
</tr>
<tr>
<td>在最后加一个1</td>
<td>(101101-&gt;1011011)</td>
<td>x &lt;&lt; 1+1</td>
</tr>
<tr>
<td>把最后一位变成1</td>
<td>(101100-&gt;101101)</td>
<td>x or 1</td>
</tr>
<tr>
<td>把最后一位变成0</td>
<td>(101101-&gt;101100)</td>
<td>x or 1-1</td>
</tr>
<tr>
<td>最后一位取反</td>
<td>(101101-&gt;101100)</td>
<td>x xor 1</td>
</tr>
<tr>
<td>把右数第k位变成1</td>
<td>(101001-&gt;101101,k=3)</td>
<td>x or (1 &lt;&lt; (k-1))</td>
</tr>
<tr>
<td>把右数第k位变成0</td>
<td>(101101-&gt;101001,k=3)</td>
<td>x and not (1 &lt;&lt; (k-1))</td>
</tr>
<tr>
<td>右数第k位取反</td>
<td>(101001-&gt;101101,k=3)</td>
<td>x xor (1 &lt;&lt; (k-1))</td>
</tr>
<tr>
<td>取末三位</td>
<td>(1101101-&gt;101)</td>
<td>x and 7</td>
</tr>
<tr>
<td>取末k位</td>
<td>(1101101-&gt;1101,k=5)</td>
<td>x and (1 &lt;&lt; k-1)</td>
</tr>
<tr>
<td>取右数第k位</td>
<td>(1101101-&gt;1,k=4)</td>
<td>x &gt;&gt; (k-1) and 1</td>
</tr>
<tr>
<td>把末k位变成1</td>
<td>(101001-&gt;101111,k=4)</td>
<td>x or (1 &lt;&lt; k-1)</td>
</tr>
<tr>
<td>末k位取反</td>
<td>(101001-&gt;100110,k=4)</td>
<td>x xor (1 &lt;&lt; k-1)</td>
</tr>
<tr>
<td>把右边连续的1变成0</td>
<td>(100101111-&gt;100100000)</td>
<td>x and (x+1)</td>
</tr>
<tr>
<td>把右起第一个0变成1</td>
<td>(100101111-&gt;100111111)</td>
<td>x or (x+1)</td>
</tr>
<tr>
<td>把右边连续的0变成1</td>
<td>(11011000-&gt;11011111)</td>
<td>x or (x-1)</td>
</tr>
<tr>
<td>取右边连续的1</td>
<td>(100101111-&gt;1111)</td>
<td>(x xor (x+1)) &gt;&gt; 1</td>
</tr>
<tr>
<td>去掉右起第一个1的左边</td>
<td>(100101000-&gt;1000)</td>
<td>x and (x xor (x-1))</td>
</tr>
</tbody>
</table>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Bit manipulation]]></title>
    <link href="http://larryim.cc/15028888588964.html"/>
    <updated>2017-08-16T21:07:38+08:00</updated>
    <id>http://larryim.cc/15028888588964.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">136. Single Number</h2>

<p><a href="https://leetcode.com/problems/single-number/description/">LeetCode Single Number</a></p>

<p>Given an array of integers, every element appears <em>twice</em> except for one. Find that single one.</p>

<p><strong>Note</strong>:<br/><br/>
Your algorithm should have a linear runtime complexity. Could you implement it without using extra memory?</p>

<p><code>^</code>(<code>xor</code>, <code>Exclusive or</code>):  outputs true whenever the inputs differ:</p>

<pre><code class="language-python">0 ^ 0 = 0 
n ^ 0 = n 
n ^ n = 0
</code></pre>

<p><strong>Solution</strong>:</p>

<pre><code class="language-python">class Solution(object):
    def singleNumber(self, nums):
        &quot;&quot;&quot;
        :type nums: List[int]
        :rtype: int
        &quot;&quot;&quot;
        n = 0
        for num in nums:
            n = n^num
        return n     
</code></pre>

<h2 id="toc_1">137 Single Number II</h2>

<p>Given an array of integers, every element appears <em>three</em> times except for one, which appears exactly once. Find that single one.</p>

<p><strong>Note</strong>:<br/>
Your algorithm should have a linear runtime complexity. Could you implement it without using extra memory?</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Thread]]></title>
    <link href="http://larryim.cc/15028699473698.html"/>
    <updated>2017-08-16T15:52:27+08:00</updated>
    <id>http://larryim.cc/15028699473698.html</id>
    <content type="html"><![CDATA[

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Process]]></title>
    <link href="http://larryim.cc/15028699346565.html"/>
    <updated>2017-08-16T15:52:14+08:00</updated>
    <id>http://larryim.cc/15028699346565.html</id>
    <content type="html"><![CDATA[

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python GIL]]></title>
    <link href="http://larryim.cc/Python%E5%85%A8%E5%B1%80%E8%A7%A3%E9%87%8A%E5%99%A8%E9%94%81GIL.html"/>
    <updated>2017-08-16T15:31:02+08:00</updated>
    <id>http://larryim.cc/Python%E5%85%A8%E5%B1%80%E8%A7%A3%E9%87%8A%E5%99%A8%E9%94%81GIL.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">An example</a>
</li>
<li>
<a href="#toc_1">Introduction</a>
<ul>
<li>
<a href="#toc_2">Visualization:</a>
</li>
<li>
<a href="#toc_3">Cooperative Multitasking</a>
</li>
<li>
<a href="#toc_4">Preemptive Multitasking</a>
</li>
</ul>
</li>
<li>
<a href="#toc_5">解决办法</a>
<ul>
<li>
<a href="#toc_6">用multiprocessing替代Thread</a>
</li>
<li>
<a href="#toc_7">利用<code>ctypes</code>绕过<code>GIL</code></a>
</li>
</ul>
</li>
<li>
<a href="#toc_8">Reference</a>
</li>
</ul>


<h2 id="toc_0">An example</h2>

<p>In the following Python program, it seems that the program may reach 100% CPU usuage. In fact, it takes only 50% of CPU resources.</p>

<pre><code class="language-python">import threading


# 子线程死循环
def test():
    while True:
        pass


t1 = threading.Thread(target=test)
t1.start()

# 主线程死循环
while True:
    pass
</code></pre>

<h2 id="toc_1">Introduction</h2>

<p>Python全局解释器锁(<code>Global Interpreter Lock</code>)是用于同步线程的一种机制，它使得任何时刻仅有一个线程在执行。上面例子中虽然两个线程是死循环，而且有两个物理CPU内核，但因为 <code>GIL</code>的限制，两个线程只是做着分时切换，总的CPU占用率还略低于50％。</p>

<p><strong>Note: One thread runs Python, while N others sleep or await I/O</strong></p>

<h3 id="toc_2">Visualization:</h3>

<p>All of those red regions indicate times where the operating system has scheduled a Python thread on one of the cores, but it can&#39;t run because the thread on the other core is holding it.</p>

<p><img src="media/15028686628076/15028730462252.png" alt="demo for GIL"/><br/>
<img src="media/15028686628076/15028733300914.png" alt=""/></p>

<h3 id="toc_3">Cooperative Multitasking</h3>

<p>When it begins a task, such as network I/O, that is of long or uncertain duration and does not require running any Python code, a thread relinquishes the <code>GIL</code> so another thread can take it and run Python. This polite conduct is called <code>cooperative multitasking</code>(协同式多任务处理), and it allows concurrency; many threads can wait for different events at the same time.</p>

<p>For <code>cooperative multitasking</code>, processes voluntarily yield control periodically or when idle in order to enable multiple applications to be run simultaneously. All programs must cooperate for the entire scheduling scheme to work.</p>

<p>Say that two threads each connect a <code>socket</code>:</p>

<pre><code class="language-python">def do_connect():
    s = socket.socket()
    s.connect((&#39;python.org&#39;, 80))  # drop the GIL

for i in range(2):
    t = threading.Thread(target=do_connect)
    t.start()
</code></pre>

<p>Only one of these two threads can execute Python at a time, but once the thread has begun connecting, it drops the GIL so the other thread can run. This means that both threads could be waiting for their sockets to connect concurrently, which is a good thing. They can do more work in the same amount of time.</p>

<p>Let&#39;s try to open the box and see how a Python thread actually drops the GIL while it waits for a connection to be established, in <code>socketmodule.c</code>:</p>

<pre><code class="language-C">/* s.connect((host, port)) method */
static PyObject *
sock_connect(PySocketSockObject *s, PyObject *addro)
{
    sock_addr_t addrbuf;
    int addrlen;
    int res;

    /* convert (host, port) tuple to C address */
    getsockaddrarg(s, addro, SAS2SA(&amp;addrbuf), &amp;addrlen);

    Py_BEGIN_ALLOW_THREADS
    res = connect(s-&gt;sock_fd, addr, addrlen);
    Py_END_ALLOW_THREADS

    /* error handling and so on .... */
}
</code></pre>

<p>The <code>Py_BEGIN_ALLOW_THREADS</code> macro is where the thread drops the <code>GIL</code>; it is defined simply as:</p>

<pre><code>PyThread_release_lock(interpreter_lock);
</code></pre>

<p>And of course <code>Py_END_ALLOW_THREADS</code> reacquires the lock. A thread might block at this spot, waiting for another thread to release the lock; once that happens, the waiting thread grabs the GIL back and resumes executing your Python code. In short: While N threads are blocked on network I/O or waiting to reacquire the <code>GIL</code>, one thread can run Python.</p>

<p>Let&#39;s contrast cooperative multitasking with the other kind of multitasking.</p>

<h3 id="toc_4">Preemptive Multitasking</h3>

<p>A Python thread can voluntarily release the <code>GIL,</code> but it can also have the GIL seized from it preemptively(<code>Preemptive multitasking</code>, 抢占式多任务处理).</p>

<p>Let&#39;s back up and talk about how Python is executed. Your program is run in two stages. First, your Python program is compiled into a simpler binary format called <code>bytecode</code>. Second, the Python interpreter&#39;s main loop, a function mellifluously named <code>PyEval_EvalFrameEx()</code>, reads the <code>bytecode</code> and executes the instructions in it one by one.</p>

<p>While the interpreter steps through your<code>bytecode</code> it periodically drops the <code>GIL</code>, without asking permission of the thread whose code it is executing, so other threads can run:</p>

<pre><code class="language-C">for (;;) {
    if (--ticker &lt; 0) {
        ticker = check_interval;
    
        /* Give another thread a chance */
        PyThread_release_lock(interpreter_lock);
    
        /* Other threads may run now */
    
        PyThread_acquire_lock(interpreter_lock, 1);
    }

    bytecode = *next_instr++;
    switch (bytecode) {
        /* execute the next instruction ... */ 
    }
}
</code></pre>

<p>By default the check interval is 1000 <code>bytecodes</code>. All threads run this same code and have the lock taken from them periodically in the same way. In Python 3 the GIL&#39;s implementation is more complex, and the check interval is not a fixed number of <code>bytecodes</code>, but 15 milliseconds. For your code, however, these differences are not significant.</p>

<h2 id="toc_5">解决办法</h2>

<h3 id="toc_6">用multiprocessing替代Thread</h3>

<p>利用<code>multiprocessing</code>模块，可以很方便的处理。但进程会增加程序实现时线程间数据通讯和同步的困难。</p>

<h3 id="toc_7">利用<code>ctypes</code>绕过<code>GIL</code></h3>

<p><code>ctypes</code>可以让Python 接调用任意的C动态库的导出函数。</p>

<pre><code class="language-python">from ctypes import *
from threading import Thread

#加载动态库
lib = cdll.LoadLibrary(&quot;./libdeadloop.so&quot;)

#创建一个子线程，让其执行ｃ语言编写的函数，此函数是一个死循环
t = Thread(target=lib.DeadLoop)
t.start()

#主线程，也调用ｃ语言编写的那个死循环函数
#lib.DeadLoop()

while True:
    pass
</code></pre>

<h2 id="toc_8">Reference</h2>

<ul>
<li><a href="https://opensource.com/article/17/4/grok-gil">Grok the GIL: How to write fast and thread-safe Python</a></li>
<li><a href="http://www.dabeaz.com/GIL/">Understanding the Python GIL, David Beazley</a></li>
<li><a href="https://en.wikipedia.org/wiki/Cooperative_multitasking">Cooperative multitasking WEKIPEDIA</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python and MySQL]]></title>
    <link href="http://larryim.cc/15026944183320.html"/>
    <updated>2017-08-14T15:06:58+08:00</updated>
    <id>http://larryim.cc/15026944183320.html</id>
    <content type="html"><![CDATA[
<p>There are a lot of python driver available for MySQL and two stand out the most. The one, traditionally everybody’s choice, sort of industrial standard <code>MySQLdb</code>. It uses a C module to link to MySQL’s client library.  For Python3, use <code>PyMySQL</code> instead, because <code>MySQLdb</code> don&#39;t support Python3. Oracle’s <code>mysql-connector</code> on the other hand is pure python so no MySQL libraries and no compilation is necessary.</p>

<h2 id="toc_0">MySQL Connector/Python</h2>

<p>To use Python with MySQL, you can use the MySQL Connector/Python (<a href="https://dev.mysql.com/downloads/connector/python/">Download Here</a>). For those with anaconda, just install with <code>conda</code>:</p>

<pre><code>conda install -c anaconda mysql-connector-python 
</code></pre>

<p>MySQL Connector/Python includes support for:</p>

<ul>
<li>Almost all features provided by MySQL Server.</li>
<li>Converting parameter values back and forth between Python and MySQL data types.</li>
<li>All MySQL extensions to standard SQL syntax.</li>
</ul>

<pre><code class="language-python">import re
import mysql.connector

# establishes the connection to MySQL.
cnx = mysql.connector.connect(user=&#39;root&#39;, password=&#39;8032804254qq&#39;,
                              host=&#39;127.0.0.1&#39;, database=&#39;rookery&#39;, port=&#39;3306&#39;)

# creates a cursor object (cur) to use for executing queries on the database.


cur = cnx.cursor(buffered=True)

# query mysql database for list of user accounts
sql_stmnt = &quot;SELECT DISTINCT User, Host FROM mysql.db &quot;
sql_stmnt += &quot;WHERE Db IN(&#39;rookery&#39;,&#39;birdwatchers&#39;) &quot;
sql_stmnt += &quot;ORDER BY User, Host&quot;
cur.execute(sql_stmnt)

# loop through list of user accounts
for user_accounts in cur.fetchall():
    user_name = user_accounts[0]
    host_address = user_accounts[1]
    user_account = user_name.decode(&#39;utf-8&#39;) + &quot;@&quot; + host_address.decode(&#39;utf-8&#39;)

# display user account heading
print(&#39;User Account:&#39;, (user_name, host_address))
print(&quot;------------------------------------------&quot;)

# query mysql for grants for user account
sql_stmnt = &quot;show grants for &quot; + user_account
cur.execute(sql_stmnt)

# loop through grant entries for user account
for grants in cur.fetchall():
    # skip &#39;usage&#39; entry
    if re.search(&#39;USAGE&#39;, grants[0]):
        continue
    # extract name of database and table
    dbtb = re.search(&#39;ON\s(.*)\.+?(.+?)\sTO&#39;, grants[0])
    db = dbtb.group(1)
    tb = dbtb.group(2)

    # change wildcard for tables to &#39;all&#39;
    if re.search(&#39;\*&#39;, tb): tb = &quot;all&quot;

    # display database and table name for privileges
    print(&quot;database: &quot;, db, &quot;table: &quot;, tb)

    # extract and display privileges for user account # for database and table
    privs = re.search(&#39;GRANT\s(.+?)\sON&#39;, grants[0])
    print(&#39;privileges: &#39;, privs.group(1))

cur.close()
cnx.close()
</code></pre>

<h2 id="toc_1">PyMySQL</h2>

<p><a href="https://github.com/PyMySQL/PyMySQL">PyMySQL</a> is to be a drop-in replacement for <code>MySQLdb</code> and work on <code>CPython</code>, <code>PyPy</code> and <code>IronPython</code>.</p>

<h3 id="toc_2">Installation</h3>

<pre><code class="language-bash">$ pip install PyMySQL
</code></pre>

<h3 id="toc_3">Example</h3>

<pre><code class="language-python">import pymysql

# connect to the datacase
connection = pymysql.connect(host=&#39;localhost&#39;, user=&#39;root&#39;, password=&#39;8032804254qq&#39;, db=&#39;test&#39;)

with connection.cursor() as cursor:
    &quot;&quot;&quot;
    Create Table | CREATE TABLE `books` (
    `book_id` int(3) NOT NULL AUTO_INCREMENT,
    `title` text DEFAULT NULL,
    `status` bit(1) DEFAULT b&#39;0&#39;,
    PRIMARY KEY (`book_id`)
    ) ENGINE=MyISAM AUTO_INCREMENT=103 DEFAULT CHARSET=utf8
    &quot;&quot;&quot;
    # create a new record
    sql = &quot;INSERT INTO `books` (`title`, `status`) VALUES (%s, %s)&quot;
    cursor.execute(sql, (&#39;GRE Reading&#39;, 1))
    
    # connection is not autocommit by default. So you must commit to save your changes.
    connection.commit()

with connection.cursor() as cursor:
    sql = &quot;select `book_id`, `title`, `status` from `books`&quot;
    cursor.execute(sql)
    # read records
    for book in cursor.fetchall():
        book = list(book)
        book[2] = ord(book[2])
        print(book)
</code></pre>

<h2 id="toc_4">Difference? which is better?</h2>

<p><code>MySQLdb</code> is a thin python wrapper around C module which implements API for MySQL database.</p>

<p><code>MySQL Connector</code>is a Python module that reimplements the <a href="https://www.python.org/dev/peps/pep-0249/"><code>MySQL Database API</code></a> in Python. It is written in Python and does not have any dependencies except for the Python Standard Library. It is slower, but does not require the C library and so is more portable.</p>

<h3 id="toc_5">Query Performance</h3>

<p>Obviously, <code>MySQLdb</code> has better performance, especially when it comes to large data sets.</p>

<p><img src="media/15026944183320/Untitled.png" alt="Untitled"/></p>

<h2 id="toc_6">Reference</h2>

<p>1.<a href="https://stackoverflow.com/questions/32575857/the-differences-between-mysqldb-and-mysqlconnector">The differences between MySQLdb and mysqlconnector</a><br/>
2.<a href="http://charlesnagy.info/it/python/python-mysqldb-vs-mysql-connector-query-performance">Python MySQLdb vs mysql-connector query performance</a><br/>
3. <a href="https://wiki.openstack.org/wiki/PyMySQL_evaluation">PyMySQL evaluation</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[逻辑删除]]></title>
    <link href="http://larryim.cc/15026939409542.html"/>
    <updated>2017-08-14T14:59:00+08:00</updated>
    <id>http://larryim.cc/15026939409542.html</id>
    <content type="html"><![CDATA[
<p><strong>逻辑删除</strong>：当数据非常重要时，在实际执行删除操作时，只是将数据标记为删除。在数据查询操作时，加上是否删除语句。</p>

<pre><code class="language-SQL">mysql&gt; alter table birds add column isDelete bit default b&#39;0&#39;;
mysql&gt; update birds
    -&gt; set isDelete = b&#39;1&#39;
    -&gt; where bird_id = 101;
mysql&gt; select count(*)
    -&gt; from birds
    -&gt; where `isDelete` = 0;
+----------+
| count(*) |
+----------+
| 28890    |
+----------+
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Regular Expression in MySQL]]></title>
    <link href="http://larryim.cc/Regular%20Expression%20in%20MySQL.html"/>
    <updated>2017-08-14T14:08:00+08:00</updated>
    <id>http://larryim.cc/Regular%20Expression%20in%20MySQL.html</id>
    <content type="html"><![CDATA[
<p><code>MySQL</code> support pattern-matching operations performed with the <code>REGEXP</code> operator in SQL statements.</p>

<pre><code class="language-SQL">mysql &gt; select common_name as &#39;Birds Great and Small&#39;
      -&gt; from birds
      -&gt; where common_name regexp &#39;Least|Great&#39;
      -&gt; order by common_name desc
      -&gt; limit 10;

+-----------------------+
| Birds Great and Small |
+-----------------------+
| Least Tern            |
| Least Storm-Petrel    |
| Least Seedsnipe       |
| Least Sandpiper       |
| Least Pygmy-Owl       |
| Least Pauraque        |
| Least Nighthawk       |
| Least Honeyguide      |
| Least Grebe           |
| Least Flycatcher      |
+-----------------------+

</code></pre>

<p><code>REGEXP</code> Operator is case insensitive. If we want an expression to be case sensitive, we&#39;ll need to add the <code>Binary</code> option.</p>

<pre><code class="language-SQL">mysql &gt; select common_name as &#39;Hawks&#39;
      -&gt; from birds
      -&gt; where common_name regexp binary &#39;Hawk&#39;
      -&gt; and common_name not regexp &#39;Hawk-Owl&#39;
      -&gt; order by family_id
      -&gt; limit 10;
+------------------------+
| Hawks                  |
+------------------------+
| Ornate Hawk-Eagle      |
| Wallace&#39;s Hawk-Eagle   |
| Pinsker&#39;s Hawk-Eagle   |
| Philippine Hawk-Eagle  |
| Sulawesi Hawk-Eagle    |
| Javan Hawk-Eagle       |
| Blyth&#39;s Hawk-Eagle     |
| Mountain Hawk-Eagle    |
| Flores Hawk-Eagle      |
| Madagascar Cuckoo-Hawk |
+------------------------+
</code></pre>

<h2 id="toc_0">Syntax of Regular Expressions</h2>

<ul>
<li><code>^</code>: Match the beginning of a string</li>
<li><code>$</code>: Match the end of a string</li>
<li><code>.</code>: Match any character</li>
<li><code>a*</code>: Match any sequence of zero or more <code>a</code> characters</li>
<li><code>a+</code>: Match any sequence of one or more <code>a</code> characters</li>
<li><code>a?</code>: Match either zero or one <code>a</code> character</li>
<li><code>de|abc</code>: Match either of the sequences <code>de</code> or <code>abc</code></li>
<li><code>(abc)*</code>: Match zero or more instances of the sequence <code>abc</code></li>
</ul>

<h2 id="toc_1">Rescources</h2>

<ul>
<li><a href="https://dev.mysql.com/doc/refman/5.7/en/regexp.html">Regular Expressions</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Deep Learning]]></title>
    <link href="http://larryim.cc/15025954300059.html"/>
    <updated>2017-08-13T11:37:10+08:00</updated>
    <id>http://larryim.cc/15025954300059.html</id>
    <content type="html"><![CDATA[

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Latex 技巧]]></title>
    <link href="http://larryim.cc/15025903235887.html"/>
    <updated>2017-08-13T10:12:03+08:00</updated>
    <id>http://larryim.cc/15025903235887.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">Latex 查询</h2>

<p>可以在<code>Dash</code>中下载Latex Cheat Sheet以便随时查询。</p>

<h2 id="toc_1">Latex 如何将下标放在正上方／正下方</h2>

<p>在 <code>\sum</code>，<code>\max</code>等之后添加<code>\limits^{upper}_{lower}</code>：\(\sum\limits^{upper}_{lower}\)</p>

<p>例如：<br/>
\(cost(i,j) = \sum \limits^{j}_{k=i}freq[k] + \min \limits_{r=i}^j\)</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tensorflow Introduction]]></title>
    <link href="http://larryim.cc/TensorFlow_introduction.html"/>
    <updated>2017-08-12T16:44:23+08:00</updated>
    <id>http://larryim.cc/TensorFlow_introduction.html</id>
    <content type="html"><![CDATA[
<p><a href="https://www.tensorflow.org">Tensorflow</a> is an open-source software library for Machine Intelligence.</p>

<h2 id="toc_0">Install</h2>

<p>Using <code>conda</code> tool under <code>Anaconda</code> to install tensorflow is a very convenient and direct way.</p>

<h2 id="toc_1">Learning Resources</h2>

<ol>
<li><a href="https://web.stanford.edu/class/cs20si/">CS 20SI: Tensorflow for Deep Learning Research</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MySQL(3): Storage Engines]]></title>
    <link href="http://larryim.cc/15025201109276.html"/>
    <updated>2017-08-12T14:41:50+08:00</updated>
    <id>http://larryim.cc/15025201109276.html</id>
    <content type="html"><![CDATA[
<p>You can use the <code>SHOW TABLE STATUS</code> command to display information about tables, which include types of storage engines.</p>

<pre><code class="language-sql">mysql root@localhost:rookery&gt; show table status\G
***************************[ 1. row ]***************************
Name            | bird_families
Engine          | InnoDB
Version         | 10
Row_format      | Dynamic
Rows            | 12
Avg_row_length  | 1365
Data_length     | 16384
Max_data_length | 0
Index_length    | 16384
Data_free       | 0
Auto_increment  | 113
Create_time     | 2017-08-11 00:18:18
Update_time     | 2017-08-12 13:11:47
Check_time      | &lt;null&gt;
Collation       | latin1_bin
Checksum        | &lt;null&gt;
Create_options  |
Comment         |
</code></pre>

<h2 id="toc_0">InnoDB</h2>

<p><code>InnoDB</code> is the default transactional storage engine for MySQL and the most important and broadly useful engine overall. <em>You should use InnoDB for your tables unless you have a compelling need to use a different engine</em>.</p>

<h2 id="toc_1">Reference</h2>

<ol>
<li>Baron S, Peter Z, Vadim T. 2012. High Performance MySQL 3rd Edition.</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[NP-completeness]]></title>
    <link href="http://larryim.cc/15023384623586.html"/>
    <updated>2017-08-10T12:14:22+08:00</updated>
    <id>http://larryim.cc/15023384623586.html</id>
    <content type="html"><![CDATA[
<p><code>NP</code> stands for <code>None-deterministic Polynomial</code>. This means that the problem can be solved in Polynomial time using a None-deterministic Turing machine.</p>

<h2 id="toc_0">Polynomial-Time Solvability</h2>

<p>A problem is <code>polynomial-time solvable</code> if there is an algorithm that correctly solves it in \(O(n^k)\) time, for some constant \(k\).</p>

<p>Definition: P = the set of polynomial-time solvable problems.</p>

<h2 id="toc_1">Reference</h2>

<ul>
<li><a href="https://stackoverflow.com/questions/210829/what-is-an-np-complete-in-computer-science">What is an NP-complete in computer science?</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Dynamic Programming (4): Optimal Binary Search Tree]]></title>
    <link href="http://larryim.cc/Dynamic_Programming_Optimal_Binary_Search_Tree.html"/>
    <updated>2017-08-09T16:42:39+08:00</updated>
    <id>http://larryim.cc/Dynamic_Programming_Optimal_Binary_Search_Tree.html</id>
    <content type="html"><![CDATA[
<p>An <code>Optimal Binary Search Tree (Optimal BST)</code> is a binary search tree which provides the smallest possible search time for a given sequence of access probabilities. The cost of a BST node is the level of that node plus one multiplied by its frequency.</p>

<p><strong>Problem Statement</strong>: Given a sorted array \(keys[0.. n-1]\) of search keys and an array \(freq[0.. n-1]\) of frequency counts, where \(freq[i]\) is the number of searches to \(keys[i]\). Construct a binary search tree of all keys such that the total cost of all the searches is as small as possible.</p>

<h2 id="toc_0">Optimal Substructure</h2>

<p>The optimal cost of node \(i,...,j\) can be recursively calculated using the following formula.</p>

<p>\(Cost(i,j) = \sum \limits^{j}_{k=i}freq[k] + \min \limits_{r=i}^j[Cost(i,r-1)+Cost(r+1,j)]\)</p>

<p>We try all nodes one by one as root (\(r\) varies from \(i\) to \(j\)). When we make \(r\)th node as root, we recursively calculate optimal cost from \(i\) to \(r-1\) and \(r+1\) to \(j\). </p>

<h3 id="toc_1">Implementation: top-down with memoization</h3>

<pre><code class="language-python">def optimal_bst(freq):
    def opt_cost(hash_table, freq, i, j):
        &quot;&quot;&quot;
        Optimal Binary Search Tree
        Recursive Method used here
        &quot;&quot;&quot;

        # base case: only 0~1 element
        if j &lt; i:
            return 0
        if j == i:
            return freq[i]

        # if already exists
        if (i, j) in hash_table:
            return hash_table[(i, j)]

        min_cost = sys.maxsize
        for r in range(i, j + 1):
            cost = opt_cost(hash_table, freq, i, r - 1) + opt_cost(hash_table, freq, r + 1, j)
            if cost &lt; min_cost:
                min_cost = cost

        hash_table[(i, j)] = sum(freq[i:j + 1]) + min_cost
        return hash_table[(i, j)]

    return opt_cost({}, freq, 0, len(freq) - 1)
</code></pre>

<h2 id="toc_2">Reference</h2>

<ul>
<li><a href="http://ac.els-cdn.com/S0304397596003209/1-s2.0-S0304397596003209-main.pdf?_tid=1505980e-7cd4-11e7-83de-00000aacb361&amp;acdnat=1502263778_53b76d131a06906e9fe3adfadc778677">Optimal Binary Search Tree</a></li>
<li><a href="http://www.geeksforgeeks.org/dynamic-programming-set-24-optimal-binary-search-tree/">Optimal Binary Search Tree GeeksforGeeks</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Dynamic Programming (3): 0-1 Knapsack Problem]]></title>
    <link href="http://larryim.cc/Dynamic_Programming_Knapsack_Problem.html"/>
    <updated>2017-08-09T16:41:03+08:00</updated>
    <id>http://larryim.cc/Dynamic_Programming_Knapsack_Problem.html</id>
    <content type="html"><![CDATA[
<p>The most common problem being solved is the <code>0-1 knapsack problem</code>, which restricts the number \(w_i\) of copies of each kind of item to zero or one. Given a set of \(n\) items numbered from 1 up to \(n\), each with a weight \(w_i\) and a value \(v_i\), along with a maximum weight capacity \(W\),</p>

<p>\[\begin{equation}<br/>
\begin{split}<br/>
&amp;\text{maximize} \sum_{i=1}^nv_ix_i\\<br/>
&amp;\text{subject to} \sum_{i=1}^n w_ix_i \le W \quad \text{and} \quad x_i \in \{0,1\}\\<br/>
\end{split}<br/>
\end{equation}\]</p>

<p>Informally, the problem is to maximize the sum of the values of the items in the knapsack so that the sum of the weights is less than or equal to the knapsack&#39;s capacity.</p>

<h2 id="toc_0">Bottom-up method</h2>

<h3 id="toc_1">Solving</h3>

<p>Assume \(w_i, w_2,..., w_n, W\) are strictly positive integers. Define \(m[i,w]\) to be the maximum value that can be attained with weight less than or queal to \(w\) using items up to \(i\) (first \(i\) items).</p>

<p>We can define \(m[i,w]\) recursively as follows:</p>

<ul>
<li>\(m[0, w]=0 \)</li>
<li>\(m[i,w]=m[i-1, w]\) \(if w_i &gt; w\) ( the new item is more than the current weight limit)</li>
<li>\(m[i,w]= max(m[i-1,w],m[i-1,w-w_i]+v_i\)) if \(w_i \le w\)</li>
</ul>

<p>The following is pseudo code for the dynamic program:</p>

<pre><code class="language-python">// Input:
// Values (stored in array v)
// Weights (stored in array w)
// Number of distinct items (n)
// Knapsack capacity (W)
  
for j from 0 to W do:
     m[0, j] := 0
  
for i from 1 to n do:
    for j from 0 to W do:
        if w[i] &gt; j then:
            m[i, j] := m[i-1, j]
        else:
            m[i, j] := max(m[i-1, j], m[i-1, j-w[i]] + v[i])
</code></pre>

<h3 id="toc_2">Implementation</h3>

<pre><code class="language-python">def knapsack01(value, weight, w_size):
    &quot;&quot;&quot;
    kanapsack01 solves a 0-1 knapsack problem,
    input: values(value) and weights(W) of items to put into knapsack ( size of which is w_size)
    output: the index of items that maximize the value of items putted in the knapsack
    the index of items counts from 0, and corresponding value
    &quot;&quot;&quot;
    n = len(value)  # the number of items
    # maximum value that can be attained with weight &lt;= weight using first i items
    m = np.zeros(shape=(n+1, w_size+1), dtype=int)
    for i in range(w_size+1):
        m[0, i] = 0

    if not isinstance(w_size, int):
        raise ValueError(&#39;knapsack_size should be an integer&#39;)

    for i in range(1, n+1):  # items
        for j in range(1, w_size+1):  # sizes
            if weight[i-1] &gt; j:
                m[i, j] = m[i - 1, j]
            else:
                m[i, j] = max(m[i - 1, j], m[i - 1, j - weight[i-1]] + value[i-1])

    max_val = m[-1, -1]
    items = set()

    while max_val &gt; 0 and i &gt; 0:
        if max_val - value[i-1] in m[i - 1, :]:
            max_val = max_val - value[i-1]
            items.add(i-1)
            i -= 1
        else:
            i -= 1
    return items, m[-1, -1]
</code></pre>

<h2 id="toc_3">top-down with memoization</h2>

<pre><code class="language-python">def knapsack01_recursive(hash_table, value, weight, i, j):
    # base case: when 1 items here
    if i == 1:
        if weight[i - 1] &lt; j:
            hash_table[(i, j)] = value[i - 1]
            return value[i - 1]
        else:
            hash_table[(i, j)] = 0
        return 0

    # sub-problem computed
    if (i, j) in hash_table:
        return hash_table[(i, j)]
    else:
        if weight[i - 1] &gt; j:
            # only case 1
            hash_table[(i, j)] = knapsack01_recursive(hash_table, value, weight, i - 1, j)
        else:
            #  case 1, item i-1 excluded
            case1 = knapsack01_recursive(hash_table, value, weight, i - 1, j)
            # case 2, item i-1 included
            case2 = knapsack01_recursive(hash_table, value, weight, i - 1, j - weight[i - 1]) + value[i - 1]
            hash_table[(i, j)] = max(case1, case2)

    return hash_table[(i, j)]
</code></pre>

<h2 id="toc_4">Reference</h2>

<ul>
<li><a href="https://en.wikipedia.org/wiki/Knapsack_problem#cite_ref-plateau85_15-0">Knapsack Problem WIKIPEDIA</a></li>
<li><a href="https://stackoverflow.com/questions/5683066/knapsack-problem-classic">Knapsack problem Stack Overflow</a></li>
<li><a href="http://www.es.ele.tue.nl/education/5MC10/Solutions/knapsack.pdf">The knapsack Problem PPT</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Dynamic Programming (2): Minimum weight independent set]]></title>
    <link href="http://larryim.cc/Dynamic_Programming_Minimum_weight_independent_set.html"/>
    <updated>2017-08-09T22:52:15+08:00</updated>
    <id>http://larryim.cc/Dynamic_Programming_Minimum_weight_independent_set.html</id>
    <content type="html"><![CDATA[
<p><strong>Input</strong>: A path graph \(G=(V,E)\) with nonnegative weights on vertices.<br/>
<strong>Output</strong>: an <code>independent set</code> of maximum total weight.</p>

<p><strong>Background</strong>:<br/>
A <code>path graph</code> is a tree with two nodes of vertex degree 1, and the other \(n-2\) nodes of vertex degree 2. A path graph is therefore a graph that can be drawn so that all of its vertices and edges lie on a single straight line.</p>

<p>A <code>Independent set</code> is a set of vertices in a graph, no two of which are adjacent.</p>

<h2 id="toc_0">Proposed algorithm</h2>

<ul>
<li>Recursively compute \(S_1=\) max-weight independent set of \(G&#39;\)
rsively compute \(S_2=\) max-weight independent set of \(G&quot;\)</li>
<li>Return \(S_1\) or \(S_2 \cup{v_n}\), whichever is better.</li>
</ul>

<h3 id="toc_1">Implementation</h3>

<pre><code class="language-python">def maximum_weight_independent_set(weights):
    &quot;&quot;&quot;
    Compute Maximum weight independent set for a path graph.
    Weights of vertexes in the path graph are given.
    Return maximum weight independent set.
    &quot;&quot;&quot;
    num = len(weights)
    a = [0] * (num + 1)
    in_set = []  # independent set
    a[0], a[1] = 0, weights[0]

    for i in range(2, num + 1):
        a[i] = max(a[i - 1], a[i - 2] + weights[i - 1])

    while i &gt;= 1:
        if a[i - 1] &gt;= a[i - 2] + weights[i - 1]:
            i -= 1
        else:
            in_set.append(i - 1)
            i -= 2
    in_set.reverse()
    return in_set
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Dynamic Programming (1): Introduction]]></title>
    <link href="http://larryim.cc/Dynamic_Programming_Introduction.html"/>
    <updated>2017-08-08T15:48:13+08:00</updated>
    <id>http://larryim.cc/Dynamic_Programming_Introduction.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">Dynamic programming v.s. Divide-and-conquer algorithm</a>
</li>
<li>
<a href="#toc_1">Approach</a>
<ul>
<li>
<a href="#toc_2">Top-down with memoization</a>
</li>
<li>
<a href="#toc_3">Bottom-up method</a>
</li>
</ul>
</li>
</ul>
</li>
<li>
<a href="#toc_4">****## Which Method?</a>


<p><code>Dynamic programming</code> is a method for solving a complex problem by breaking it down into a collection of simpler subproblems, solving each of those subproblems just once, and storing their solutions. It stores solutions to subproblems instead of recomputing them(called <code>memoization</code> technique).</p>

<h2 id="toc_0">Dynamic programming v.s. Divide-and-conquer algorithm</h2>

<p><code>Divide-and-conquer</code> algorithms partition the problem into disjoint subproblems, solve the subproblems recursively, and then combine their solutions to solve the original problem. In contrast, <code>dynamic programming</code> partition the problem into subproblems that overlaps, that is, subproblems share subproblems.</p>

<h2 id="toc_1">Approach</h2>

<p>There are usually two equivalent ways to implement a dynamic-programming approach.</p>

<h3 id="toc_2">Top-down with memoization</h3>

<p>We write the procedure recursively in a natural manner, but modified to save the result of each subproblem in the hash table. The procedure first checks to see whether it has previously solved this subproblem. If so, it returns the saved value, saving further computation at this level; if not, the procedure computes the value in the usual manner. </p>

<h3 id="toc_3">Bottom-up method</h3>

<p>We sort the subproblems by size and solve them in size order, smallest first. When solving a particular subproblem, we have already solved all of the smaller subproblems its solution depends upon, and we have saved their solutions. We solve each subproblem only once, and when we first see it, we have already solved all of its prerequisite subproblems.</p>

<h1 id="toc_4">****## Which Method?</h1>

<ul>
<li>Top-down approach often results in slightly simpler and clearer code</li>
<li>Top-down approach only computes the partial results that are needed for the particular problem instance, whereas the bottom-up approach computes all partial results even if some of them go unused.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Huffman Coding]]></title>
    <link href="http://larryim.cc/15020346646852.html"/>
    <updated>2017-08-06T23:51:04+08:00</updated>
    <id>http://larryim.cc/15020346646852.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">Implementation</h2>

<pre><code class="language-python">from queue import PriorityQueue

class HuffmanNode:
    def __init__(self, left, right):
        self.left_child = left
        self.right_child = right


def huffman_coding(freqs):
    pq = PriorityQueue()
    for value in freqs:
        pq.put(value)

    while pq.qsize() &gt; 1:
        l, r = pq.get(), pq.get()
        node = HuffmanNode(l, r)
        pq.put((l[0]+r[0], node))
    return pq.get()


def walk_tree(node, prefix=&quot;&quot;, code={}):
    if isinstance(node[1], HuffmanNode):
        walk_tree(node[1].left_child, prefix + &quot;0&quot;, code)
        walk_tree(node[1].right_child, prefix + &quot;1&quot;, code)
    else:
        code[node[1]] = prefix

    return code




freq = [ (8.167, &#39;a&#39;), (1.492, &#39;b&#39;), (2.782, &#39;c&#39;), (4.253, &#39;d&#39;),
    (12.702, &#39;e&#39;), (2.228, &#39;f&#39;), (2.015, &#39;g&#39;), (6.094, &#39;h&#39;),
    (6.966, &#39;i&#39;), (0.153, &#39;j&#39;), (0.747, &#39;k&#39;), (4.025, &#39;l&#39;),
    (2.406, &#39;m&#39;), (6.749, &#39;n&#39;), (7.507, &#39;o&#39;), (1.929, &#39;p&#39;),
    (0.095, &#39;q&#39;), (5.987, &#39;r&#39;), (6.327, &#39;s&#39;), (9.056, &#39;t&#39;),
    (2.758, &#39;u&#39;), (1.037, &#39;v&#39;), (2.365, &#39;w&#39;), (0.150, &#39;x&#39;),
    (1.974, &#39;y&#39;), (0.074, &#39;z&#39;)]

root_node = huffman_coding(freq)
code = walk_tree(root_node)
for i in sorted(freq, reverse=True):
    print(i[1], &#39;{:6.2f}&#39;.format(i[0]), code[i[1]])
</code></pre>

]]></content>
  </entry>
  
</feed>
