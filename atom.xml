<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[techlarry]]></title>
  <link href="http://larryim.cc/atom.xml" rel="self"/>
  <link href="http://larryim.cc/"/>
  <updated>2017-07-12T10:37:34+08:00</updated>
  <id>http://larryim.cc/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im/">MWeb</generator>
  
  <entry>
    <title type="html"><![CDATA[Priority queue]]></title>
    <link href="http://larryim.cc/priority_queue.html"/>
    <updated>2017-07-12T10:09:04+08:00</updated>
    <id>http://larryim.cc/priority_queue.html</id>
    <content type="html"><![CDATA[
<p><strong>A priority queue</strong>(优先队列) is an ADP which likes a regular queue or stack , but where additionally each element has a <code>priority</code> associated with it. In a priority queue, an element with high priority is served before an element with low priority. If two elements have the same priority, they are served according to their order in the queue.</p>

<p>A priority queue must at least support the following operations:</p>

<ul>
<li><code>insert_with_priority</code>: add an element to the queue with an associated priority.</li>
<li><code>pull_highest_priority_element</code>: remove the element from the queue that has the highest priority, and return it.
*<code>peek</code> :return the highest-priority element but does not modify the queue</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Heap]]></title>
    <link href="http://larryim.cc/heap.html"/>
    <updated>2017-07-12T10:09:03+08:00</updated>
    <id>http://larryim.cc/heap.html</id>
    <content type="html"><![CDATA[
<p>A <strong>heap</strong> (堆)is a specialized tree-based data structure。 A <strong>heap</strong> can be classified as either a <strong>max heap</strong> or a <strong>min heap</strong>. </p>

<ul>
<li>In a <strong>max heap</strong>, the keys of parent nodes are always greater than or equal to those of the children and the highest key is in the root node. </li>
<li>In a <strong>min heap</strong>, the keys of parent nodes are less than or equal to those of the children and the lowest key is in the root node.</li>
</ul>

<p><strong>Heap</strong> is one maximally efficient implementation of an abstract data type called a <strong>priority queue</strong> (see <a href="http://larryim.cc/priority_queue.html">here</a>), and in fact priority queues are often referred to as <strong>heaps</strong>, regardless of how they may be implemented. </p>

<h2 id="toc_0">Implementation</h2>

<p>Priority queues typically use a heap as backbone, giving \(O(\log n)\) performance for inserts and removals, and \(O(n \log n)\) to build initially.</p>

<h2 id="toc_1">Binary Heap</h2>

<p>A common implementation of a heap is the <strong>binary heap</strong>(二叉堆), in which the tree is a <strong>complete binary tree</strong>(完全二叉树). </p>

<p>二叉堆的操作与实现</p>

<ul>
<li><code>BinaryHeap()</code>：创建一个新的、空的二叉堆对象</li>
<li><code>insert(k)</code>：把新元素加入到堆中</li>
<li><code>findMin()</code>：返回堆中的最小项，最小项仍保留在堆中</li>
<li><code>delMin()</code>：返回堆中的最小项，同时从堆中删除</li>
<li><code>isEmpty()</code>：返回堆是否为空</li>
<li><code>size()</code>：返回堆中元素的个数</li>
<li><code>buildHeap(list)</code>：从一个包含元素的列表创建新堆</li>
</ul>

<p>有两个关键的操作：<br/>
1. <code>insert</code>方法。首先，为了满足“完全二叉树”的性质，新键值应该添加到列表的末尾。然而新键值简单地添加在列表末尾，显然无法满足堆次序。所以要通过比较父节点和新加入的元素的方法来重新满足堆次序。如果新加入的元素比父节点要小，可以与父节点互换位置；不断交换，直到到达树的顶端。下图所示一系列交换操作来使新加入元素“上浮”到正确的位置。</p>

<p><img src="media/14973171257726/14974156268132.jpg" alt=""/></p>

<p>2.<code>delMin</code>方法 移走根节点的元素后如何保持堆结构和堆次序: 首先，用最后一个节点来代替根节点, 移走最后一个节点保持了堆结构的性质。这么简单的替换，还是会破坏堆次序。第二步，将新节点“下沉”来恢复堆次序。下图所示的是一系列交换操作来使新节点“下沉”到正确的位置。</p>

<p><img src="media/14973171257726/14974156980805.jpg" alt=""/></p>

<pre><code class="language-python">class BinHeap(object):
    &quot;&quot;&quot;
    创建一个新的、空的二叉堆对象
    &quot;&quot;&quot;
    def __init__(self):
        self.list = [0]
        self.size = 0

    def perc_up(self,i):
        &quot;&quot;&quot;
        Percolate the new node into proper position
        &quot;&quot;&quot;
        while i: 
            if self.list[i] &lt; self.list[i//2]:
                self.list[i], self.list[i//2] = self.list[i//2], self.list[i]
            i = i//2
    
    def insert(self, item):
        &quot;&quot;&quot;把新元素加入到堆中&quot;&quot;&quot;
        self.list.append(item)
        self.size += 1
        self.perc_up(self.size)

    def findMin(self):
        &quot;&quot;&quot;返回堆中的最小项，最小项仍保留在堆中&quot;&quot;&quot;
        return self.list[1]

    def delMin(self):
        &quot;&quot;&quot;返回堆中的最小项，同时从堆中删除&quot;&quot;&quot;
        retval = self.list[1]
        self.list[1] = self.list[-1]
        self.size -=1
        self.list.pop()
        self.perc_down(1)

        return retval


    def perc_down(self, i):
        &quot;&quot;&quot;
        Percolate the root node  down the tree
        &quot;&quot;&quot;
        while i*2 &lt;= self.size:
            if i*2+1 &gt; self.size:
                self.list[i*2], self.list[i] = self.list[i], self.list[i*2]
                i = i*2
            else:
                if self.list[i*2] &gt; self.list[i*2+1]:
                    self.list[i*2+1], self.list[i] =  self.list[i], self.list[i*2+1]
                    i = i*2+1
                else:
                    self.list[i*2], self.list[i] = self.list[i], self.list[i*2]
                    i = i*2


    def isEmpty(self):
        &quot;&quot;&quot;返回堆是否为空&quot;&quot;&quot;
        return self.size == 0

    def __len__(self):
        &quot;&quot;&quot;返回堆中元素的个数&quot;&quot;&quot;
        return self.size

    def buildHeap(self,alist):
        &quot;&quot;&quot;从一个包含元素的列表创建新堆&quot;&quot;&quot;
        self.size = len(alist)
        self.list.extend(alist)
        i = self.size//2
        while i &gt;0:
            self.perc_down(i)
            i -= 1

</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[首先有一个概念：回溯]]></title>
    <link href="http://larryim.cc/14992350772045.html"/>
    <updated>2017-07-05T14:11:17+08:00</updated>
    <id>http://larryim.cc/14992350772045.html</id>
    <content type="html"><![CDATA[
<p>　　回溯法(探索与回溯法)是一种选优搜索法，按选优条件向前搜索，以达到目标。但当探索到某一步时，发现原先选择并不优或达不到目标，就退回一步重新选择，这种走不通就退回再走的技术为回溯法，而满足回溯条件的某个状态的点称为“回溯点”。</p>

<p>深度优先算法：</p>

<p>（1）访问初始顶点v并标记顶点v已访问。<br/>
（2）查找顶点v的第一个邻接顶点w。<br/>
（3）若顶点v的邻接顶点w存在，则继续执行；否则回溯到v，再找v的另外一个未访问过的邻接点。<br/>
（4）若顶点w尚未被访问，则访问顶点w并标记顶点w为已访问。<br/>
（5）继续查找顶点w的下一个邻接顶点wi，如果v取值wi转到步骤（3）。直到连通图中所有顶点全部访问过为止。</p>

<p>广度优先算法：</p>

<p>（1）顶点v入队列。<br/>
（2）当队列非空时则继续执行，否则算法结束。<br/>
（3）出队列取得队头顶点v；访问顶点v并标记顶点v已被访问。<br/>
（4）查找顶点v的第一个邻接顶点col。<br/>
（5）若v的邻接顶点col未被访问过的，则col入队列。<br/>
（6）继续查找顶点v的另一个新的邻接顶点col，转到步骤（5）。直到顶点v的所有未被访问过的邻接点处理完。转到步骤（2）。</p>

<p>代码：</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Graph]]></title>
    <link href="http://larryim.cc/14988021902558.html"/>
    <updated>2017-06-30T13:56:30+08:00</updated>
    <id>http://larryim.cc/14988021902558.html</id>
    <content type="html"><![CDATA[
<p>Definition: an (undirected) graph is a collection V of vertices, and a collection E of edges each of which connects a pair of vertices.</p>

<p>Vertices: points<br/>
Edges: lines</p>

<p>Loops: connect a vertices to itself.</p>

<h2 id="toc_0">Representation</h2>

<p>Adjacency Matrix<br/>
Adjacency List<br/>
Edge List<br/>
<img src="media/14988021902558/Screen%20Shot%202017-06-30%20at%202.06.25%20PM.png" alt="Screen Shot 2017-06-30 at 2.06.25 P"/></p>

<p>A <code>path</code> in a graph \(G\) is a sequence of vertices \(v_0,v_1,...,v_n\) so that for all \(i, (v_i,v_{i+1})\) is an edge of \(G\).</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine Learning (5): Neural Networks]]></title>
    <link href="http://larryim.cc/14987831190581.html"/>
    <updated>2017-06-30T08:38:39+08:00</updated>
    <id>http://larryim.cc/14987831190581.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">Why Neural Networks</a>
</li>
<li>
<a href="#toc_1">Background of Neural Networks</a>
</li>
<li>
<a href="#toc_2">Model Representation</a>
</li>
<li>
<a href="#toc_3">Examples and Intuitions</a>
</li>
<li>
<a href="#toc_4">Multiclass Classification</a>
</li>
</ul>


<h2 id="toc_0">Why Neural Networks</h2>

<p>The number of quadratic features closes to \(\frac{n^2}{2}\), it is computationally expensive.</p>

<p>The number of cubic features closes to \(O(n^3)\), it is more computationally expensive.</p>

<p>Computer vision problem looks at matrixes. Because dimensions of pixel images often large (e.g. n= 7500 for 50\(\times\)50 pixel images(RGB)), the number of quadratic features for the problem are 3 million.</p>

<h2 id="toc_1">Background of Neural Networks</h2>

<p><strong>Origins</strong>: Algorithms that try to mimic the brain. It was very widely used in 80s and early 90s; popularity diminished in late 90s. It is now a state of the art technique for many application, because its expensive computation can be meet.</p>

<p>Input wire: Dendrite<br/>
Output wire: Axon</p>

<h2 id="toc_2">Model Representation</h2>

<p>At a very simple level, neurons are basically computational units that take inputs (<code>dendrites</code>) as electrical inputs (<code>spikes</code>) that are channeled to outputs (<code>axons</code>). </p>

<p>In neural networks, dendrites are like the input features \(x_1⋯x_n\), and the output is the result of our hypothesis function. \(x_0\) input node is sometimes called the <code>bias unit</code>. It is always equal to 1. In neural networks, we use the same logistic function as in classification, \(\frac{1}{1+e^{-\theta^Tx}}\), yet we sometimes call it a sigmoid (logistic) activation function. In this situation, our &quot;theta&quot; parameters are sometimes called <code>weights</code>.</p>

<p><strong>Visually, a simplistic representation looks like:</strong><br/>
\[\begin{bmatrix}x_0 \newline x_1 \newline x_2 \newline \end{bmatrix}\rightarrow\begin{bmatrix}\ \ \ \newline \end{bmatrix}\rightarrow h_\theta(x)\]</p>

<p>Input nodes (layer 1), also known as the <code>input layer</code>, go into another node (layer 2), which finally outputs the hypothesis function, known as the <code>output layer</code>.</p>

<p>We can have intermediate layers of nodes between the input and output layers called the <code>hidden layers</code>.</p>

<p>In this example, we label these intermediate or hidden layer nodes \(a^2_0⋯a^2_n\) and call them <code>activation units</code>.</p>

<p>\[\begin{align*}&amp; a_i^{(j)} = \text{&quot;activation&quot; of unit $i$ in layer $j$} \newline&amp; \Theta^{(j)} = \text{matrix of weights controlling function mapping from layer $j$ to $j+1$}\end{align*}\]</p>

<p>The values for each of the &quot;activation&quot; nodes is obtained as follows:</p>

<p>\[\begin{align*} a_1^{(2)} = g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1 + \Theta_{12}^{(1)}x_2 + \Theta_{13}^{(1)}x_3) \newline a_2^{(2)} = g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1 + \Theta_{22}^{(1)}x_2 + \Theta_{23}^{(1)}x_3) \newline a_3^{(2)} = g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1 + \Theta_{32}^{(1)}x_2 + \Theta_{33}^{(1)}x_3) \newline h_\Theta(x) = a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)}) \newline \end{align*}\]</p>

<p>If network has \(s_j\) units in layer \(j\) and \(s_{j+1}\) units in layer \(j+1\), then \(\Theta^{(j)}\) will be of dimension \(s_{j+1}×(s_j+1)\). The \(+1\) comes from the addition in \(\Theta^{(j)}\) of the <code>bias nodes</code>, \(x_0\) and \(\Theta^{(j)}_0\). In other words the output nodes will not include the bias nodes while the inputs will. </p>

<p>We&#39;re going to define a new variable \(z^{(j)}_k\) that encompasses the parameters inside our \(g\) function. In our previous example if we replaced by the variable \(z\) for all the parameters we would get:</p>

<p>\[\begin{align*}a_1^{(2)} = g(z_1^{(2)}) \newline a_2^{(2)} = g(z_2^{(2)}) \newline a_3^{(2)} = g(z_3^{(2)}) \newline \end{align*}\]</p>

<p>In other words, for layer \(j=2\) and node \(k\), the variable \(z\) will be:</p>

<p>\[z_k^{(2)} = \Theta_{k,0}^{(1)}x_0 + \Theta_{k,1}^{(1)}x_1 + \cdots + \Theta_{k,n}^{(1)}x_n\]</p>

<p>The vector representation of \(x\) and \(z_j\) is:</p>

<p>\[\begin{align*}x = \begin{bmatrix}x_0 \newline x_1 \newline\cdots \newline x_n\end{bmatrix} &amp;z^{(j)} = \begin{bmatrix}z_1^{(j)} \newline z_2^{(j)} \newline\cdots \newline z_n^{(j)}\end{bmatrix}\end{align*}\]</p>

<p>Setting \(x=a^{(1)}\), we can rewrite the equation as:</p>

<p>\[z^{(j)} = \Theta^{(j-1)}a^{(j-1)}\]</p>

<p>Now we can get a vector of our activation nodes for layer \(j\) as follows:</p>

<p>\[a^{(j)} = g(z^{(j)})\]</p>

<p>Last Step:<br/>
\[h_\Theta(x) = a^{(j+1)} = g(z^{(j+1)})\]</p>

<h2 id="toc_3">Examples and Intuitions</h2>

<p>The \(\Theta^{(1)}\) matrices for AND, NOR, and OR are:</p>

<p>\[\begin{align*}AND:\newline\Theta^{(1)} &amp;=\begin{bmatrix}-30 &amp; 20 &amp; 20\end{bmatrix} \newline NOR:\newline\Theta^{(1)} &amp;= \begin{bmatrix}10 &amp; -20 &amp; -20\end{bmatrix} \newline OR:\newline\Theta^{(1)} &amp;= \begin{bmatrix}-10 &amp; 20 &amp; 20\end{bmatrix} \newline\end{align*}\]</p>

<p>We can combine these to get the \(XNOR\) logical operator (which gives 1 if \(x_1\) and \(x_2\) are both 0 or both 1).</p>

<p>\[\begin{align*}\begin{bmatrix}x_0 \newline x_1 \newline x_2\end{bmatrix} \rightarrow\begin{bmatrix}a_1^{(2)} \newline a_2^{(2)} \end{bmatrix} \rightarrow\begin{bmatrix}a^{(3)}\end{bmatrix} \rightarrow h_\Theta(x)\end{align*}\]</p>

<p>For the transition between the first and second layer, we&#39;ll use a \(\Theta^{(1)}\) matrix that combines the values for AND and NOR:</p>

<p>\[\Theta^{(1)} =\begin{bmatrix}-30 &amp; 20 &amp; 20 \newline 10 &amp; -20 &amp; -20\end{bmatrix}\]</p>

<p>For the transition between the second and third layer, we&#39;ll use a \(\Theta^{(2)}\) matrix that uses the value for OR:</p>

<p>\[\Theta^{(2)} =\begin{bmatrix}-10 &amp; 20 &amp; 20\end{bmatrix}\]</p>

<p>Let&#39;s write out the values for all our nodes:</p>

<p>\[\begin{align*}&amp; a^{(2)} = g(\Theta^{(1)} \cdot x) \newline&amp; a^{(3)} = g(\Theta^{(2)} \cdot a^{(2)}) \newline&amp; h_\Theta(x) = a^{(3)}\end{align*}\]</p>

<h2 id="toc_4">Multiclass Classification</h2>

<p>To classify data into multiple classes, we let our hypothesis function return a vector of values. Say we wanted to classify our data into one of four categories. We will use the following example to see how this classification is done. This algorithm takes as input an image and classifies it accordingly:</p>

<p><img src="media/14987831190581/14987929949268.png" alt=""/></p>

<p>We can define our set of resulting classes as \(y\):</p>

<p><img src="media/14987831190581/14987930862309.png" alt=""/></p>

<p>Each \(y(i)\) represents a different image corresponding to either a car, pedestrian, truck, or motorcycle. The inner layers, each provide us with some new information which leads to our final hypothesis function. The setup looks like:</p>

<p><img src="media/14987831190581/14987931059100.png" alt=""/></p>

<p>Our resulting hypothesis for one set of inputs may look like:</p>

<p>\[h_\Theta(x) =\begin{bmatrix}0 \newline 0 \newline 1 \newline 0 \newline\end{bmatrix}\]</p>

<p>In which case our resulting class is the third one down, or \(h_\Theta(x)_3\), which represents the motorcycle.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine Learning (4): Overfitting and normalization]]></title>
    <link href="http://larryim.cc/14987114340830.html"/>
    <updated>2017-06-29T12:43:54+08:00</updated>
    <id>http://larryim.cc/14987114340830.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">The problem of Overfitting</a>
</li>
<li>
<a href="#toc_1">Regularized Linear Regression</a>
<ul>
<li>
<a href="#toc_2">Normal Equation</a>
</li>
</ul>
</li>
<li>
<a href="#toc_3">Regularized Logistic Regression</a>
</li>
</ul>


<h2 id="toc_0">The problem of Overfitting</h2>

<p><strong>Underfitting</strong>, or <strong>high bias</strong>, is when the form of our hypothesis function \(h\) maps poorly to the trend of the data. It is usually caused by a function that is too simple or uses too few features. </p>

<p><strong>Overfitting</strong>, or <strong>high variance</strong>, is caused by a hypothesis function that fits the available data but does not generalize well to predict new data. It is usually caused by a complicated function that creates a lot of unnecessary curves and angles unrelated to the data.</p>

<p>There are two main options to address the issue of overfitting:</p>

<ol>
<li><p><strong><em>Reduce the number of features</em></strong>:<br/>
Manually select which features to keep.<br/>
(Use a model selection algorithm).</p></li>
<li><p><strong><em>Regularization</em></strong><br/>
Keep all the features, but reduce the magnitude of parameters \(\theta_j\). Regularization works well when we have a lot of slightly useful features.</p></li>
</ol>

<p><img src="media/14985711297859/14987109337751.png" alt="sd"/></p>

<p>The figure above shows the Underfitting, Normal, Overfitting.</p>

<h2 id="toc_1">Regularized Linear Regression</h2>

<p>We regularize all of theta parameters in a single summation as:</p>

<p>\[J(\theta)= \dfrac{1}{2m}[ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda\ \sum_{j=1}^n \theta_j^2]\]</p>

<p>where the \(\lambda\), or lambda, is the <strong>regularization parameter</strong>. It determines how much the costs of our theta parameters are inflated.  If \(\lambda\) is chosen to be too large, it may smooth out the function too much and cause underfitting. </p>

<p><strong>Note that you should not regularize the parameter \(\theta_0\)</strong>.</p>

<p>The corresponding gradient descent is</p>

<p>\[\begin{align*} &amp; \text{Repeat}\ \lbrace \newline &amp; \ \ \ \ \theta_0 := \theta_0 - \alpha\ \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \newline &amp; \ \ \ \ \theta_j := \theta_j - \alpha\ \left[ \left( \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right] &amp;\ \ \ \ \ \ \ \ \ \ j \in \lbrace 1,2...n\rbrace\newline &amp; \rbrace \end{align*}\]</p>

<p>With some manipulation our update rule can also be represented as:<br/>
\[\theta_j := \theta_j(1 - \alpha\frac{\lambda}{m}) - \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}\]</p>

<h3 id="toc_2">Normal Equation</h3>

<p>To add in regularization, the equation is the same as our original, except that we add another term inside the parentheses:</p>

<p>\[\begin{align*}&amp; \theta = \left( X^TX + \lambda \cdot L \right)^{-1} X^Ty \newline&amp; \text{where}\ \ L = \begin{bmatrix} 0 &amp; &amp; &amp; &amp; \newline &amp; 1 &amp; &amp; &amp; \newline &amp; &amp; 1 &amp; &amp; \newline &amp; &amp; &amp; \ddots &amp; \newline &amp; &amp; &amp; &amp; 1 \newline\end{bmatrix}\end{align*}\]</p>

<p>Recall that if \(m &lt; n\), then \(XTX\) is non-invertible. However, when we add the term \(\lambda L\), then \(XTX + \lambda L\) becomes invertible.</p>

<h2 id="toc_3">Regularized Logistic Regression</h2>

<p>We regularize all of \(\theta\) parameters in a single summation as:</p>

<p>\[ J(\theta) = -\dfrac{1}{m} \sum_{i=1}^m[ y ^{(i)}\log(h_\theta(x^{(i)}))+(1-y^{(i)}) \log(1-h_\theta(x^{(i)}))]+ \frac{\lambda}{2m} \sum_{j=1}^n \theta_j^2\]</p>

<p>The corresponding gradient descent is<br/>
\[\theta_j:=\theta_j-\frac{\alpha}{m}\Sigma^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j+\frac{\alpha\lambda}{m}\theta_j\]</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[口语]]></title>
    <link href="http://larryim.cc/14987012023353.html"/>
    <updated>2017-06-29T09:53:22+08:00</updated>
    <id>http://larryim.cc/14987012023353.html</id>
    <content type="html"><![CDATA[
<ol>
<li>发音 native</li>
<li>流畅度</li>
<li>地道的语料</li>
</ol>

<p>方法</p>

<ol>
<li>逐句跟读   <strong>不看原文</strong></li>
<li>影子跟读</li>
<li>全文复数  用到关键词句</li>
</ol>

<p>每天2个小时，1个小时跟读，1个小时背诵和跟读</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine Learning (3): Classification]]></title>
    <link href="http://larryim.cc/14985711297859.html"/>
    <updated>2017-06-27T21:45:29+08:00</updated>
    <id>http://larryim.cc/14985711297859.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">Binary Classification</a>
<ul>
<li>
<a href="#toc_1">Examples</a>
</li>
<li>
<a href="#toc_2">Hypothesis Representation</a>
</li>
<li>
<a href="#toc_3">Decision boundary</a>
</li>
<li>
<a href="#toc_4">Cost Function</a>
</li>
<li>
<a href="#toc_5">Gradient Descent</a>
</li>
<li>
<a href="#toc_6">Advanced Optimization</a>
</li>
</ul>
</li>
<li>
<a href="#toc_7">Multiclass classification</a>
<ul>
<li>
<a href="#toc_8">Example</a>
</li>
<li>
<a href="#toc_9">Method</a>
</li>
</ul>
</li>
</ul>


<p>The classification problem is just like the regression problem, except that the values y we now want to predict take on only a small number of discrete values.</p>

<h2 id="toc_0">Binary Classification</h2>

<p><strong>Binary classification problem</strong> : y can take on only two values, 0 and 1.</p>

<h3 id="toc_1">Examples</h3>

<ul>
<li>Email: Spam/Not Spam?</li>
<li>Online Transaction: Fraudulent(Yes/NO)?</li>
<li>Tumor: Malignant/ Benign?</li>
</ul>

<p>Given \(x(i)\), the corresponding \(y(i)\) is also called the <strong>label</strong> for the training example.</p>

<h3 id="toc_2">Hypothesis Representation</h3>

<p>It doesn&#39;t make sense for \(h_\theta(x)\) to take values larger than 1 or smaller than0, when we know that \( y\in \{0,1\}\). To fix this, let&#39;s change the form for our hypotheses \(h_\theta(x)\) to satisfy \(0\leq_\theta(x)\leq 1\). This is accomplished by plugging \(\theta^Tx\) into the Logistic Function.</p>

<p><strong>Logistic Function</strong>:<br/>
also called sigmoid (/&#39;sɪgmɔɪd/) function<br/>
   \[\begin{align*}&amp; h_\theta (x) = g ( \theta^T x ) \newline \newline&amp; z = \theta^T x \newline&amp; g(z) = \dfrac{1}{1 + e^{-z}}\end{align*}\]</p>

<ol>
<li><p>The sigmoid function g(z), maps any real number to the (0, 1) interval, making it useful for transforming an arbitrary-valued function into a function better suited for classification.</p></li>
<li><p>\(h_\theta(x)\) gives us the probability that our output is 1. Probability that prediction is 0 is just the complement of probability that it is 1:</p></li>
</ol>

<p>\[\begin{align*}&amp; h_\theta(x) = P(y=1 | x ; \theta) = 1 - P(y=0 | x ; \theta) \newline&amp; P(y = 0 | x;\theta) + P(y = 1 | x ; \theta) = 1\end{align*}\]</p>

<h3 id="toc_3">Decision boundary</h3>

<p>The <strong>decision boundary</strong> is the line that separates the area where y = 0 and where y = 1. It is defined by hypothesis function.</p>

<h3 id="toc_4">Cost Function</h3>

<p>We cannot use the same cost function that we use for linear regression because the Logistic Function will cause the output to be wary, causing many local optima.</p>

<p>Instead, our cost function for logistic regression looks like:</p>

<p>\[\begin{align*}&amp; J(\theta) = \dfrac{1}{m} \sum_{i=1}^m \mathrm{Cost}(h_\theta(x^{(i)}),y^{(i)}) \newline &amp; \mathrm{Cost}(h_\theta(x),y) = -\log(h_\theta(x)) \; &amp; \text{if y = 1} \newline &amp; \mathrm{Cost}(h_\theta(x),y) = -\log(1-h_\theta(x)) \; &amp; \text{if y = 0}\end{align*}\]</p>

<p>Cost function&#39;s two conditional cases can be compressed into one case:<br/>
\[Cost(h_\theta(x),y)= -y \log(h_\theta(x))-(1-y) \log(1-h_\theta(x))\]</p>

<p>The entire cost function can be written as follows:</p>

<p>\[ J(\theta) = -\dfrac{1}{m} \sum_{i=1}^m[ y ^{(i)}\log(h_\theta(x^{(i)}))+(1-y^{(i)}) \log(1-h_\theta(x^{(i)}))]\]</p>

<p>A vectorized implementation is:<br/>
\[h=g(X\theta)\]<br/>
\[J(\theta)=\frac{1}{m}(-y^T\log(h)-(1-y)^T\log(1-h))\]</p>

<p>And the gradient of the cost is a vector of the same length as \(\theta\) where the \(j^{th}\) element is defined as follows:</p>

<p>\[\frac{\partial J(\theta)}{\partial \theta_j}=\frac{1}{m}\Sigma_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j\]</p>

<p>A vectorized implementation is:<br/>
\[\frac{\partial J(\theta)}{\partial \theta_j}=\frac{1}{m}(h-y)^TX\]</p>

<h3 id="toc_5">Gradient Descent</h3>

<p>Repeat until {</p>

<p>\[\theta_j:=\theta_j-\frac{\alpha}{m}\Sigma^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j\]<br/>
}</p>

<p>A vectorized implementation is:</p>

<p>\[\theta:=\theta-\frac{\alpha}{m}X^T(g(X\theta)-y)\]</p>

<h3 id="toc_6">Advanced Optimization</h3>

<p>There are more sophisticated, faster ways to optimize \(\theta\) that can be used instead of gradient descent:</p>

<ul>
<li>Conjugate gradient</li>
<li>BFGS</li>
<li>L-BFGS</li>
</ul>

<p>You should not write these algorithms yourself, but use the libraries provided by Octave and Matlab instead.</p>

<h2 id="toc_7">Multiclass classification</h2>

<h3 id="toc_8">Example</h3>

<ul>
<li>Email foldering/tagging: Work, friends, family, hobby</li>
<li>Medical diagrams: Not ill, Cold, Flu</li>
<li>Weather: Sunny, Cloudy, Rain, Snow</li>
</ul>

<h3 id="toc_9">Method</h3>

<p>Since \(y = \{0,1...n\}\), we divide our problem into \(n+1\) (+1 because the index starts at 0) binary classification problems; in each one, we predict the probability that &#39;y&#39; is a member of one of our classes.</p>

<p>We are basically choosing one class and then lumping all the others into a single second class. We do this repeatedly, applying binary logistic regression to each case, and then use the hypothesis that returned the highest value as our prediction.</p>

<p>\[\begin{align*}&amp; y \in \lbrace0, 1 ... n\rbrace \newline&amp; h_\theta^{(0)}(x) = P(y = 0 | x ; \theta) \newline&amp; h_\theta^{(1)}(x) = P(y = 1 | x ; \theta) \newline&amp; \cdots \newline&amp; h_\theta^{(n)}(x) = P(y = n | x ; \theta) \newline&amp; \mathrm{prediction} = \max_i( h_\theta ^{(i)}(x) )\newline\end{align*}\]</p>

<p><strong>To summarize</strong>:</p>

<p>Train a logistic regression classifier hθ(x) for each class to predict the probability that  \(y = i\) .</p>

<p>To make a prediction on a new x, pick the class that maximizes \(h_\theta(x)\)</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python科学计算三维可视化]]></title>
    <link href="http://larryim.cc/14985240916710.html"/>
    <updated>2017-06-27T08:41:31+08:00</updated>
    <id>http://larryim.cc/14985240916710.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">第三方库</h2>

<ul>
<li>VTK</li>
<li>Mayavi</li>
<li>Numpy</li>
<li>PyQt4</li>
<li>Traites</li>
<li>TraitsUI</li>
</ul>

<h2 id="toc_1">实例</h2>

<ul>
<li>流体数据的标量可视化、矢量可视化实例</li>
<li>三维扫描数据（）可视化实例</li>
<li>三维地球场景可视化实例</li>
<li>曲线UI交互控制可视化实例</li>
</ul>

<h2 id="toc_2">TVTK</h2>

<p>The tvtk module (<a href="http://docs.enthought.com/mayavi/tvtk/README.html">TVTK</a>) provides a traits enabled version of VTK. TVTK objects wrap around VTK objects but additionally support traits, and provide a convenient Pythonic API. TVTK is implemented mostly in pure Python (except for a small extension module). Here is a list of current features.</p>

<h3 id="toc_3">Installation</h3>

<ol>
<li>安装 <a href="https://www.continuum.io/downloads">Anaconda</a></li>
<li>在终端依次运行：</li>
</ol>

<pre><code>    conda install vtk
    conda install numpy
    conda install traits
    conda install mayavi
    conda install PyQt
</code></pre>

<h3 id="toc_4">创建一个三维对象</h3>

<p>tvtk.CubeSource()<br/>
s = tvtk.CubeSource()</p>

<h3 id="toc_5">traits</h3>

<p>traits 就是TVTK的属性</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MySql 数据库]]></title>
    <link href="http://larryim.cc/14974355508816.html"/>
    <updated>2017-06-14T18:19:10+08:00</updated>
    <id>http://larryim.cc/14974355508816.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">E-R 关系模型，</a>
</li>
<li>
<a href="#toc_1">三范式</a>
</li>
<li>
<a href="#toc_2">MySQL 数据类型</a>
</li>
<li>
<a href="#toc_3">约束</a>
</li>
</ul>


<p>数据库（Database）是按照数据结构来组织、存储和管理数据的仓库，每个数据库都有一个或多个不同的API用于创建，访问，管理，搜索和复制所保存的数据。</p>

<p><code>mysql</code>数据库，是当前应用非常广泛的一款关系型数据库。</p>

<p>数据库系统解决的问题：持久化存储，优化读写，保证数据的有效性</p>

<p>当前使用的数据库，主要分为两类：</p>

<ul>
<li>文档型，如<code>sqlite</code>, 就是一个文件，通过对文件的复制完成数据库的复制</li>
<li>服务型，如<code>mysql</code>,<code>postgre</code>, 数据存储在一个物理文件中，但是需要使用终端以tcp/ip协议连接，进行数据库的读写操作 （有服务端、客户端，客户端连接服务端）</li>
</ul>

<h2 id="toc_0">E-R 关系模型，</h2>

<p>当前物理的数据库都是按照E-R模型进行设计的。</p>

<ul>
<li>E表示Enty, 实体</li>
<li>R表示relationship, 关系</li>
<li>一个实体转换为数据库中的一个表</li>
<li><p>关系描述两个实体之间的对应规则，包括</p>

<ul>
<li>一对一</li>
<li>一对多</li>
<li>多对多</li>
</ul></li>
<li><p>关系转换为数据表中的一个利</p></li>
<li><p>在关系型数据库中一行就是一个对象。</p></li>
</ul>

<h2 id="toc_1">三范式</h2>

<p>经过研究和对使用中问题的总结，对于设计设局哭提出了一些规范，这些规范被称为范式</p>

<ul>
<li>第一范式(IF1): 列不可拆分</li>
<li>第二范式(2NF): 唯一标识</li>
<li>第三范式(3NF): 引用主键</li>
<li>说明：后一个范式，都是在前一个范式的基础上建立的</li>
</ul>

<h2 id="toc_2">MySQL 数据类型</h2>

<p>MySQL支持多种类型，这里主要列出来常用几种</p>

<ul>
<li>数字

<ul>
<li>int 整数</li>
<li>decimal 小数</li>
</ul></li>
<li>字符串

<ul>
<li>char 定长字符串</li>
<li>varchar 变长字符串</li>
<li>text 长文本数据</li>
</ul></li>
<li>日期

<ul>
<li>datetime 混合日期和时间值，时间戳</li>
</ul></li>
<li>布尔

<ul>
<li>bit </li>
</ul></li>
</ul>

<h2 id="toc_3">约束</h2>

<ul>
<li>主键约束 primary key 主键不能重复</li>
<li>非空约束 not null 比如要求姓名不能为空</li>
<li>唯一约束 unique 主键约束只能有一个，唯一约束可以有多个</li>
<li>默认约束 default 比如默认性别为男</li>
<li>外键约束 foreign key </li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python树(二)：二叉搜索树]]></title>
    <link href="http://larryim.cc/14974279917175.html"/>
    <updated>2017-06-14T16:13:11+08:00</updated>
    <id>http://larryim.cc/14974279917175.html</id>
    <content type="html"><![CDATA[

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python树(一)：二叉树和二叉堆]]></title>
    <link href="http://larryim.cc/14973171257726.html"/>
    <updated>2017-06-13T09:25:25+08:00</updated>
    <id>http://larryim.cc/14973171257726.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">定义</a>
</li>
<li>
<a href="#toc_1">树的应用</a>
</li>
<li>
<a href="#toc_2">树的术语</a>
</li>
<li>
<a href="#toc_3">树的种类</a>
</li>
<li>
<a href="#toc_4">树的存储</a>
</li>
<li>
<a href="#toc_5">树的实现</a>
<ul>
<li>
<a href="#toc_6">嵌套列表表示树</a>
</li>
<li>
<a href="#toc_7">节点和引用</a>
</li>
</ul>
</li>
<li>
<a href="#toc_8">优先队列和二叉堆</a>
<ul>
<li>
<a href="#toc_9">列表与完全二叉树</a>
</li>
<li>
<a href="#toc_10">二叉堆的操作与实现</a>
</li>
</ul>
</li>
<li>
<a href="#toc_11">分析树</a>
</li>
<li>
<a href="#toc_12">树的遍历</a>
</li>
</ul>


<p>树(tree)是一种抽象数据类型（ADT），用来模拟具有树状结构性质的数据集合。它是由\(n\)个有限节点组成一个具有层次关系的集合。树在计算机科学里应用广泛，包括操作系统，图形学，数据库和计算机网络。树和真正的树有许多相似的地方，也包括根、树枝和叶子，它们的不同在于计算机中的树的根在顶层而它的叶子在底部。它具有以下的特点：</p>

<ul>
<li>树是分级的，即树的顶层部分更加宽泛，而底部更加具体</li>
<li>一个节点（node）的所有子节点（children）和另一个节点的子节点是完全独立的</li>
<li>每个它的叶节点（leaf）都是不同的</li>
</ul>

<h2 id="toc_0">定义</h2>

<p>每个树或者为空，或者包含一个根节点和 0 个或多个子树，其中每个子树也符合这样的定义。每个子树的根节点和其父树的根节点之间通过边相连。</p>

<p>A tree is either empty or consists of a root and zeros or more subtrees, each of which is also a tree. The root of each subtree is connected to the root of the parent tree by an edge.</p>

<h2 id="toc_1">树的应用</h2>

<ul>
<li><p><strong>文件系统</strong>。在文件系统中，磁盘的分支或者说子目录都是运用了树来构建的。下图展示了Unix文件系统的部分的分层情况。<br/>
<img src="media/14973171257726/14973998176466.jpg" alt=""/></p></li>
<li><p><strong>网页</strong>。下图是一个利用超文本标记语言（HTML）编写的简单网页，以及构成网页的超文本标记语言中的标签相互关联关系所构成的树。</p></li>
</ul>

<pre><code class="language-html">&lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot; xml:lang=&quot;en&quot; lang=&quot;en&quot;&gt;
    &lt;head&gt;
        &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot; /&gt;
        &lt;title&gt;simple&lt;/title&gt;
    &lt;/head&gt;
    &lt;body&gt;
        &lt;h1&gt;A simple web page&lt;/h1&gt;
            &lt;ul&gt;
                &lt;li&gt;List item one&lt;/li&gt;
                &lt;li&gt;List item two&lt;/li&gt;
            &lt;/ul&gt;
        &lt;h2&gt;&lt;a href=&quot;http://www.cs.luther.edu&quot;&gt;Luther CS &lt;/a&gt;&lt;h2&gt;
    &lt;/body&gt;
&lt;/html&gt;
</code></pre>

<p><img src="media/14973171257726/14973999293521.jpg" alt=""/></p>

<h2 id="toc_2">树的术语</h2>

<ul>
<li><p><strong>节点</strong>（Node） <br/>
节点是树的基本构成部分。它可能有其他专属的名称，我们称之为“键（key）”。一个节点也可能有更多的信息，我们称之为“负载”。虽然负载信息和树的许多算法并不直接相关，但是它对于树的应用至关重要。</p></li>
<li><p><strong>边</strong>（Edge） <br/>
边也是树的基本构成部分。边连接两个节点，并表示它们之间存在联系。除了根节点外每个节点都有且只有一条与其他节点相连的入边（指向该节点的边），每个节点可能有许多条出边（从该节点指向其他节点的边）。</p></li>
<li><p><strong>根节点</strong>（Root） <br/>
根节点是树种中唯一一个没有入边的节点。在文件系统中，“/”是树的根节点。</p></li>
<li><p><strong>路径</strong>（Path） <br/>
路径是由边连接起来的节点的有序排列。例如：（动物界——脊索动物门——哺乳动物纲——食肉动物目——猫科——猫属——家猫）就是一条路径。</p></li>
<li><p><strong>子节点集</strong>（Children） <br/>
当一个节点的入边来自另一个节点时，我们称前者是后者的子节点，同一个节点的所有子节点构成子节点集。在文件系统中，节点log/,spool/,yp/构成节点var/的子节点集。</p></li>
<li><p><strong>父节点</strong>（Parent） <br/>
一个节点是它出边所连接的所有节点的父节点。在文件系统中，节点var/是节点log/,spool/,yp/的父节点。</p></li>
<li><p><strong>兄弟节点</strong>（Sibling） <br/>
同一个节点的所有子节点互为兄弟节点，在文件系统树中节点etc/和节点usr/是兄弟节点。</p></li>
<li><p><strong>子树</strong>（Subtree） <br/>
子树是一个父节点的某个子节点的所有边和后代节点所构成的集合。</p></li>
<li><p><strong>叶节点</strong>（Leaf Node） <br/>
没有子节点的节点成为称为叶节点。</p></li>
<li><p><strong>层数</strong>（Level） <br/>
一个节点的层数是指从根节点到该节点的路径中的边的数目。定义根节点的层数为 0。</p></li>
<li><p><strong>高度</strong>（Height） <br/>
树的高度等于所有节点的层数的最大值。图 2 中树的高度为 2。</p></li>
</ul>

<h2 id="toc_3">树的种类</h2>

<ul>
<li><strong>无序树</strong>：树中任意节点的子节点之间没有顺序关系，这种树称为无序树，也称为自由树；</li>
<li><strong>有序树</strong>：树中任意节点的子节点之间有顺序关系，这种树称为有序树；

<ul>
<li><strong>霍夫曼树</strong>（用于信息编码）：带权路径最短的二叉树称为哈夫曼树或最优二叉树；</li>
<li><strong>B树</strong>：一种对读写操作进行优化的自平衡的二叉查找树，能够保持数据有序，拥有多余两个子树；</li>
<li><strong>二叉树</strong>：每个节点最多含有两个子树的树称为二叉树；

<ul>
<li><strong>完全二叉树</strong>：对于一颗二叉树，假设其深度为d(d&gt;1)。除了第d层外，其它各层的节点数目均已达最大值，且第d层所有节点从左向右连续地紧密排列，这样的二叉树被称为完全二叉树，其中满二叉树的定义是所有叶节点都在最底层的完全二叉树;</li>
<li><strong>平衡二叉树</strong>（AVL树）：当且仅当任何节点的两棵子树的高度差不大于1的二叉树；</li>
<li><strong>二叉搜索树</strong>（Binary Search Tree）</li>
</ul></li>
</ul></li>
</ul>

<h2 id="toc_4">树的存储</h2>

<p>由于对节点的个数无法掌握，常见树的存储表示都转换成二叉树进行处理，子节点个数最多为2。</p>

<ul>
<li><p>嵌套列表：将数据结构存储在固定的数组中，然在遍历速度上有一定的优势，但因所占空间比较大，是非主流二叉树。二叉树通常以链式存储。</p></li>
<li><p>节点和引用：</p></li>
</ul>

<h2 id="toc_5">树的实现</h2>

<p>树可以具有以下方法：</p>

<ul>
<li><code>BinaryTree()</code> creates a new instance of a binary tree.</li>
<li><code>get_left_child()</code> returns the binary tree corresponding to the left child of the current node</li>
<li><code>get_right_child()</code> return the binary tree corresponding to the right child of the current node</li>
<li><code>set_root_val(val)</code> stores the object stored in the current node</li>
<li><code>get_root_val()</code> returns the object stored in the current node</li>
<li><code>insert_left(val)</code> creates a new binary tree and installs it as the left child of the current node</li>
<li><code>insert_right(val)</code> creates a new binary tree and installs it as the right child of the current node</li>
</ul>

<h3 id="toc_6">嵌套列表表示树</h3>

<p>在列表实现树时，我们将存储根节点作为列表的第一个元素的值。列表的第二个元素的本身是一个表示左子树的列表。这个列表的第三个元素表示在右子树的另一个列表。</p>

<pre><code class="language-python"># coding: utf-8


def BinaryTree(r):
    &quot;&quot;&quot;
    creates a new instance of a binary tree.
    &quot;&quot;&quot;
    return [r,[],[]]


def get_left_child(root):
    &quot;&quot;&quot;
    returns the binary tree corresponding to the left child of the current node
    &quot;&quot;&quot;
    return root[1]


def get_right_child(root):
    &quot;&quot;&quot;
    return the binary tree corresponding to the right child of the current node
    &quot;&quot;&quot;
    return  root[2]


def set_root_val(root, val):
    &quot;&quot;&quot;stores the object stored in the current node
    &quot;&quot;&quot;
    root[0] = val



def get_root_val(root):
    &quot;&quot;&quot;
    returns the object stored in the current node
    &quot;&quot;&quot;
    return root[0]

def insert_left(root, new_branch):
    &quot;&quot;&quot;
    creates a new binary tree and installs it as the left child of the current node

    插入一个左子节点，首先获取对应于当前左子节点的列表（可能是空的）。
    然后，添加新的左子节点，将原来的左子节点作为新节点的左子节点。
    这使我们能够将新节点插入到树中的任何位置.
    &quot;&quot;&quot;
    if root[1]:
        root[1] = [new_branch,root[1], []]
    else:
        root[1] = [new_branch, [], []]
    return root



def insert_right(root, new_branch):
    &quot;&quot;&quot;
    creates a new binary tree and installs it as the right child of the current node
    &quot;&quot;&quot;
    if root[2]:
        root[2] = [new_branch, [], root[2]]
    else:
        root[2] = [new_branch, [], []]
    return root


if __name__ == &quot;__main__&quot;:
    r = BinaryTree(3)
    insert_left(r,4)
    insert_left(r,5)
    insert_right(r,6)
    insert_right(r,7)
    l = get_left_child(r)
    print(l)

    set_root_val(l,9)
    print(r)
    insert_left(l,11)
    print(r)
    print(get_right_child(get_right_child(r)))

# result
#[5, [4, [], []], []]
#[3, [9, [4, [], []], []], [7, [], [6, [], []]]]
#[3, [9, [11, [4, [], []], []], []], [7, [], [6, [], []]]]
#[6, [], []]

</code></pre>

<h3 id="toc_7">节点和引用</h3>

<p>节点和引用方法，定义一个类，具有根、以及左、右子树属性。这种表示更紧密地结合了面向对象的方式。树的结构类似于下图所示。</p>

<p><img src="media/14973171257726/14974056999880.jpg" alt="binary tree"/></p>

<p>左右子树是其他二叉树实例的引用。例如，当插入一个新的左子节点到树上时，即创建了二叉树的另一个实例，并修改了根节点的<code>self.left_child</code>使之指向新的树。</p>

<pre><code class="language-python">
class BinaryTree(object):
    def __init__(self, root):
        self.key = root
        self.left_child = None
        self.right_child = None

    def insert_left(self, item):
        &quot;&quot;&quot;
        creates a new binary tree and installs it as the left child of the current node
        &quot;&quot;&quot;
        if self.left_child:
            self.left_child = BinaryTree(item)
        else:
            t = self.left_child
            self.left_child = BinaryTree(item)
            self.left_child.left_child = t


    def insert_right(self, item):
        &quot;&quot;&quot;
        creates a new binary tree and installs it as the right child of the current node
        &quot;&quot;&quot;
        if self.right_child:
            self.right_child = BinaryTree(item)
        else:
            t = self.right_child
            self.right_child = BinaryTree(item)
            self.right_child.right_child = t

    def get_right_child(self):
        &quot;&quot;&quot;
        return the binary tree corresponding to the right child of the current node
        &quot;&quot;&quot;
        return  self.right_child


    def get_left_child(self):
        &quot;&quot;&quot;
        return the binary tree corresponding to the left child of the current node
        &quot;&quot;&quot;
        return  self.left_child

    def set_root_val(self, root):
        &quot;&quot;&quot;
        stores the object stored in the current node
        &quot;&quot;&quot;
        self.key = root
        

    def get_root_val(self):
        &quot;&quot;&quot;
        returns the object stored in the current node
        &quot;&quot;&quot;
        return  self.key

    def pre_order(self, root):
        &quot;&quot;&quot;
        preorder traversal
        &quot;&quot;&quot;
        print(root.key, end=&#39; &#39;)
        if root.left_child:
            root.pre_order(root.left_child)
        if root.right_child:
            root.pre_order(root.right_child)

    def post_order(self, root):
        &quot;&quot;&quot;
        postorder traversal
        &quot;&quot;&quot;
        if root != None:
            self.post_order(root.left_child)
            self.post_order(root.right_child)
            print(root.key, end=&#39; &#39;)


    def pre_order(self, root):
        &quot;&quot;&quot;
        preorder traversal
        &quot;&quot;&quot;
        print(root.key, end=&#39; &#39;)
        if root.left_child:
            self.pre_order(root.left_child)
        if root.right_child:
            self.pre_order(root.right_child)       

    def in_order(self, root):
        &quot;&quot;&quot;
        postorder traversal
        &quot;&quot;&quot;
        if root != None:
            self.post_order(root.left_child)
            print(root.key, end=&#39; &#39;)  
            self.post_order(root.right_child)

</code></pre>

<h2 id="toc_8">优先队列和二叉堆</h2>

<p><strong>优先队列</strong>(Priority Queues)是一类抽象数据类型。优先队列中的每个元素都有各自的优先级，优先级最高的元素最先得到服务；优先级相同的元素按照其在优先队列中的顺序得到服务。在优先队列的内部，元素的次序是由“优先级”来决定：高优先级的元素排在队首，而低优先级的元素则排在后面。</p>

<p>实现优先队列的经典方法是采用<strong>二叉堆</strong>(Binary Heap)。因为使用二叉堆能将优先队列的入队和出队复杂度都保持在\(O(\log n)\)。 二叉堆有两种：键值总是最小的排在队首称为<strong>最小堆</strong>(min heap)，反之，键值总是最大的排在队首称为<strong>最大堆</strong>(max heap)。</p>

<p>储存元素要满足<strong>堆次序</strong>，即堆中任何一个节点\(x\)，其父节点\(p\)的键值均小于或等于\(x\)的键值。下图所示是具备堆次序性质的完全二叉树。</p>

<h3 id="toc_9">列表与完全二叉树</h3>

<p><strong>不需要使用节点，引用或嵌套列表，用单个列表就能代表完全二叉树</strong>。因为对于完全二叉树，如果节点在列表中的下标为\(p\)，那么其左子节点下标为\(2p\)，右节点为\(2p+1\)。当我们要找任何节点的父节点时，可以直接使用 python 的整除。如果节点在列表中下标为\(n\)，那么父节点下标为\(n//2\)（参考下图）。使用列表, 能够使用简单的数学方法高效地遍历一棵完全二叉树，这也导致了二叉堆的高效实现。</p>

<p><img src="media/14973171257726/14974117374605.jpg" alt=""/></p>

<h3 id="toc_10">二叉堆的操作与实现</h3>

<ul>
<li><code>BinaryHeap()</code>：创建一个新的、空的二叉堆对象</li>
<li><code>insert(k)</code>：把新元素加入到堆中</li>
<li><code>findMin()</code>：返回堆中的最小项，最小项仍保留在堆中</li>
<li><code>delMin()</code>：返回堆中的最小项，同时从堆中删除</li>
<li><code>isEmpty()</code>：返回堆是否为空</li>
<li><code>size()</code>：返回堆中元素的个数</li>
<li><code>buildHeap(list)</code>：从一个包含元素的列表创建新堆</li>
</ul>

<p>有两个关键的操作：<br/>
1. <code>insert</code>方法。首先，为了满足“完全二叉树”的性质，新键值应该添加到列表的末尾。然而新键值简单地添加在列表末尾，显然无法满足堆次序。所以要通过比较父节点和新加入的元素的方法来重新满足堆次序。如果新加入的元素比父节点要小，可以与父节点互换位置；不断交换，直到到达树的顶端。下图所示一系列交换操作来使新加入元素“上浮”到正确的位置。</p>

<p><img src="media/14973171257726/14974156268132.jpg" alt=""/></p>

<p>2.<code>delMin</code>方法 移走根节点的元素后如何保持堆结构和堆次序: 首先，用最后一个节点来代替根节点, 移走最后一个节点保持了堆结构的性质。这么简单的替换，还是会破坏堆次序。第二步，将新节点“下沉”来恢复堆次序。下图所示的是一系列交换操作来使新节点“下沉”到正确的位置。</p>

<p><img src="media/14973171257726/14974156980805.jpg" alt=""/></p>

<pre><code class="language-python">class BinHeap(object):
    &quot;&quot;&quot;
    创建一个新的、空的二叉堆对象
    &quot;&quot;&quot;
    def __init__(self):
        self.list = [0]
        self.size = 0

    def perc_up(self,i):
        &quot;&quot;&quot;
        Percolate the new node into proper position
        &quot;&quot;&quot;
        while i: 
            if self.list[i] &lt; self.list[i//2]:
                self.list[i], self.list[i//2] = self.list[i//2], self.list[i]
            i = i//2
    
    def insert(self, item):
        &quot;&quot;&quot;把新元素加入到堆中&quot;&quot;&quot;
        self.list.append(item)
        self.size += 1
        self.perc_up(self.size)

    def findMin(self):
        &quot;&quot;&quot;返回堆中的最小项，最小项仍保留在堆中&quot;&quot;&quot;
        return self.list[1]

    def delMin(self):
        &quot;&quot;&quot;返回堆中的最小项，同时从堆中删除&quot;&quot;&quot;
        retval = self.list[1]
        self.list[1] = self.list[-1]
        self.size -=1
        self.list.pop()
        self.perc_down(1)

        return retval


    def perc_down(self, i):
        &quot;&quot;&quot;
        Percolate the root node  down the tree
        &quot;&quot;&quot;
        while i*2 &lt;= self.size:
            if i*2+1 &gt; self.size:
                self.list[i*2], self.list[i] = self.list[i], self.list[i*2]
                i = i*2
            else:
                if self.list[i*2] &gt; self.list[i*2+1]:
                    self.list[i*2+1], self.list[i] =  self.list[i], self.list[i*2+1]
                    i = i*2+1
                else:
                    self.list[i*2], self.list[i] = self.list[i], self.list[i*2]
                    i = i*2


    def isEmpty(self):
        &quot;&quot;&quot;返回堆是否为空&quot;&quot;&quot;
        return self.size == 0

    def __len__(self):
        &quot;&quot;&quot;返回堆中元素的个数&quot;&quot;&quot;
        return self.size

    def buildHeap(self,alist):
        &quot;&quot;&quot;从一个包含元素的列表创建新堆&quot;&quot;&quot;
        self.size = len(alist)
        self.list.extend(alist)
        i = self.size//2
        while i &gt;0:
            self.perc_down(i)
            i -= 1

</code></pre>

<h2 id="toc_11">分析树</h2>

<p><strong>分析树</strong>(Parse Tree)是一个反映某种形式语言字符串的语法关系的有根有序树, 常常用于真实世界的结构表示，例如句子或数学表达式。</p>

<p>下图是\( ((7+3)*(5−2))\) 的分析树, 树的层级结构帮我们理解了整个表达式的运算顺序。在计算最顶上的乘法运算前，我们先要计算子树中的加法和减法运算。左子树的加法运算结果为\(10\)，右子树的减法运算结果为\(3\)。利用树的层级结构，一旦我们计算出了子节点中表达式的结果，我们能够将整个子树用一个节点来替换。</p>

<p><img src="media/14973171257726/14974242592531.jpg" alt=""/></p>

<p>建立分析树的第一步是将表达式字符串分解成符号保存在列表里。有四种符号需要考虑：<strong>左括号</strong>，<strong>右括号</strong>，<strong>操作符</strong>和<strong>操作数</strong>。当读到一个左括号时，将开始一个新的表达式，因此需要创建一个子树来对应这个新的表达式。相反，每当读到一个右括号，就得结束这个表达式。另外，操作数将成为叶节点和他们所属的操作符的子节点。最后，每个操作符都应该有一个左子节点和一个右子节点。通过上面的分析我们定义以下四条规则：</p>

<ul>
<li>如果当前读入的字符是<code>(</code>，添加一个新的节点作为当前节点的左子节点，并下降到左子节点处。</li>
<li>如果当前读入的字符在列表[<code>+</code>, <code>-</code>, <code>/</code>, <code>*</code>]中，将当前节点的根值设置为当前读入的字符。添加一个新的节点作为当前节点的右子节点，并下降到右子节点处。</li>
<li>如果当前读入的字符是一个数字，将当前节点的根值设置为该数字，并返回到它的父节点。</li>
<li>如果当前读入的字符是<code>)</code>，返回当前节点的父节点。</li>
</ul>

<p>利用<code>get_left_child</code>和<code>get_right_child</code>方法可以获得子节点的方法。<strong>利用栈跟踪父节点</strong>：当要下降到当前节点的子节点时，将当前节点压入栈；当要返回当前节点的父节点时，从栈中弹出该父节点。</p>

<p>所以使用栈和二叉树来创建分析树，代码如下：</p>

<pre><code class="language-python">from stack import Stack
from binary_tree2 import BinaryTree
import operator

def build_parse_tree(fp_exp):
    fp_list = fp_exp.split()
    p_stack = Stack()
    e_tree = BinaryTree(&#39;&#39;)
    p_stack.push(e_tree)
    current_tree = e_tree

    for i in fp_list:
        if i == &#39;(&#39;:
            current_tree.insert_left(&#39;&#39;)
            p_stack.push(current_tree)
            current_tree = current_tree.get_left_child()
        elif i not in [&#39;+&#39;,&#39;-&#39;,&#39;*&#39;,&#39;/&#39;,&#39;)&#39;]:
            current_tree.set_root_val(int(i))
            parent = p_stack.pop()
            current_tree = parent
        elif i in [&#39;+&#39;,&#39;-&#39;,&#39;*&#39;,&#39;/&#39;]:
            current_tree.set_root_val(i)
            current_tree.insert_right(&#39;&#39;)
            p_stack.push(current_tree)
            current_tree = current_tree.get_right_child()
        elif i == &#39;)&#39;:
            current_tree = p_stack.pop()
        else:
            raise ValueError

    return e_tree

def postorder_eval(tree):
    opers = {&#39;+&#39;:operator.add, &#39;-&#39;:operator.sub, &#39;*&#39;:operator.mul, &#39;/&#39;: operator.truediv}
    res1 = None
    res2 = None

    if tree:
        res1 = postorder_eval(tree.get_left_child())
        res2 = postorder_eval(tree.get_right_child())
        if res1 and res2:
            return opers[tree.get_root_val()](res1, res2)
        else:
            return tree.get_root_val()

pt = build_parse_tree(&quot;( ( 10 + 5 ) * 3 )&quot;)

print(&#39;result = %d&#39; %postorder_eval(pt))
</code></pre>

<h2 id="toc_12">树的遍历</h2>

<p>对树中所有节点的访问称为<strong>遍历</strong>(traversal)。按照节点的访问方式不同，树的遍历模式可分为 3 种。这三种方式常被用于访问树的节点，它们之间的不同在于访问每个节点的次序不同。这三种遍历分别叫做<strong>先序遍历</strong>(preorder)，<strong>中序遍历</strong>(inorder)和<strong>后序遍历</strong>(postorder)。具体定义为：</p>

<ul>
<li><p><strong>先序遍历</strong> 先访问根节点，然后递归使用先序遍历访问左子树，再递归使用先序遍历访问右子树。</p></li>
<li><p><strong>中序遍历</strong> 递归使用中序遍历访问左子树，然后访问根节点，最后再递归使用中序遍历访问右子树。</p></li>
<li><p><strong>后序遍历</strong> 先递归使用后序遍历访问左子树和右子树，最后访问根节点。</p></li>
</ul>

<p>三种遍历模式的代码已经包括在<code>BinaryTree</code>类中（参见<code>节点和引用</code>一节）。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[魔法方法(一)：自定义序列]]></title>
    <link href="http://larryim.cc/14973620419454.html"/>
    <updated>2017-06-13T21:54:01+08:00</updated>
    <id>http://larryim.cc/14973620419454.html</id>
    <content type="html"><![CDATA[
<p>Python中的魔法方法可以理解为对类中的内置方法的重载。</p>

<h2 id="toc_0">自定义序列</h2>

<p>有许多办法可以让Python类表现得像是内建序列类型(字典，元组，列表，字符串等)。</p>

<h3 id="toc_1">预备知识</h3>

<p>协议类似某些语言中的接口，里面包含的是一些必须实现的方法。在Python中，协议完全是非正式的，也不需要显式的声明，事实上，它们更像是一种参考标准。</p>

<p>在Python中实现自定义容器类型需要用到一些协议。首先，不可变容器类型有如下协议：想实现一个不可变容器，你需要定义 <code>__len__</code> 和 <code>__getitem__</code> (后面会具体说明）。可变容器的协议除了上面提到的两个方法之外，还需要定义 <code>__setitem__</code> 和 <code>__delitem__</code> 。最后，如果你想让你的对象可以迭代，你需要定义 <code>__iter__</code> ，这个方法返回一个迭代器。迭代器必须遵守迭代器协议，需要定义 <code>__iter__</code> （返回它自己）和 <code>next</code> 方法。</p>

<h3 id="toc_2">容器背后的魔法方法</h3>

<ul>
<li><p><code>__len__(self)</code><br/>
返回容器的长度，可变和不可变类型都需要实现。</p></li>
<li><p><code>__getitem__(self, key)</code><br/>
定义对容器中某一项使用 self[key] 的方式进行读取操作时的行为。这也是可变和不可变容器类型都需要实现的一个方法。它应该在键的类型错误式产生 <code>TypeError</code> 异常，同时在没有与键值相匹配的内容时产生 <code>KeyError</code> 异常。</p></li>
<li><p><code>__setitem__(self, key)</code><br/>
定义对容器中某一项使用 self[key] 的方式进行赋值操作时的行为。它是可变容器类型必须实现的一个方法，同样应该在合适的时候产生 <code>KeyError</code> 和 <code>TypeError</code> 异常。</p></li>
<li><p><code>__iter__(self, key)</code><br/>
它应该返回当前容器的一个迭代器。迭代器以一连串内容的形式返回，最常见的是使用<code>iter()</code>函数调用，以及在类似 for x in container: 的循环中被调用。迭代器是他们自己的对象，需要定义 <code>__iter__</code>方法并在其中返回自己。</p></li>
<li><p><code>__reversed__(self)</code><br/>
定义了对容器使用<code>reversed()</code>内建函数时的行为。它应该返回一个反转之后的序列。当你的序列类是有序时，类似列表和元组，再实现这个方法，</p></li>
<li><p><code>__contains__(self, item)</code><br/>
<code>__contains__</code>定义了使用<code>in</code>和<code>not in</code>进行成员测试时类的行为。你可能好奇为什么这个方法不是序列协议的一部分，原因是，如果<code>__contains__</code>没有定义，Python就会迭代整个序列，如果找到了需要的一项就返回<code>True</code>。</p></li>
<li><p><code>__missing__(self ,key)</code><br/>
<code>__missing__</code>在字典的子类中使用，它定义了当试图访问一个字典中不存在的键时的行为（目前为止是指字典的实例，例如我有一个字典 d ， “george” 不是字典中的一个键，当试图访问 d[“george’] 时就会调用 d.__missing__(“george”))。</p></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hash table and Python dictionary]]></title>
    <link href="http://larryim.cc/14973464173252.html"/>
    <updated>2017-06-13T17:33:37+08:00</updated>
    <id>http://larryim.cc/14973464173252.html</id>
    <content type="html"><![CDATA[
<p>One of the most useful Python collections is the dictionary, which is an associative data type where you can store key-data pairs. It is implemented using <strong>hash tables</strong>(哈希表).</p>

<p><strong>Hash table</strong> (哈希表) is a collection of items which are stored in such a way as to make it easy to find item. Each position of the hash table, often called a <strong>slot</strong>, can hold an item and is named by an integer value starting at 0. The mapping between an item and the slot where that item belongs in the hash table is called the <strong>hash function</strong>(哈希函数). A <strong>perfect hash function</strong> maps every items into a unique slot. When two items hash to the same slot, <strong>collision</strong> happens. And a systematic method, which called <strong>collision resolution</strong>, for placing the second item in the hash table must be put forward.</p>

<h2 id="toc_0">Direct-address tables</h2>

<p>If the number of possible keys is small and they are unique, direct-address tables (直接寻址表) can be used. Each slot corresponds to a unique key.</p>

<pre><code class="language-python">
Insert(T,x)
    T[key(x)] = x
    
Search(T,x)
    return(T[key(x)])
    
Delete(T,x)
    T[key(x)] = NIL
</code></pre>

<p><img src="media/14973464173252/14973484465039.gif" alt="nameasdfasfas"/></p>

<p>The range of the key determines the size of the direct address table and may be too large to be practical. For instance, it&#39;s not likely that you&#39;ll be able to use a direct address table to store elements which have arbitary 32-but integers as their keys for a few years yet.</p>

<h2 id="toc_1">Hash function</h2>

<p><code>Remainder method</code> simply takes an item and divides it by the table size, returning the remainder as its hash value, i.e. \(h(item)=item\%11\)</p>

<p><code>Folding method</code> begins by dividing the item into equal size pieces (the last piece may not be of equal size), and then added together to give the resulting hash value by extra step of dividing by the table size and keeping the remainder.</p>

<h2 id="toc_2">Collision Resolution</h2>

<p>Various techniques are used to manage collision:</p>

<ul>
<li>Chaining (链接法)</li>
<li>Open addressing (开放寻址法)

<ul>
<li>linear probing (线性探查)</li>
<li>quadratic probing (二次探查)</li>
</ul></li>
</ul>

<h3 id="toc_3">chaining</h3>

<p>Chaining(链接法) allows each slot to hold a reference to a collection of items. It allows many items to exist at the same location in the hash table.</p>

<p><img src="media/14973464173252/Screen%20Shot%202017-06-13%20at%206.16.40%20PM.png" alt="Screen Shot 2017-06-13 at 6.16.40 P"/></p>

<h3 id="toc_4">Open addressing</h3>

<p>In <strong>open addressing</strong>, all elements occupy the hash table itself. That is, each table entry contains either an element of the dynamic set or NIL. When searching for an element, we systematically examine table slots until either we find the desired element or we have ascertained that the element is not in the table. No lists and no elements are stored outside the table, unlike in chaining.</p>

<h4 id="toc_5">Linear probing</h4>

<p>One of the simplest re-hashing functions is +1(or -1) on a collision, i.e. look in the neighbouring slot in the table. It calculates new address extremely quickly.</p>

<p>A disadvantage to linear probing is the tendency for clustering: items become clustered in the table.</p>

<h4 id="toc_6">Quadratic probing</h4>

<p>Instead of using a constant &#39;skip&#39; value, quadratic probing using a rehash function that increments the hash value by 1,3,5,7,9 and so on. This means that if the first hash value is \(h\), the successive values are \(h+1\), \(h+4\),\(h+9\), \(h+16\) and so on.</p>

<h3 id="toc_7">Load factor</h3>

<p>Load factor (装载因子) is commonly denoted by \(\lambda = \frac{number\_of\_items}{table\_size}\).</p>

<p>The most important piece of information we need to analyze the use of a hash table is the load factor, \(\lambda\). Conceptually, if the load factor is small, then there is a lower chance of collisions.</p>

<p>For a successful search, using open addressing with linear probing, the average number of comparisons is approximately \(\frac{1}{2}(1+\frac{1}{1-\lambda})\) and an unsuccessful search gives \(\frac{1}{2}(1+(\frac{1}{1-\lambda})^2)\).</p>

<p>Using chaining, the average number of comparisons is \(1+\lambda/2\) for the successful case, and simply \(\lambda\) comparisons if the search is unsuccessful.</p>

<h2 id="toc_8"><code>Map</code> ADT</h2>

<p>Here, we try to define an abstract data type <code>map</code>, which is an unordered collections of associations between keys and values. The operation are given below:</p>

<ul>
<li><code>Map()</code> create a new, empty map. It returns an empty map collection.</li>
<li><code>put(key, val)</code> Add a new key-value pair to the map. If the key is already in the map then replace the old value with the new value.</li>
<li><code>get(key)</code> Given a key, return the values stored in the map or None otherwise.</li>
<li><code>del</code> Delete the key-value pair from the map using a statement of the form del map[key]</li>
<li><code>len()</code> Return the number of key-value pairs stored in the map</li>
<li><code>in</code> Return <code>True</code> for a statement of the form key in map, if the given key is in the map, <code>False</code> otherwise.</li>
</ul>

<p>Using two lists, <code>slots</code> which holds the key items, and <code>data</code> which holds the data values, we implements the <code>Map</code> ADT.</p>

<p><code>hash_function</code> implements the hash function by simple <code>remainder method</code>. The collision resolution is linear probing. The <code>put</code> function assumes that there will eventually be an empty slot unless the key is already present in the <code>self.slots</code></p>

<pre><code class="language-python">
class HashTable(object):
    &quot;&quot;&quot;
    Hash Table
    &quot;&quot;&quot;
    def __init__(self):
        self.size = 11
        self.slots = [None]*self.size
        self.data = [None]*self.size

    def hash_function(self, key, size):
        &quot;&quot;&quot;
        Hash function: simple remainder method
        &quot;&quot;&quot;
        return key%size

    def rehash(self, old_hash, size):
        &quot;&quot;&quot;
        Rehash function: linear probing, just add 1 every time
        &quot;&quot;&quot;
        return (old_hash+1)%size

    def put(self, key, data):
        &quot;&quot;&quot;
        Put data into table
        &quot;&quot;&quot;

        # generate a slot based on hash_function
        hash_value = self.hash_function(key, self.size)

        # check hash_value
        if self.slots[hash_value] is None:
            self.slots[hash_value] = key
            self.data[hash_value] = data
        else:
            
            # replace data
            if self.slots[hash_value] == key:
                self.data[hash_value] = data

            # re-hash until empty slots
            next_slot = self.rehash(hash_value, self.size)
            while self.slots[next_slot] is not None and self.slots[next_slot] != key:
                next_slot = self.rehash(next_slot, self.size)

            if self.slots[next_slot] is None:
                self.slots[next_slot] = key
                self.data[next_slot] = data
            else:
                self.data[next_slot] = data

    def get(self, key):
        start = self.hash_function(key, self.size)

        if self.slots[start] == key:
            return self.data[start]

        while self.slots[start] is not None:
            if self.slots[start] == key:
                return self.data[start]
            start = self.rehash(start, self.size)

        return False

    def __getitem__(self, key):
        return self.get(key)

    def __setitem__(self, key, data):
        self.put(key, data)

    def __len__(self):
        &quot;&quot;&quot;
        返回长度
        &quot;&quot;&quot;
        count = 0
        for item in self.slots:
            if item is not None:
                count +=1

        return count

    def __contains__(self, item):
        &quot;&quot;&quot;
        定义了使用in和not in进行成员测试时类的行为
        &quot;&quot;&quot;
        for key in self.slots:
            if key == item:
                return True

        return False




if __name__== &quot;__main__&quot;:
    h=HashTable()
    h.put(54, &quot;cat&quot;)
    print(len(h))
    h.put(26, &quot;dog&quot;)
    h.put(93, &quot;lion&quot;)
    h.put(17, &quot;tiger&quot;)
    h.put(77, &quot;bird&quot;)
    h.put(31, &quot;cow&quot;)
    h.put(44, &quot;goat&quot;)
    h.put(55, &quot;pig&quot;)
    h.put(20, &quot;chicken&quot;)
    print(h.slots)
    print(h.data)
    h[20] = &quot;duck&quot;  # replace
    print(h.data)
    print(len(h))
    print( 4 in h)
    print( 17 in h)
</code></pre>

<p>其中出现的魔法方法可以参见<a href="http://larryim.cc/14973620419454.html">Link</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python 导入模块]]></title>
    <link href="http://larryim.cc/14973218394741.html"/>
    <updated>2017-06-13T10:43:59+08:00</updated>
    <id>http://larryim.cc/14973218394741.html</id>
    <content type="html"><![CDATA[

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[搜索]]></title>
    <link href="http://larryim.cc/14973149842140.html"/>
    <updated>2017-06-13T08:49:44+08:00</updated>
    <id>http://larryim.cc/14973149842140.html</id>
    <content type="html"><![CDATA[
<p>搜索是在一个项目集合中找到一个特定项目的算法过程。返回值一般是<code>bool</code>:<code>True</code> or <code>False</code>. 搜索的几种常见方法：顺序查找、二分法查找、二叉树查找、哈希查找</p>

<h2 id="toc_0">在python中查找</h2>

<p>在python中，查找一个元素是否在列表中非常简单，可以用<code>in</code>操作符</p>

<pre><code class="language-python">&gt;&gt;&gt; 15 in [3,5,2,4,1]
False
&gt;&gt;&gt; 3 in [3,5,2,4,1]
True
&gt;&gt;&gt;
</code></pre>

<h2 id="toc_1">顺序查找</h2>

<p>顺序查找从列表中的第一个项目开始，我们按照顺序次序，简单地从一个项移动到另一个项，直到找到我们正在查找的项或遍历完整个列表。如果我们遍历完整个列表，则说明正在搜索的项不存在。</p>

<p><img src="media/14973149842140/14973392316280.png" alt=""/></p>

<pre><code class="language-python">def sequential_search(alist, item):
    &quot;&quot;&quot;
    顺序查找
    &quot;&quot;&quot;

    for i in range(len(alist)):
        if alist[i] == item:
            return True

    return False
</code></pre>

<h2 id="toc_2">二分查找</h2>

<p>二分查找过程类似于查字典。首先，假设表中元素是按升序排列，将表中间位置记录的关键字与查找关键字比较，如果两者相等，则查找成功；否则利用中间位置记录将表分成前、后两个子表，如果中间位置记录的关键字大于查找关键字，则进一步查找前一子表，否则进一步查找后一子表。重复以上过程，直到找到满足条件的记录，使查找成功，或直到子表不存在为止，此时查找不成功。</p>

<ul>
<li>优点是比较次数少，查找速度快，平均性能好</li>
<li>缺点是要求待查表为有序表，且插入删除困难</li>
<li>因此，折半查找方法适用于不经常变动而查找频繁的有序列表。</li>
</ul>

<p><img src="media/14973149842140/14973392434825.png" alt=""/></p>

<pre><code class="language-python">
def binary_search(alist, item):
    &quot;&quot;&quot;
    二分查找, 递归
    &quot;&quot;&quot;

    n = len(alist)
    if n &gt;0:
        mid = n//2

        if alist[mid] == item:
            return True
        elif alist[mid] &gt; item:
            return binary_search(alist[:mid], item)
        else:
            return binary_search(alist[mid+1:], item)
    return False

def binary_search2(alist, item):
    &quot;&quot;&quot;
    二分查找，非递归
    &quot;&quot;&quot;
    n = len(alist)
    first = 0
    last = n-1
    while first &lt;= last:
        mid = (first+last)//2
        if alist[mid] == item:
            return True
        elif alist[mid] &gt; item:
            last = mid-1
        else:
            first = mid+1

    return False

</code></pre>

<h2 id="toc_3">时间复杂度</h2>

<ul>
<li>最优时间复杂度: \(O(1)\)</li>
<li>最坏时间复杂度: \(O(\log n)\)</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine Learning (2): Linear Regression with Multiple Variables]]></title>
    <link href="http://larryim.cc/14972614146835.html"/>
    <updated>2017-06-12T17:56:54+08:00</updated>
    <id>http://larryim.cc/14972614146835.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">Multiple Features (variables)</a>
</li>
<li>
<a href="#toc_1">Gradient descent</a>
<ul>
<li>
<a href="#toc_2">Feature Scaling</a>
</li>
<li>
<a href="#toc_3">Learning rate</a>
</li>
</ul>
</li>
<li>
<a href="#toc_4">Computing Parameters Analytically</a>
<ul>
<li>
<a href="#toc_5">Normal equation:</a>
</li>
<li>
<a href="#toc_6">Gradient descent v.s. Normal equation</a>
</li>
<li>
<a href="#toc_7">Normal Equation Non-invertible</a>
</li>
</ul>
</li>
</ul>


<h2 id="toc_0">Multiple Features (variables)</h2>

<p>Notation:<br/>
\(m\) = the number of training examples<br/>
\(n\) = the number of features<br/>
\(x^{(i)}\)  = the input (feature) of \(i^{th}\) training example<br/>
\(x^{(i)}_j\) =  value of feature \(j\) of \(i^{th}\) training example</p>

<p>The multivariable form of the hypothesis function accommodating these multiple features is as follows:</p>

<p>\[h_θ(x)=θ_0+θ_1x_1+θ_2x_2+θ_3x_3+⋯+θ_nx_n=\theta^Tx\] (\(n+1\)- dimensional vector)</p>

<p>For convenience of notation, define \(x_0=1\)</p>

<h2 id="toc_1">Gradient descent</h2>

<p><strong>Hypothesis</strong>: <br/>
\[h_\theta(x) = \theta^Tx\]</p>

<p><strong>Parameters</strong>:<br/>
\[\theta\]</p>

<p><strong>Cost Function</strong>:<br/>
\[J(\theta)=\frac{1}{2m}\Sigma^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2\]</p>

<p><strong>Gradient descent</strong>:<br/>
Repeat until converge{</p>

<p>\(\theta_j := \theta_j -\alpha\frac{\partial}{\partial \theta_j}J(\theta)=\theta_j -\alpha\frac{1}{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}\)  (simultaneously update for every j= 0,1,...,n)</p>

<h3 id="toc_2">Feature Scaling</h3>

<p>We can speed up gradient descent by having each of our input values in roughly the same range. This is because \(\theta\) will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.</p>

<p><strong>Idea: Make sure features are on a similar scale.</strong></p>

<p>Two techniques to help with this are <strong>feature scaling</strong> and <strong>mean normalization</strong>.</p>

<ul>
<li><p><strong>Feature scaling</strong> involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1. </p></li>
<li><p><strong>Mean normalization</strong> involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero. </p></li>
</ul>

<p>Replace \(x_i\) with \( x_i -\mu_i\) to make features have approximately zero mean (Do not apply to \(x_0=1\))</p>

<p>\[x_i =\frac{ x_i - \mu_i}{S_i}\]</p>

<p>where  \(\mu_i\) is average value of \(x_i\) in training set, \(S_i\) is the range (max-min) or standard deviation of \(x_i\).</p>

<p>E.g. \[x_1=\frac{size-1000}{2000}\]<br/>
\[x_2=\frac{\#bedrooms-2}{5}\]</p>

<h3 id="toc_3">Learning rate</h3>

<p><strong>Debugging</strong>: How to make sure gradient descent is working correctly.</p>

<p>-- How to choose learning rate \(\alpha\).</p>

<p>Gradient descent is working correctly if \(J(\theta)\) decreases after every iteration.</p>

<p>Use smaller \(\alpha\). For sufficiently small \(\alpha\), \(J(\theta)\) should decrease on every iteration.</p>

<p><strong>Automatic convergence test</strong>. Declare convergence if \(J(\theta)\) decreases by less than \(E\) in one iteration, where \(E\) is some small value such as \(10^{−3}\). However in practice it&#39;s difficult to choose this threshold value.</p>

<p><strong>Summary</strong>:</p>

<ul>
<li>If  \(\alpha\)  is too small: slow convergence.</li>
<li>If \(\alpha\) is too large: may not decrease on every iteration; may not converge.</li>
</ul>

<h2 id="toc_4">Computing Parameters Analytically</h2>

<h3 id="toc_5">Normal equation:</h3>

<p>Gradient descent gives one way of minimizing \(J\). The &quot;Normal Equation&quot; method minimizes \(J\) by explicitly taking its derivatives with respect to the \(θj\) ’s, and setting them to zero. This allows us to find the optimum theta without iteration. The normal equation formula is given below:</p>

<p>\[\theta = (X^TX)^{-1}X^Ty\]</p>

<p>Matlab command: </p>

<pre><code class="language-Matlab">pinv(X&#39;*X)*X&#39;*y
</code></pre>

<p>where <code>pinv</code> is <code>peudoinversion</code> of matrix. It is different to <code>inv</code>.</p>

<h3 id="toc_6">Gradient descent v.s. Normal equation</h3>

<p><img src="media/14972614146835/Screen%20Shot%202017-06-27%20at%202.38.45%20PM.png" alt="Screen Shot 2017-06-27 at 2.38.45 P"/></p>

<h3 id="toc_7">Normal Equation Non-invertible</h3>

<p>The common reason causes non-invertible:</p>

<ul>
<li>Redundant features(linearly dependent)
E.g. \(x_1\) = size in feet\(^2\), \(x_2\) = size in m\(^2\)</li>
<li>Too many features(e.g. \(m&lt;=n\)).
-- Delete some features, or use regularization.</li>
</ul>

<p>where \(m\) is the number of training examples, \(n\) is the number of features.</p>

<p>Solutions to the above problems include deleting a feature that is linearly dependent with another or deleting one or more features when there are too many features.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine Learning (1): Introduction to Machine Learning]]></title>
    <link href="http://larryim.cc/14970886401689.html"/>
    <updated>2017-06-10T17:57:20+08:00</updated>
    <id>http://larryim.cc/14970886401689.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">Definition</a>
</li>
<li>
<a href="#toc_1">Examples</a>
</li>
<li>
<a href="#toc_2">Machine learning algorithms</a>
</li>
<li>
<a href="#toc_3">Supervised learning</a>
<ul>
<li>
<a href="#toc_4">Example</a>
</li>
</ul>
</li>
<li>
<a href="#toc_5">Unsupervised Learning</a>
</li>
<li>
<a href="#toc_6">Cost Function</a>
</li>
<li>
<a href="#toc_7">Gradient descent</a>
</li>
</ul>


<h2 id="toc_0">Definition</h2>

<p>Arthur Samuel(1959). Machine Learning: A Field of study that gives computers the ability to learn without being explicitly programmed.</p>

<p>Tom Mitchell(1998) Well-posed Learning Problem: A computer program is said to learn from experience <code>E</code> with respect to some task <code>T</code> and some performance measure <code>P</code>, if its performance on <code>T</code>, as measured by <code>P</code>, improves with experience <code>E</code>.</p>

<p>In general, any machine learning problem can be assigned to one of two broad classifications:</p>

<p><strong>Supervised learning</strong> and <strong>Unsupervised learning</strong>.</p>

<h2 id="toc_1">Examples</h2>

<ul>
<li><p>Database mining</p>

<p>Large datasets from growth of automation/web<br/>
E.g, Web click data, medical records, biology, engineering</p></li>
<li><p>Applications can&#39;t program by hand</p>

<p>E.g., Autonomous helicopter, handwriting recognition, most of Natural Language Processing( NLP), Computer Vision</p></li>
<li><p>Self-customizing programs</p>

<p>E.g. Amazon, Netflix product recommendations</p></li>
<li><p>Understanding human learning (brain, real AI)</p></li>
</ul>

<h2 id="toc_2">Machine learning algorithms</h2>

<ul>
<li>Supervised learning</li>
<li>Unsupervised learning</li>
<li>Reinforcement learning</li>
<li>Recommender systems</li>
</ul>

<h2 id="toc_3">Supervised learning</h2>

<p>In  supervised learning, we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output.</p>

<ul>
<li><strong>Regression problem</strong>: Predict continuous valued output (eg.price)</li>
<li><strong>classification problem</strong>: Predict discrete valued output (eg. tumor or not)</li>
</ul>

<h3 id="toc_4">Example</h3>

<p>Given data about the size of houses on the real estate market, try to predict their price. Price as a function of size is a continuous output, so this is a regression problem.</p>

<p>We could turn this example into a classification problem by instead making our output about whether the house &quot;sells for more or less than the asking price.&quot; Here we are classifying the houses based on price into two discrete categories.</p>

<p><img src="media/14970886401689/Screen%20Shot%202017-06-12%20at%2010.15.14%20AM.png" alt="Screen Shot 2017-06-12 at 10.15.14 A"/></p>

<h2 id="toc_5">Unsupervised Learning</h2>

<p>Unsupervised learning is the task of making an inference from data without the “correct answers” given (unlabeled data). With unsupervised learning there is no feedback based on the prediction results.</p>

<h2 id="toc_6">Cost Function</h2>

<p>We can measure the accuracy of our hypothesis function by using a <strong>cost function</strong>. </p>

<p><strong>Idea</strong>: Choose \(\theta_0, \theta_1\) so that \(h_\theta(x)\) is close to \(y\) for our training examples (x,y)</p>

<p><strong>Hypothesis</strong>: <br/>
\[h_\theta(x) = \theta_0+\theta_1x \]</p>

<p><strong>Parameters</strong>:<br/>
\[\theta_0, \theta_1\]</p>

<p><strong>Cost Function</strong>:<br/>
\[J(\theta_0, \theta_1)=\frac{1}{2m}\Sigma^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2\]</p>

<p>This function is otherwise called the &quot;Squared error function&quot;, or &quot;Mean squared error&quot;. </p>

<p><strong>Goal</strong>:<br/>
\[ \min \limits_{\theta_0,\theta_1} J(\theta_0, \theta_1) \]</p>

<h2 id="toc_7">Gradient descent</h2>

<p>Have some function Goal: \(J(\theta_0, \theta_1) \)</p>

<p>Want \( \min \limits_{\theta_0,\theta_1} J(\theta_0, \theta_1) \)</p>

<p><strong>Outline</strong>:</p>

<ul>
<li>Start with some \(\theta_0,\theta_1\)</li>
<li>Keep changing \(\theta_0,\theta_1\) to reduce \(J(\theta_0,\theta_1)\) until we hopefully end up at a minimum</li>
</ul>

<p><strong>Gradient descent algorithm</strong>:<br/>
repeat until converge{</p>

<p>\(\theta_j := \theta_j -\alpha\frac{\partial}{\partial \theta_j}J(\theta_0,\theta_1)\)  (for \(j=0\) and \(j=1\))</p>

<p>注：</p>

<ol>
<li><code>:=</code> is assignment, not truth assertion</li>
</ol>

<p>2.At each iteration j, one should simultaneously update the parameters \(\theta_0,\theta_1\). Updating a specific parameter prior to calculating another one on the j(th) iteration would yield to a wrong implementation.</p>

<p>\(temp0 := \theta_0 -\alpha\frac{\partial}{\partial \theta_0}J(\theta_0,\theta_1)\)</p>

<p>\(temp1 := \theta_1 -\alpha\frac{\partial}{\partial \theta_1}J(\theta_0,\theta_1)\)</p>

<p>\(\theta_0:=temp0\)<br/>
\(\theta_1:=temp1\)</p>

<ol>
<li>\(\alpha\) is learning rate: if \(\alpha\) is too small, gradient descent can be slow. If \(\alpha\)  is too large, gradient descent can overshoot the minimum. It may fail to converge, or even diverge.</li>
<li>As we approach a local minimum, gradient descent will automatically take smaller steps. So, no need to decrease \(\alpha\) over time.</li>
</ol>

<p>convex function(often Bowl shaped function)</p>

<p><strong>Batch Gradient Descent</strong>: Each step of gradient descent uses all the training examples.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[调试]]></title>
    <link href="http://larryim.cc/14970832553234.html"/>
    <updated>2017-06-10T16:27:35+08:00</updated>
    <id>http://larryim.cc/14970832553234.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">pdb</h2>

<p>pdb是基于命令行的调试工具，是Python的调试器，非常类似GNU的GDB（调试c/c++）</p>

<pre><code class="language-python">python -m pdb some.py # -m是模块的意思, pdb模块
</code></pre>

<h3 id="toc_1">命令</h3>

<ul>
<li>输入命令<code>l</code> (list) 查看代码</li>
<li>输入命令<code>n</code> (next) 可以单步执行代码</li>
<li>输入命令<code>c</code> (continue) 继续执行代码</li>
<li>输入命令<code>b</code> (break) 添加断点</li>
<li>输入命令<code>clear</code> 清除断点</li>
<li>输入命令<code>p</code> (print) 打印一个变量的值</li>
<li>输入命令<code>s</code> (step) 进入到一个函数</li>
<li>输入命令<code>a</code> (args) 打印所有的形参数据</li>
<li>输入命令<code>r</code> (return) 快速执行到函数的最后一行</li>
<li>输入命令<code>q</code> quit 退出调试</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python排序]]></title>
    <link href="http://larryim.cc/14970829036232.html"/>
    <updated>2017-06-10T16:21:43+08:00</updated>
    <id>http://larryim.cc/14970829036232.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">排序算法的稳定性及意义</a>
</li>
<li>
<a href="#toc_1">冒泡排序</a>
<ul>
<li>
<a href="#toc_2">复杂度与稳定性</a>
</li>
</ul>
</li>
<li>
<a href="#toc_3">选择排序</a>
</li>
<li>
<a href="#toc_4">插入排序</a>
</li>
<li>
<a href="#toc_5">希尔排序</a>
</li>
<li>
<a href="#toc_6">快速排序</a>
</li>
<li>
<a href="#toc_7">归并排序</a>
<ul>
<li>
<a href="#toc_8">分治法</a>
</li>
</ul>
</li>
<li>
<a href="#toc_9">常见排序算法效率比较</a>
</li>
</ul>


<h2 id="toc_0">排序算法的稳定性及意义</h2>

<p>在待排序的序列中，存在具有相同关键字的记录，在排序后这些记录的相对次序保持不变，则排序算法是稳定的。</p>

<p>不稳定排序无法完成多个关键字的排序。例如整数排序，位数越高的数字优先级越高，从高位数到低位数一次排序。那么每一位的排序都需要稳定算法，否则无法得到正确的结果。</p>

<p>即，<strong>当要对多个关键词多次排序时，必须使用稳定算法</strong></p>

<h2 id="toc_1">冒泡排序</h2>

<p><img src="media/14970829036232/Screen%20Shot%202017-06-11%20at%2010.23.12%20AM.png" alt="Screen Shot 2017-06-11 at 10.23.12 A"/></p>

<pre><code class="language-python">def bubble_sort(alist):
    &quot;&quot;&quot;
    冒泡排序
    &quot;&quot;&quot;
    if len(alist) &lt;= 1:
        return alist

    for j in range(len(alist)-1,0,-1):
        for i in range(j):
            if alist[i] &gt; alist[i+1]:
                alist[i], alist[i+1] = alist[i+1], alist[i]

    return alist
</code></pre>

<h3 id="toc_2">复杂度与稳定性</h3>

<ul>
<li>最优时间复杂度：\(O(n)\) 遍历没有发现任何可以交换的元素，排序结束</li>
<li>最坏时间复杂度：\(O(n^2)\)</li>
<li>稳定性：稳定</li>
</ul>

<h2 id="toc_3">选择排序</h2>

<p>选择排序（Selection sort）是一种简单直观的排序算法。它的工作原理如下。首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。</p>

<h2 id="toc_4">插入排序</h2>

<p>插入排序通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。插入排序在实现上，在从后向前扫描过程中，需要反复把已排序元素逐步向后挪位，为最新元素提供插入空间。</p>

<p><img src="media/14970829036232/Screen%20Shot%202017-06-12%20at%207.07.03%20PM.png" alt="Screen Shot 2017-06-12 at 7.07.03 P"/></p>

<pre><code class="language-python">def insert_sort(alist):
    &quot;&quot;&quot;
    插入排序
    &quot;&quot;&quot;
    n = len(alist)
    if n &lt;= 1:
        return alist

    # 从第二个位置，即下表为1的元素开始向前插入
    for i in range(1, n):
        j = i
        # 向前向前比较，如果小于前一个元素，交换两个元素
        while alist[j] &lt; alist[j-1] and j &gt; 0:
            alist[j], alist[j-1] = alist[j-1], alist[j]
            j-=1
    return alist
</code></pre>

<p>复杂度与稳定性</p>

<ul>
<li>最优时间复杂度：O(\(n\)) （升序排列，序列已经处于升序状态）</li>
<li>最坏时间复杂度：O(\(n^2\))</li>
<li>稳定性：稳定</li>
</ul>

<h2 id="toc_5">希尔排序</h2>

<p>希尔排序(Shell Sort)是插入排序的改进, 排序非稳定。希尔排序是把记录按下标的一定<em>增量</em>分组，对每组使用直接插入排序算法排序；随着增量逐渐减少，每组包含的关键词越来越多，当增量减至1时，整个文件恰被分成一组，算法便终止。</p>

<pre><code class="language-python">def shell_sort(alist):
    
    n = len(alist)
    gap = n//2
    
    # gap 变化到0之前，插入算法之行的次数
    while gap &gt; 0:
        
        # 希尔排序， 与普通的插入算法的区别就是gap步长
        for i in range(gap,n):
            j = i
            while alist[j] &lt; alist[j-gap] and j &gt; 0:
                alist[j], alist[j-gap] = alist[j-gap], alist[j]
                j-=gap
    
        gap = gap//2

    return alist
</code></pre>

<p>复杂度与稳定性</p>

<ul>
<li>最优时间复杂度：\(O(n^{1.3})\) （不要求本身有序）</li>
<li>最坏时间复杂度：\(O(n^2)\)</li>
<li>稳定性：不稳定</li>
</ul>

<h2 id="toc_6">快速排序</h2>

<p>快速排序(Quicksort)，通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列。</p>

<p>步骤为：</p>

<ol>
<li>从数列中挑出一个元素，称为&quot;基准&quot;(pivot)</li>
<li>重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区结束之后，该基准就处于数列的中间位置。这个称为分区(partition)操作。</li>
<li>递归地(recursive)把小于基准值元素的子数列和大于基准值元素的子数列排序。</li>
</ol>

<p>递归的最底部情形，是数列的大小是零或一，也就是永远都已经被排序好了。虽然一直递归下去，但是这个算法总会结束，因为在每次的迭代(iteration)中，它至少会把一个元素摆到它最后的位置去。</p>

<h2 id="toc_7">归并排序</h2>

<p>归并排序是采用分治法的一个非常典型的应用。归并排序的思想就是先递归分解数组，再合并数组。</p>

<p>将数组分解最小之后，然后合并两个有序数组，基本思路是比较两个数组的最前面的数，谁小就先取谁，取了后相应的指针就往后移一位。然后再比较，直至一个数组为空，最后把另一个数组的剩余部分复制过来即可。</p>

<h3 id="toc_8">分治法</h3>

<p>分治法的思想：将原问题分解为几个规模较小但类似于原问题的子问题，递归地求解这些子问题，然后再合并这些子问题的解来建立原问题的解。</p>

<p>分治模式在每层递归时都有三个步骤：</p>

<ul>
<li><strong>分解</strong>原问题为若干子问题，这些子问题是原问题的规模较小的实例</li>
<li><strong>解决</strong>这些子问题，递归地求解各子问题。然而，若子问题的规模足够小，则直接求解</li>
<li><strong>合并</strong>这些子问题的解成原问题的解</li>
</ul>

<p>归并排序算法完全遵循分治模式。直观上其操作如下：</p>

<ul>
<li><strong>分解</strong>：分解待排序的n个元素的序列成各具n/2个元素的两个子序列</li>
<li><strong>解决</strong>：使用归并排序递归地排序两个子序列</li>
<li><strong>合并</strong>：合并两个已排序的子序列以产生已排序的答案。</li>
</ul>

<h2 id="toc_9">常见排序算法效率比较</h2>

<p><img src="media/14970829036232/14972715837154.jpg" alt=""/></p>

]]></content>
  </entry>
  
</feed>
