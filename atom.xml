<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[techlarry]]></title>
  <link href="http://larryim.cc/atom.xml" rel="self"/>
  <link href="http://larryim.cc/"/>
  <updated>2017-12-14T00:39:42+08:00</updated>
  <id>http://larryim.cc/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im/">MWeb</generator>
  
  <entry>
    <title type="html"><![CDATA[CMU 15-213 Introduction to Computer Systems]]></title>
    <link href="http://larryim.cc/15132289229128.html"/>
    <updated>2017-12-14T13:22:02+08:00</updated>
    <id>http://larryim.cc/15132289229128.html</id>
    <content type="html"><![CDATA[
<p>CMU 15-213 <code>Introduction to Computer Systems</code>可能是世界上最受欢迎的一门课。与这门课对应的课本CSAPP在全球被广泛采用，也被无数学生拜读。课程内容涉及广泛、由浅入深，是进入计算机科学的最佳课程，也是各种击破BAT笔试题的必备良药(<a href="https://book.douban.com/review/5627139/">这篇帖子描述了笔试题所对应的章节</a>)。</p>

<p>非常幸运的是，CMU(Carnegie Mellon University)在网上分享了上课视频和其他所有资料，点击<a href="https://scs.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx#folderID=%22b96d90ae-9871-4fae-91e2-b1627b43e25e%22&amp;sortColumn=0&amp;sortAscending=true">这里</a>观看。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Nand2Tetris: The elements of computing systems]]></title>
    <link href="http://larryim.cc/nand2tetris.html"/>
    <updated>2017-12-09T14:45:42+08:00</updated>
    <id>http://larryim.cc/nand2tetris.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">1.布尔逻辑 Boolean Logic</a>
</li>
<li>
<a href="#toc_1">2.布尔运算 Boolean Arithmetic</a>
</li>
<li>
<a href="#toc_2">3.时序逻辑 Sequential Logic</a>
</li>
<li>
<a href="#toc_3">4.机器语言 Machine Language</a>
</li>
<li>
<a href="#toc_4">5.计算机体系结构 Computer Architecture</a>
</li>
</ul>


<p><code>Nand2Tetris 计算机系统要素</code>是一门教你从零开始构建现代计算机的课程。该课程已经在Coursera上开设(分为上下两门课程)，有免费的课本和课程需要的代码，也有专门的论坛来交流学习，课程相关资源如下：</p>

<ul>
<li><a href="https://www.coursera.org/learn/build-a-computer">Coursera Course Part1</a></li>
<li><a href="https://www.coursera.org/learn/nand2tetris2">Coursera Course Part2</a></li>
<li><a href="http://nand2tetris.org">Nand2Tetris Project Homepage</a></li>
<li><a href="http://nand2tetris-questions-and-answers-forum.32033.n3.nabble.com">Nand2Tetris Questions and Answer Forum</a></li>
<li><a href="https://book.douban.com/subject/1998645/">Textbook: The elements of computing system</a></li>
</ul>

<p>该课程适合学生掌握了计算机入门课程和数据结构以后学习，课程涉及布尔逻辑及运算、机器语言、计算机体系结构、汇编、虚拟机、编译器、操作系统。课程内容与CMU的深入理解计算机系统不一样，个人感觉涉及面更广、更加注重实战。</p>

<h2 id="toc_0">1.布尔逻辑 Boolean Logic</h2>

<p><strong>多通道/多位Multiplexor</strong>: 一个拥有m个通道、每个通道数据宽度为n位的multiplexor选择器，将m个n位输入变量中选择一个并从其单一的n位输出总线上输出。可以用k个控制位来指定这个选择(\(k=\log_2 m\))。</p>

<p><strong>例</strong>：4通道16位的multiplexor(4-way Multiplexor):</p>

<p><img src="media/15128019428341/4-way%20multiplexor.png" alt="4-way multiplexo"/></p>

<p><strong>多通道/多位Demultiplexor</strong>: m通道、n位的demult<br/>
iplexor从m个可能的n位输出通道中选择一个n位的输入变量。可以用k个控制位来指定这个选择(\(k=\log_2 m\))。</p>

<p><strong>例</strong>：4-way Demultiplexor:</p>

<p><img src="media/15128019428341/4-way%20demultiplexor.png" alt="4-way demultiplexo"/></p>

<pre><code>/**
 * 4-way demultiplexor:
 * {a, b, c, d} = {in, 0, 0, 0} if sel == 00
 *                {0, in, 0, 0} if sel == 01
 *                {0, 0, in, 0} if sel == 10
 *                {0, 0, 0, in} if sel == 11
 */

CHIP DMux4Way {
    IN in, sel[2];
    OUT a, b, c, d;

    PARTS:
    DMux(in=in, sel=sel[1], a=c1, b=c2);
    DMux(in=c1, sel=sel[0], a=a, b=b);
    DMux(in=c2, sel=sel[0], a=c, b=d);

}
</code></pre>

<h2 id="toc_1">2.布尔运算 Boolean Arithmetic</h2>

<p>半加器(Half Adder): 进行二进制数加法的第一步就是要能够对两个二进制位进行相加。我们把结果的LSB(Least Significant Bit)称为sum，MSB(Most Significant Bit)称为carry。</p>

<p><img src="media/15128019428341/Half%20Adder.png" alt="Half Adde"/></p>

<p>HDL语言：</p>

<pre><code>CHIP HalfAdder {
    IN a, b;    // 1-bit inputs
    OUT sum,    // Right bit of a + b 
        carry;  // Left bit of a + b

    PARTS:
    And(a=a, b=b, out=carry);
    Xor(a=a, b=b, out=sum);
}
</code></pre>

<p>全加器：用来对三个位相加。</p>

<p><img src="media/15128019428341/Full%20Adder.png" alt="Full Adde"/></p>

<p>HDL 语言：</p>

<pre><code>CHIP FullAdder {
    IN a, b, c;  // 1-bit inputs
    OUT sum,     // Right bit of a + b + c
        carry;   // Left bit of a + b + c

    PARTS:
    HalfAdder(a=a, b=b, sum=sum1, carry=carry1);
    HalfAdder(a=c, b=sum1, sum=sum, carry=carry2);
    Or(a=carry1, b=carry2, out=carry);
}
</code></pre>

<p>加法器(Adder): 存储器和寄存器电路用n位的形式来表示整数，n可以是16、32、64等等, 这依赖于所在的计算机平台。进行n位加法的芯片称为多位加法器(multi-bit adder),或者简称为加法器。下图为16位加法器</p>

<p><img src="media/15128019428341/16-bits%20Adder.png" alt="16-bits Adde"/></p>

<p>HDL:</p>

<pre><code>CHIP Add16 {
    IN a[16], b[16];
    OUT out[16];

    PARTS:
   HalfAdder(a=a[0], b=b[0], carry=carry1, sum=out[0]);
   FullAdder(a=a[1], b=b[1], c=carry1, carry=carry2, sum=out[1]);
   FullAdder(a=a[2], b=b[2], c=carry2, carry=carry3, sum=out[2]);
   FullAdder(a=a[3], b=b[3], c=carry3, carry=carry4, sum=out[3]);
   FullAdder(a=a[4], b=b[4], c=carry4, carry=carry5, sum=out[4]);
   FullAdder(a=a[5], b=b[5], c=carry5, carry=carry6, sum=out[5]);
   FullAdder(a=a[6], b=b[6], c=carry6, carry=carry7, sum=out[6]);
   FullAdder(a=a[7], b=b[7], c=carry7, carry=carry8, sum=out[7]);
   FullAdder(a=a[8], b=b[8], c=carry8, carry=carry9, sum=out[8]);
   FullAdder(a=a[9], b=b[9], c=carry9, carry=carry10, sum=out[9]);
   FullAdder(a=a[10], b=b[10], c=carry10, carry=carry11, sum=out[10]);
   FullAdder(a=a[11], b=b[11], c=carry11, carry=carry12, sum=out[11]);
   FullAdder(a=a[12], b=b[12], c=carry12, carry=carry13, sum=out[12]);
   FullAdder(a=a[13], b=b[13], c=carry13, carry=carry14, sum=out[13]);
   FullAdder(a=a[14], b=b[14], c=carry14, carry=carry15, sum=out[14]);
   FullAdder(a=a[15], b=b[15], c=carry15, carry=carry, sum=out[15]);

}
</code></pre>

<p>算数逻辑单元(Arithmetic Logic Unit, ALU): 计算一组固定的函数\(out=f_i(x,y)\),这里\(x\)和\(y\)是芯片的两个16位输入，\(out\)是芯片的16位输出, \(f_i\)是位于一个函数表中的函数，该函数表通过6个控制位(control bit)的输入位(zx, nx, zy, ny, f, no)来告诉ALU用哪一个函数来进行何种计算。</p>

<p><img src="media/15128019428341/ALU.png" alt="ALU"/></p>

<p>HDL:</p>

<pre><code>*
 * The ALU (Arithmetic Logic Unit).
 * Computes one of the following functions:
 * x+y, x-y, y-x, 0, 1, -1, x, y, -x, -y, !x, !y,
 * x+1, y+1, x-1, y-1, x&amp;y, x|y on two 16-bit inputs, 
 * according to 6 input bits denoted zx,nx,zy,ny,f,no.
 * In addition, the ALU computes two 1-bit outputs:
 * if the ALU output == 0, zr is set to 1; otherwise zr is set to 0;
 * if the ALU output &lt; 0, ng is set to 1; otherwise ng is set to 0.
 */

// Implementation: the ALU logic manipulates the x and y inputs
// and operates on the resulting values, as follows:
// if (zx == 1) set x = 0        // 16-bit constant
// if (nx == 1) set x = !x       // bitwise not
// if (zy == 1) set y = 0        // 16-bit constant
// if (ny == 1) set y = !y       // bitwise not
// if (f == 1)  set out = x + y  // integer 2&#39;s complement addition
// if (f == 0)  set out = x &amp; y  // bitwise and
// if (no == 1) set out = !out   // bitwise not
// if (out == 0) set zr = 1
// if (out &lt; 0) set ng = 1

CHIP ALU {
    IN  
        x[16], y[16],  // 16-bit inputs        
        zx, // zero the x input?
        nx, // negate the x input?
        zy, // zero the y input?
        ny, // negate the y input?
        f,  // compute out = x + y (if 1) or x &amp; y (if 0)
        no; // negate the out output?

    OUT 
        out[16], // 16-bit output
        zr, // 1 if (out == 0), 0 otherwise
        ng; // 1 if (out &lt; 0),  0 otherwise

    PARTS:
    // if (zx==1) set x = 0
    Mux16(a=x,b=false,sel=zx,out=zxout);

    // if (zy==1) set y = 0
    Mux16(a=y,b=false,sel=zy,out=zyout); 

    // if (nx==1) set x = ~x
    // if (ny==1) set y = ~y  
    Not16(in=zxout,out=notx);
    Not16(in=zyout,out=noty);
    Mux16(a=zxout,b=notx,sel=nx,out=nxout); 
    Mux16(a=zyout,b=noty,sel=ny,out=nyout);

    // if (f==1)  set out = x + y 
    // if (f==0)  set out = x &amp; y
    Add16(a=nxout,b=nyout,out=addout);
    And16(a=nxout,b=nyout,out=andout);
    Mux16(a=andout,b=addout,sel=f,out=fout);
    
    // if (no==1) set out = ~out
    // 1 if (out&lt;0),  0 otherwise
    Not16(in=fout,out=nfout);
    Mux16(a=fout,b=nfout,sel=no,out=out,out[0..7]=zr1,out[8..15]=zr2,out[15]=ng);
    
    //if (out==0), zr=1
    Or8Way(in=zr1,out=or1);
    Or8Way(in=zr2,out=or2);
    Or(a=or1,b=or2,out=or3);
    Not(in=or3,out=zr);

}
</code></pre>

<h2 id="toc_2">3.时序逻辑 Sequential Logic</h2>

<p>时钟(Clock)：在大多数计算机里，时间的流逝是用主时钟(master clock)来表示的，它提供连续的交变信号序列。其精确的硬件实现通常基于振荡器(oscillator),其在两个信号值0-1之间交替变化。两个响铃的时间间隔称为时钟的周期。</p>

<p>触发器(Flip-Flop): 计算机里最基本的时序单元，有多个变种。数据触发器(Data Flip-Flop, <code>DFF</code>)，其接口包含1比特位输入和1比特位输出。 <u>DFF简单地将前一个时间周期的输入值作为当前周期的输出</u> 。</p>

<p><img src="media/15128019428341/flip-flop.png" alt="flip-flop"/></p>

<p>寄存器(Registers): 具有记忆功能的设备，能“存储”某一时刻的值，实现经典的存储行为<code>out(t)=out(t-1)</code>。从另一个方面来说，DFF仅能够输出它前一时钟周期的输入，也就是<code>out(t)=in(t-1)</code>。寄存器分为地址寄存器(address register), 数据寄存器(data register), 程序计数器(program counter).</p>

<p><img src="media/15128019428341/registers.png" alt="registers"/></p>

<p>1-位寄存器:</p>

<pre><code class="language-hdl">/**
 * 1-bit register:
 * If load[t] == 1 then out[t+1] = in[t]
 *                 else out does not change (out[t+1] = out[t])
 */

CHIP Bit {
    IN in, load;
    OUT out;

    PARTS:
    Mux(a= out1, b=in,  sel=load, out=out2);
    DFF(in=out2, out=out1, out=out);
}
</code></pre>

<p><strong>寄存器</strong>：由1-位寄存器来构建w-位寄存器是非常简单的。所需要做的就是构建一组w比特门，然后将寄存器的load输入赋予每个门。</p>

<pre><code>CHIP Register {
    IN in[16], load;
    OUT out[16];

    PARTS:
    Bit(in=in[0], load=load, out=out[0]);
    Bit(in=in[1], load=load, out=out[1]);
    Bit(in=in[2], load=load, out=out[2]);
    Bit(in=in[3], load=load, out=out[3]);
    Bit(in=in[4], load=load, out=out[4]);
    Bit(in=in[5], load=load, out=out[5]);
    Bit(in=in[6], load=load, out=out[6]);
    Bit(in=in[7], load=load, out=out[7]);
    Bit(in=in[8], load=load, out=out[8]);
    Bit(in=in[9], load=load, out=out[9]);
    Bit(in=in[10], load=load, out=out[10]);
    Bit(in=in[11], load=load, out=out[11]);
    Bit(in=in[12], load=load, out=out[12]);
    Bit(in=in[13], load=load, out=out[13]);
    Bit(in=in[14], load=load, out=out[14]);
    Bit(in=in[15], load=load, out=out[15]);
}
</code></pre>

<p><strong>内存</strong>(Memories):可以通过将寄存器堆叠起来形成 <u>随机存取存储器</u> (random access memory, <code>RAM</code>)。在RAM上能够随机访问被选择的字而不会受限于访问顺序，即随机存取存储器中的任何字都能以相等的速度被直接访问。</p>

<p><img src="media/15128019428341/RAM.png" alt="RA"/></p>

<p>RAM8:</p>

<pre><code class="language-hdl">/**
 * Memory of 8 registers, each 16 bit-wide. Out holds the value
 * stored at the memory location specified by address. If load==1, then 
 * the in value is loaded into the memory location specified by address 
 * (the loaded value will be emitted to out from the next time step onward).
 */

CHIP RAM8 {
    IN in[16], load, address[3];
    OUT out[16];

    PARTS:
    // Put your code here:
    DMux8Way(in=load, sel=address, a=loada, b=loadb, c=loadc, d=loadd, e=loade, f=loadf, g=loadg, h=loadh);
    Register(in=in, load=loada, out=outa);
    Register(in=in, load=loadb, out=outb);
    Register(in=in, load=loadc, out=outc);
    Register(in=in, load=loadd, out=outd);
    Register(in=in, load=loade, out=oute);
    Register(in=in, load=loadf, out=outf);
    Register(in=in, load=loadg, out=outg);
    Register(in=in, load=loadh, out=outh);
    Mux8Way16(a=outa, b=outb, c=outc, d=outd, e=oute, f=outf, g=outg, h=outh, sel=address, out=out);
}
</code></pre>

<p>RAM64:</p>

<pre><code>/**
 * Memory of 64 registers, each 16 bit-wide. Out holds the value
 * stored at the memory location specified by address. If load==1, then 
 * the in value is loaded into the memory location specified by address 
 * (the loaded value will be emitted to out from the next time step onward).
 */

CHIP RAM64 {
    IN in[16], load, address[6];
    OUT out[16];

    PARTS:
    DMux8Way(in=load, sel=address[3..5], a=loada, b=loadb, c=loadc, d=loadd, e=loade, f=loadf, g=loadg, h=loadh);
    RAM8(in=in, load=loada, address=address[0..2], out=outa);
    RAM8(in=in, load=loadb, address=address[0..2], out=outb);
    RAM8(in=in, load=loadc, address=address[0..2], out=outc);
    RAM8(in=in, load=loadd, address=address[0..2], out=outd);
    RAM8(in=in, load=loade, address=address[0..2], out=oute);
    RAM8(in=in, load=loadf, address=address[0..2], out=outf);
    RAM8(in=in, load=loadg, address=address[0..2], out=outg);
    RAM8(in=in, load=loadh, address=address[0..2], out=outh);
    Mux8Way16(a=outa, b=outb, c=outc, d=outd, e=oute, f=outf, g=outg, h=outh, sel=address[3..5], out=out);  
}
</code></pre>

<p>计算机芯片分为两种：组合芯片(<code>combinational chip</code>)和时序芯片(<code>sequential chip</code> or <code>clocked chip</code>时钟芯片)。组合芯片的操作具有即时性(<code>instantaneous</code>)。时序芯片的操作受时钟控制，输入的改变只有在下一个时钟周期才反应到芯片的输出管脚上: <code>out(t)=out(t-1)</code>。</p>

<p><img src="media/15128019428341/combintional%20chip%20and%20sequential%20chip.png" alt="combintional chip and sequential chip"/></p>

<p>计数器(Counter): 计数器是一种时序芯片，它的状态是证书，每经过一个时间周期，该整数就增加1个单位，执行函数<code>out(t)=out(t-1)+c</code>，这里就是1。典型的CPU包括一个程序计数器(program couter, <code>PC</code>)，它的输出是当前程序中下一步将要执行的指令地址。</p>

<p>w-位计数器包含两个主要部分：一个常规的w-位寄存器和组合逻辑。组合逻辑用来：(a)执行计数功能；(b)根据控制位的3种不同的命令值，将计数器置于正确的操作模式。</p>

<p><img src="media/15128019428341/PC.png" alt="PC"/></p>

<pre><code>/**
 * A 16-bit counter with load and reset control bits.
 * if      (reset[t] == 1) out[t+1] = 0
 * else if (load[t] == 1)  out[t+1] = in[t]
 * else if (inc[t] == 1)   out[t+1] = out[t] + 1  (integer addition)
 * else                    out[t+1] = out[t]
 */
 
CHIP PC {
    IN in[16],load,inc,reset;
    OUT out[16];

    PARTS:
    Inc16(in=regout, out=plusone);
    Mux16(a=false, b=plusone, sel=inc, out=incout);
    Mux16(a=incout, b=in, sel=load, out=loadout);
    Mux16(a=loadout, b=false, sel=reset, out=toload);
    Or(a=load, b=reset, out=loadorreset);
    Or(a=loadorreset, b=inc, out=loadflag);
    Register(in=toload, load=loadflag, out=regout);
    Or16(a=regout, b=regout, out=out);
}
</code></pre>

<p>分级存储器体系(<a href="https://en.wikipedia.org/wiki/Memory_hierarchy">memory hierarchy</a>):</p>

<p><img src="media/15128019428341/memory%20hierarchy.png" alt="memory hierarchy"/></p>

<p><img src="media/15128019428341/15128077150126.png" alt=""/></p>

<h2 id="toc_3">4.机器语言 Machine Language</h2>

<p>机器语言用来对底层程序进行编码，从而形成一系列机器指令。应用这些指令，程序员可以命令处理器执行算数和逻辑餐座，在内存中进行存取操作，让数据在寄存器之间传递，验证布尔表达式的值，等等。机器语言的设计目标是直接运行在指定的硬件平台上，能够对这个平台进行全面操控；与机器语言相反，高级语言具有通用性和较强的表达能力。</p>

<p>机器语言是整个计算机题系统意义最深奥的接口 -- 它也是硬件和软件相接的中间线。因此，既可以将机器语言看作编程工作，也可以将其看作硬件平台内部不可分割的一部分。</p>

<h2 id="toc_4">5.计算机体系结构 Computer Architecture</h2>

<p>存储程序的概念：指令没有被嵌入到硬件中，而是被存储在计算机的存储设备(memory)里，当计算机载入不同的程序指令时，同样的硬件平台可以实现不同的功能。</p>

<p>冯·诺伊曼体系结构(概念上的):冯·诺伊曼体系结构的基础是一个中央处理单元(CPU),它与内存进行交互，负责从输入设备接收数据，向输出设备发送数据。</p>

<p><img src="media/15128019428341/%E5%86%AF%E8%AF%BA%E4%BC%8A%E6%9B%BC.png" alt="冯诺伊曼"/></p>

<p>RAM: RAM不光存储数据和指令，还可以通过 <u>内存映像</u> (Memory Maps) 在CPU和计算机输入/输出设备之间充当借口。下图中屏幕和键盘都通过内存映像与计算机平台进行接口。</p>

<p><img src="media/15128019428341/memory.png" alt="memory"/></p>

<pre><code>CHIP Memory {
    IN in[16], load, address[15];
    OUT out[16];

    PARTS:

    DMux4Way(in=load, sel=address[13..14], a=ram1, b=ram2, c=screen, d=kbd);
    Or(a=ram1, b=ram2, out=ram);
    RAM16K(in=in, load=ram, address=address[0..13], out=ramout);
    Screen(in=in, load=screen, address=address[0..12], out=scrout);
    Keyboard(out=kbout);
    Mux4Way16(a=ramout, b=ramout, c=scrout, d=kbout, sel=address[13..14], out=out);
}
</code></pre>

<p>CPU包括执行指令的ALU、一组寄存器和一些用于取指令和对指令解码的控制逻辑(上上图)。用来执行下面的任务：</p>

<ul>
<li>指令解码(Instruction decoding):解析出指令所代表意思(指令的功能)。</li>
<li>指令执行(Instruction execution):发信号指示计算机的各个部分应该做什么工作来执行指令（指令的功能）。</li>
<li>读取下一条指令(Next instruction fetching):指出下一步执行哪一条指令（指令的功能以及ALU的输出）。</li>
</ul>

<p><img src="media/15128019428341/CPU_in_out.png" alt="CPU_in_out"/></p>

<pre><code>
CHIP CPU {

    IN  inM[16],         // M value input  (M = contents of RAM[A])
        instruction[16], // Instruction for execution
        reset;           // Signals whether to re-start the current program
                         // (reset == 1) or continue executing the current
                         // program (reset == 0).

    OUT outM[16],        // M value output
        writeM,          // Write into M? 
        addressM[15],    // RAM address (of M)
        pc[15];          // ROM address (of next instruction)

    PARTS:
    // get type of instruction
    Not(in=instruction[15], out=Ainstruction);
    Not(in=Ainstruction, out=Cinstruction);
    
    And(a=Cinstruction, b=instruction[5], out=ALUtoA);    // C-inst and dest to A-reg?
    Mux16(a=instruction, b=ALUout, sel=ALUtoA, out=Aregin);
    
    Or(a=Ainstruction, b=ALUtoA, out=loadA);    // load A if A-inst or C-inst&amp;dest to A-reg
    ARegister(in=Aregin, load=loadA, out=Aout);
    
    Mux16(a=Aout, b=inM, sel=instruction[12], out=AMout);   // select A or M based on a-bit

    And(a=Cinstruction, b=instruction[4], out=loadD);
    DRegister(in=ALUout, load=loadD, out=Dout);    // load the D register from ALU
    
    ALU(x=Dout, y=AMout, zx=instruction[11], nx=instruction[10], 
        zy=instruction[9], ny=instruction[8], f=instruction[7],
        no=instruction[6], out=ALUout, zr=ZRout, ng=NGout); // calculate
        
    // Set outputs for writing memory
    Or16(a=false, b=Aout, out[0..14]=addressM);
    Or16(a=false, b=ALUout, out=outM);
    And(a=Cinstruction, b=instruction[3], out=writeM);
    
    // calc PCload &amp; PCinc - whether to load PC with A reg
    And(a=ZRout, b=instruction[1], out=jeq);    // is zero and jump if zero
    And(a=NGout, b=instruction[2], out=jlt);    // is neg and jump if neg
    Or(a=ZRout, b=NGout, out=zeroOrNeg);
    Not(in=zeroOrNeg, out=positive);            // is positive (not zero and not neg)
    And(a=positive, b=instruction[0], out=jgt); // is pos and jump if pos
    Or(a=jeq, b=jlt, out=jle);
    Or(a=jle, b=jgt, out=jumpToA);              // load PC if cond met and jump if cond
    And(a=Cinstruction, b=jumpToA, out=PCload); // Only jump if C instruction
    Not(in=PCload, out=PCinc);                  // only inc if not load
    PC(in=Aout, inc=PCinc, load=PCload, reset=reset, out[0..14]=pc);
}
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[PCA基础及在Spark中的应用]]></title>
    <link href="http://larryim.cc/PCA.html"/>
    <updated>2017-11-28T22:32:35+08:00</updated>
    <id>http://larryim.cc/PCA.html</id>
    <content type="html"><![CDATA[
<p>主成分分析(Principal Component Analysis, PCA)主要是用来可视化数据和数据压缩。</p>

<p>PCA的目的是找到一组可以代表原始数据的低维数据。设原始数据为\(X\in \mathbf{R}^{n\times d}\)，有\(d\)个特征。压缩后的数据为\(\mathbf{Z=XP} \in \mathbf{R}^{n\times k}\), 有\(k\)个特征(feature)。</p>

<p>那么找到的这\(k\)个特征，有什么好的约束呢？<br/>
（Variance/Covariance constraints）</p>

<ul>
<li>这k个特征无关，也就是说协相关矩阵除对角线以外的元素全部为0</li>
<li>应该根据特征的方差(variance)选择，越大越好。因为variance越大，越能代表feature。也就是说应该选择协相关矩阵对角元素大的feature。</li>
</ul>

<p>所以 \(\mathbf{P}\)应该等于X的协方差矩阵的最大\(k\)个特征向量。</p>

<p>协方差矩阵的分布式求解方法见本博客<a href="http://larryim.cc/covariance_spark.html">Covariance Matrix and its solution in Spark</a></p>

<p>下面是特征分解的基础知识，可以在线性代数课本上找到：</p>

<p>特征分解(Eigendecomposition)是将矩阵分解为由特征值和特征向量表示矩阵之积的方法。</p>

<p>令 \(A\)是一个\(N\times N\)的方阵，且有\(N\)个线性无关的特征向量\(q_i(i=1,...,N)\)。这样，A可以被分解为<br/>
\[\mathbf{A=Q\Lambda Q}^{-1}\]<br/>
其中\(\mathbf{Q}\)是\(N\times N\)的方阵，且其第\(i\)列为\(A\)的特征向量\(q_i\)。\(\mathbf{\Lambda}\)是对角矩阵，其对角线上的元素为对应的特征值，也即\(\Lambda_{ii}=\lambda_i\).</p>

<p>那么，怎么样选择\(k\)的大小呢？一般认为保留99%或95%的variance. 由于协方差矩阵的对角元素就是对应特征向量的特征值，又是variance，所以我们选择k个对角元素，保留所要求的variance:</p>

<p>\[\text{find} \min k \quad \text{s.t.} \frac{\sum^k_{i=1}\lambda_i}{\sum^k_{i=1}\lambda_i} &gt; 99\%\]</p>

<h2 id="toc_0">PCA in Spark</h2>

<p>在python 中使用 <code>numpy.linalg.eigh</code>计算特征值和特征向量.</p>

<p>基本步骤如下：</p>

<ul>
<li>计算协方差矩阵 <code>estimateCovariance()</code></li>
<li>计算主成分和对应方差 <code>pca()</code></li>
<li>计算保留的成分比例<code>varianceExplained()</code></li>
</ul>

<pre><code class="language-Python">def estimateCovariance(data):
    &quot;&quot;&quot;Compute the covariance matrix for a given rdd.

    Args:
        data (RDD of np.ndarray):  An `RDD` consisting of NumPy arrays.

    Returns:
        np.ndarray: A multi-dimensional array where the number of rows and columns both equal the
            length of the arrays in the input `RDD`.
    &quot;&quot;&quot;
    mean = data.mean()
    normalized = data.map(lambda x: x-mean)
    return normalized.map(lambda x: np.outer(x, x)).sum()/data.count()
    
def pca(data, k=2):
    &quot;&quot;&quot;Computes the top `k` principal components, corresponding scores, and all eigenvalues.


    Args:
        data (RDD of np.ndarray): An `RDD` consisting of NumPy arrays.
        k (int): The number of principal components to return.

    Returns:
        tuple of (np.ndarray, RDD of np.ndarray, np.ndarray): A tuple of (eigenvectors, `RDD` of
            scores, eigenvalues).  Eigenvectors is a multi-dimensional array where the number of
            rows equals the length of the arrays in the input `RDD` and the number of columns equals
            `k`.  The `RDD` of scores has the same number of rows as `data` and consists of arrays
            of length `k`.  Eigenvalues is an array of length d (the number of features).
    &quot;&quot;&quot;
    cov = estimateCovariance(data)
    eigVals, eigVecs = eigh(cov)
    inds = np.argsort(eigVals)[::-1]
    topkComponent = eigVecs[:,inds[0:k]]
    scores = data.map(lambda x: np.dot(x, topkComponent))
    # Return the `k` principal components, `k` scores, and all eigenvalues
    return (topkComponent, scores, eigVals[inds])
    
    
def varianceExplained(data, k=1):
    &quot;&quot;&quot;Calculate the fraction of variance explained by the top `k` eigenvectors.

    Args:
        data (RDD of np.ndarray): An RDD that contains NumPy arrays which store the
            features for an observation.
        k: The number of principal components to consider.

    Returns:
        float: A number between 0 and 1 representing the percentage of variance explained
            by the top `k` eigenvectors.
    &quot;&quot;&quot;
    components, scores, eigenvalues = pca(data)
    return sum(eigenvalues[0:k])/sum(eigenvalues)
</code></pre>

<h2 id="toc_1">应用</h2>

<p>斑马鱼(zebrafish)大脑的响应<a href="http://larryim.cc/ML_lab5_pca_student.html">可视化</a>。</p>

<h2 id="toc_2">Reference</h2>

<ol>
<li>Machine Learning Course from Coursera. Andrew Ng</li>
<li>CS190: Scalable Machine Learning</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Covariance Matrix and its solution in Spark]]></title>
    <link href="http://larryim.cc/covariance_spark.html"/>
    <updated>2017-12-05T21:31:54+08:00</updated>
    <id>http://larryim.cc/covariance_spark.html</id>
    <content type="html"><![CDATA[
<p>A <code>covariance matrix</code>(协方差矩阵, 离差矩阵) is a matrix whose element in the \(i,j\) position is the covariance between the \(i^{th}\) and \(j^{th}\) elements of a <code>random vector</code>(随机向量,多元随机变量). Each element of a random vector is a scalar <code>random variable</code>(随机变量).</p>

<p>Because the covariance of the \(i^{th}\) random variable with itself is simply that random variable&#39;s variance, each element on the principal diagonal of the covariance matrix is the variance of one of the random variables:</p>

<p>The covariance between random variables \(X, Y\):</p>

<p>\[\text{cov}(X,Y)=\text{E}[(X-E[X])(Y-E[Y])]\]</p>

<p>Then the covariance matrix \(\sum\) is the matrix whose \((i,j)\) is the covariance</p>

<p>\[\sum(i,j) =\text{cov}(X_i,X_j)=\text{E}[(X_i-E[X_i])(Y_i-E[Y_i])]\]</p>

<p>where \(X=[X_1,...,X_n]\)</p>

<h2 id="toc_0">Distributed Computing: Spark</h2>

<p>Setup: Raw data \(\mathbf{P} \in \mathbb{R}^{n \times d}\)</p>

<ul>
<li>Step 1: Zero Mean Data \(X=\text{E}(P)\)

<ul>
<li>Compute \(d\) feature means, \(m\in \mathbf{R}^d\)</li>
<li>Communication \(m\) to all workers</li>
<li>Subtract \(m\) from each data point</li>
</ul></li>
<li>Step 2: Compute Covariance Matrix \( \mathbf{C}_{\mathbf X} = \frac{1}{n} \mathbf{X}^\top \mathbf{X} \,.\)

<ul>
<li>compute the outer product of each data point, </li>
<li>add together these outer products, and divide by the number of data points</li>
</ul></li>
</ul>

<p><img src="media/15124807146530/compute_covariance_matrix.png" alt="compute_covariance_matrix"/></p>

<p><u>Spark Code in Python</u>:</p>

<pre><code class="language-python">def estimateCovariance(data):
    &quot;&quot;&quot;Compute the covariance matrix for a given rdd.
   
    Args:
        data (RDD of np.ndarray):  An `RDD` consisting of NumPy arrays.

    Returns:
        np.ndarray: A multi-dimensional array where the number of rows and columns both equal the length of the arrays in the input `RDD`.
    &quot;&quot;&quot;
    mean = data.mean()
    normalized = data.map(lambda x: x-mean)
    return normalized.map(lambda x: np.outer(x, x)).sum()/data.count()
</code></pre>

<h2 id="toc_1">Reference</h2>

<ol>
<li><a href="https://en.wikipedia.org/wiki/Covariance_matrix">Covariance matrix from wikipedia</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Data Science Competition]]></title>
    <link href="http://larryim.cc/data_science_competition.html"/>
    <updated>2017-11-23T22:40:12+08:00</updated>
    <id>http://larryim.cc/data_science_competition.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">real-world application v. s.competition</h2>

<p><img src="media/15114480120681/real%20machine%20learning.png" alt="real machine learning"/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MongoDB]]></title>
    <link href="http://larryim.cc/15114445635476.html"/>
    <updated>2017-11-23T21:42:43+08:00</updated>
    <id>http://larryim.cc/15114445635476.html</id>
    <content type="html"><![CDATA[
<p><code>NoSQL</code>(<code>Not Only Sql</code>，非关系型数据库)，最初是为了大规模web应用，具有分布式、开源、可水平扩展等特点。它满足了对数据库高并发读写、对海量数据的高效率存储和访问、对数据库的高扩展性和高可用性的需求。</p>

<h2 id="toc_0">数据逻辑结构</h2>

<p><code>MongoDB</code>的逻辑结构主要由文档(<code>document</code>)、集合(<code>collection</code>)、数据库(<code>database</code>)这三部分组成的。</p>

<h2 id="toc_1">replica set</h2>

<p><code>Replica sets</code> distribute data across two or more machines for redundancy and automate failover in the event of server and network outages. <u>if the primary node fails, the cluster will pick a secondary node and automatically promote it to primary. When the former primary comes back online, it’ll do so as a secondary.</u></p>

<p><img src="media/15114445635476/replica%20set.png" alt="replica set"/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[`Filter`, `Map`, `Reduce`]]></title>
    <link href="http://larryim.cc/15108229432607.html"/>
    <updated>2017-11-16T17:02:23+08:00</updated>
    <id>http://larryim.cc/15108229432607.html</id>
    <content type="html"><![CDATA[
<pre><code class="language-python">&gt;&gt;&gt; foo = [2, 18, 9, 22, 17, 24, 8, 12, 27]
&gt;&gt;&gt; print filter(lambda x: x % 3 == 0, foo)
[18, 9, 24, 12, 27]
&gt;&gt;&gt; 
&gt;&gt;&gt; print map(lambda x: x * 2 + 10, foo)
[14, 46, 28, 54, 44, 58, 26, 34, 64]
&gt;&gt;&gt; 
&gt;&gt;&gt; print reduce(lambda x, y: x + y, foo)
139
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CS190: Scalable Machine Learning]]></title>
    <link href="http://larryim.cc/scalable%20machine%20learning.html"/>
    <updated>2017-11-16T14:52:20+08:00</updated>
    <id>http://larryim.cc/scalable%20machine%20learning.html</id>
    <content type="html"><![CDATA[
<p>see <a href="http://larryim.cc/notebook.html">Labs here</a></p>

<h3 id="toc_0">Distributing Computing</h3>

<p>Need more hardware to store/process modern data</p>

<p>Scale-up(one big machine)</p>

<ul>
<li>Can be very fast for medium scale problems</li>
<li>Expensive, specialized hardware</li>
<li>Eventually hit a wall</li>
</ul>

<p>Scale-out (many small machine, i.e., distributed)</p>

<ul>
<li>Commodity hardware, scales to massive problems</li>
<li>Need to deal with network communication</li>
<li>Added software complexity</li>
</ul>

<p>Apache Spark is a general, open-source cluster computing engine.</p>

<p>Well-suited for machine learning</p>

<ul>
<li>Fast iterative computations</li>
<li>Efficient communication primitives</li>
<li>Simple and expressive: APIs in Scala, Java, Python, R</li>
<li>Integrated higher-level libraries (<code>MLlib</code>, Spark SQL, Spark Streaming, GraphX)</li>
</ul>

<h3 id="toc_1">Typical Supervised Learning Pipeline</h3>

<ul>
<li>Obtain Raw Data</li>
<li>Feature Extraction</li>
<li>Supervised Learning</li>
<li>Evaluation</li>
<li>Predict</li>
</ul>

<p><img src="media/15108151409015/Typical%20Supervised%20Learning%20Pipeline.png" alt="Typical Supervised Learning Pipeline"/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[`MLlib`: Machine Learning in Apache Spark]]></title>
    <link href="http://larryim.cc/15108128842368.html"/>
    <updated>2017-11-16T14:14:44+08:00</updated>
    <id>http://larryim.cc/15108128842368.html</id>
    <content type="html"><![CDATA[
<p>[<a href="https://arxiv.org/pdf/1505.06807.pdf">https://arxiv.org/pdf/1505.06807.pdf</a>]</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Introduction to big data]]></title>
    <link href="http://larryim.cc/introdution_to_big_data.html"/>
    <updated>2017-10-31T19:52:26+08:00</updated>
    <id>http://larryim.cc/introdution_to_big_data.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">Characteristics of Big data</h2>

<p>3 V: Volume, Variety, Velocity</p>

<p>Most now agree with the characterization of big data using the 3 V’s coined by Doug Laney of Gartner:</p>

<p>·<code>Volume</code>: This refers to the vast amounts of data that is generated every second/minute/hour/day in our digitized world.</p>

<p>· <code>Velocity</code>: This refers to the speed at which data is being generated and the pace at which data moves from one point to the next.</p>

<p>· <code>Variety</code>: This refers to the ever-increasing different forms that data can come in, e.g., text, images, voice, geospatial.</p>

<p>A fourth V is now also sometimes added:</p>

<p>· <code>Veracity</code>: This refers to the quality of the data, which can vary greatly.</p>

<p>There are many other V&#39;s that gets added to these depending on the context. For our specialization, we will add:</p>

<p>· <code>Valence</code>: This refers to how big data can bond with each other, forming connections between otherwise disparate datasets.</p>

<p>The above V’s are the dimensions that characterize big data, and also embody its challenges: We have huge amounts of data, in different formats and varying quality, that must be processed quickly.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spark]]></title>
    <link href="http://larryim.cc/Spark_introduction.html"/>
    <updated>2017-10-29T10:53:40+08:00</updated>
    <id>http://larryim.cc/Spark_introduction.html</id>
    <content type="html"><![CDATA[
<p><code>Spark</code>是一个用来实现快速而通用的集群计算的平台。<code>Spark</code>扩展了广泛使用的<code>MapReduce</code>计算模型，而且高效地支持更多计算模式，包括交互式查询和流处理。</p>

<h2 id="toc_0">Reference</h2>

<ul>
<li>Holden Karau, Andy Konwinski, Patrick Wendell, et al. 2015. Learning Spark: Lightning-Fast Big Data Analysis. O&#39;Reilly Media</li>
<li></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hardware/Software Interface]]></title>
    <link href="http://larryim.cc/hardware_software_interface.html"/>
    <updated>2017-10-26T18:49:41+08:00</updated>
    <id>http://larryim.cc/hardware_software_interface.html</id>
    <content type="html"><![CDATA[
<p><code>Hardware/Software Interface</code>是由华盛顿大学在Coursera上开设的课程，与华盛顿大学CSE351课程保持一致。虽然目前这门课已经在Coursera下线，但完整视频、作业、实验仍旧在华盛顿大学<a href="https://courses.cs.washington.edu/courses/cse351/17sp/videos.html">CSE351课程主页</a>上可以观看和下载。</p>

<p>完成课程实验需要用到特定的环境，不然配置起来及其繁琐。幸好，可以下载已经配置好的虚拟机进行实验。虚拟机的地址目前仍旧有效，点击<a href="https://spark-public.s3.amazonaws.com/hardware/VM.7z">这里</a>下载(<code>MD5: 9A2A35B58FF716A84F17610FDEC972EC</code>)。</p>

<p>课程所用教科书是<code>CSAPP</code>(Computer System: A programmer&#39;s Perspective)第二版。非常幸运，教科书、作业和实验内容以及答案已经有人专门整理，并放在Github上了，可以直接<a href="https://github.com/ldfaiztt/CSE351">点击</a>下载。</p>

<p>下载完所有资料后，尽情的学习吧！</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Linux 内存布局]]></title>
    <link href="http://larryim.cc/15089188725996.html"/>
    <updated>2017-10-25T16:07:52+08:00</updated>
    <id>http://larryim.cc/15089188725996.html</id>
    <content type="html"><![CDATA[
<p><img src="media/15089188725996/15089292733459.png" alt=""/></p>

<p>典型内存空间布局<br/>
一个典型的Linux C程序内存空间由如下几部分组成：</p>

<ul>
<li>代码段（<code>.text</code>）。这里存放的是CPU要执行的指令。代码段是可共享的，相同的代码在内存中只会有一个拷贝，同时这个段是只读的，防止程序由于错误而修改自身的指令。</li>
<li>初始化数据段（<code>.data</code>）。这里存放的是程序中需要明确赋初始值的变量，例如位于所有函数之外的全局变量：<code>int val=&quot;100</code>。需要强调的是，以上两段都是位于程序的可执行文件中，内核在调用exec函数启动该程序时从源程序文件中读入。</li>
<li>未初始化数据段（<code>.bss</code>）。位于这一段中的数据，内核在执行该程序前，将其初始化为0或者null。例如出现在任何函数之外的全局变量：int sum;</li>
<li>栈（<code>Stack</code>）。函数中的局部变量以及在函数调用过程中产生的临时变量都保存在此段中。</li>
<li>堆（<code>Heap</code>）。这个段用于在程序中进行动态内存申请，例如经常用到的malloc，new系列函数就是从这个段中申请内存。</li>
</ul>

<pre><code class="language-C"> #include&lt;stdio.h&gt;    
 #include &lt;malloc.h&gt;    
     
 void print(char *,int);    
 int main()    
{    
      char *s1 = &quot;abcde&quot;;  //&quot;abcde&quot;作为字符串常量存储在常量区 s1、s2、s5拥有相同的地址  
      char *s2 = &quot;abcde&quot;;    
      char s3[] = &quot;abcd&quot;;    
      long int *s4[100];    
      char *s5 = &quot;abcde&quot;;    
      int a = 5;    
      int b =6;//a,b在栈上，&amp;a&gt;&amp;b地址反向增长    
     
     printf(&quot;variables address in main function: s1=%p  s2=%p s3=%p s4=%p s5=%p a=%p b=%p \n&quot;,     
             s1,s2,s3,s4,s5,&amp;a,&amp;b);   
     printf(&quot;variables address in processcall:n&quot;);    
        print(&quot;ddddddddd&quot;,5);//参数入栈从右至左进行,p先进栈,str后进 &amp;p&gt;&amp;str    
     printf(&quot;main=%p print=%p \n&quot;,main,print);    
     //打印代码段中主函数和子函数的地址，编译时先编译的地址低，后编译的地址高main&lt;print    
 }    
  
 void print(char *str,int p)    
{    
     char *s1 = &quot;abcde&quot;;  //abcde在常量区，s1在栈上    
     char *s2 = &quot;abcde&quot;;  //abcde在常量区，s2在栈上 s2-s1=6可能等于0，编译器优化了相同的常量，只在内存保存一份    
     //而&amp;s1&gt;&amp;s2    
     char s3[] = &quot;abcdeee&quot;;//abcdeee在常量区，s3在栈上，数组保存的内容为abcdeee的一份拷贝    
     long int *s4[100];    
     char *s5 = &quot;abcde&quot;;    
     int a = 5;    
     int b =6;    
     int c;    
     int d;           //a,b,c,d均在栈上，&amp;a&gt;&amp;b&gt;&amp;c&gt;&amp;d地址反向增长    
     char *q=str;   
     int m=p;           
     char *r=(char *)malloc(1);    
     char *w=(char *)malloc(1) ;  // r&lt;w 堆正向增长    
    
     printf(&quot;s1=%p s2=%p s3=%p s4=%p s5=%p a=%p b=%p c=%p d=%p str=%p q=%p p=%p m=%p r=%p w=%p \n&quot;,    
            s1,s2,s3,s4,s5,&amp;a,&amp;b,&amp;c,&amp;d,&amp;str,q,&amp;p,&amp;m,r,w);   
     /* 栈和堆是在程序运行时候动态分配的，局部变量均在栈上分配。 
        栈是反向增长的，地址递减；malloc等分配的内存空间在堆空间。堆是正向增长的，地址递增。   
        r,w变量在栈上(则&amp;r&gt;&amp;w)，r,w所指内容在堆中(即r&lt;w)。*/   
 }    
   
</code></pre>

<h2 id="toc_0">堆和栈区别</h2>

<p><img src="media/15089188725996/15089298484521.jpg" alt=""/></p>

<h2 id="toc_1">参考</h2>

<p><a href="http://blog.csdn.net/zhangzhebjut/article/details/39060253">http://blog.csdn.net/zhangzhebjut/article/details/39060253</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Valgrind]]></title>
    <link href="http://larryim.cc/Valgrind.html"/>
    <updated>2017-10-25T16:03:46+08:00</updated>
    <id>http://larryim.cc/Valgrind.html</id>
    <content type="html"><![CDATA[
<p><code>Valgrind</code> is an instrumentation framework for building dynamic analysis tools. There are Valgrind tools that can automatically detect many memory management and threading bugs, and profile your programs in detail. You can also use Valgrind to build new tools.</p>

<p>The Valgrind distribution currently includes six production-quality tools: </p>

<ul>
<li>a memory error detector</li>
<li>two thread error detectors </li>
<li>a cache and branch-prediction profiler</li>
<li>a call-graph generating cache and branch-prediction profiler</li>
<li>a heap profiler.</li>
</ul>

<p>It also includes three experimental tools: </p>

<ul>
<li>a stack/global array overrun detector,</li>
<li>a second heap profiler that examines how heap blocks are used</li>
<li>a SimPoint basic block vector generator.</li>
</ul>

<h2 id="toc_0">Installing on Mac</h2>

<p>The simplest way to install <code>valgrind</code> is using <code>brew</code>:</p>

<pre><code>brew install --HEAD valgrind
</code></pre>

<h2 id="toc_1">Demo</h2>

<p>Writing a C program named &#39;arrays.c&#39; without freeing the memory after allocating it dynamically using <code>malloc</code>, for example,</p>

<pre><code>int* heap_array = (int*) malloc(sizeof(int) * 5);
free(heap_array); ## deleting the statement...
</code></pre>

<p>After running by command</p>

<pre><code>valgrind ./arrays
</code></pre>

<p>you will see &quot;<strong>definitely lost: 20 bytes in 1 blocks</strong>!!&quot;</p>

<p><img src="media/15089186263565/Screen%20Shot%202017-10-25%20at%204.35.47%20PM.png" alt="Screen Shot 2017-10-25 at 4.35.47 P"/></p>

<h2 id="toc_2">Reference</h2>

<ul>
<li><a href="https://stackoverflow.com/questions/26564125/yosemite-and-valgrind">Yosemite and Valgrind</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine Learning Techniques: Support Vector Machine (SVM)]]></title>
    <link href="http://larryim.cc/machine_learning_techniques_SVM.html"/>
    <updated>2017-10-24T19:43:35+08:00</updated>
    <id>http://larryim.cc/machine_learning_techniques_SVM.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">Linear SVM</a>
<ul>
<li>
<a href="#toc_1">Standard Large-Margin Problem</a>
</li>
<li>
<a href="#toc_2">Support Vector Machine</a>
</li>
</ul>
</li>
<li>
<a href="#toc_3">Dual Support Vector Machine</a>
<ul>
<li>
<a href="#toc_4">Lagrange Dual Problem</a>
</li>
</ul>
</li>
</ul>


<h2 id="toc_0">Linear SVM</h2>

<p><img src="media/15088454152744/large_margin.png" alt="large_margin"/></p>

<p>Our goal is to find <code>largest-margin</code> separating hyperplane.</p>

<p><strong>Distance to Hyperplane</strong>: distance(\(x,b,w) = \frac{1}{\lVert w\rVert}|w^Tx+b|\)</p>

<h3 id="toc_1">Standard Large-Margin Problem</h3>

<p>Now the problem becomes:<br/>
<img src="media/15088454152744/hyperplane%20distance.png" alt="hyperplane distance"/></p>

<p><img src="media/15088454152744/standard%20problem.png" alt="standard proble"/></p>

<h3 id="toc_2">Support Vector Machine</h3>

<p><img src="media/15088454152744/svm%20with%20qp%20solver.png" alt="svm with qp solve"/></p>

<h2 id="toc_3">Dual Support Vector Machine</h2>

<p>\[\text{SVM} \equiv \min\limits_{b,w} (\max\limits_{\text{all}\,\alpha_n \ge 0} \mathcal{L}(b,w,\alpha))\]</p>

<h3 id="toc_4">Lagrange Dual Problem</h3>

<p>for any fixed \(\alpha&#39;\)  with all \(\alpha&#39;_n\ge\) 0,<br/>
\[\min\limits_{b,w} (\max\limits_{\text{all}\,\alpha_n \ge 0} \mathcal{L}(b,w,\alpha)) \ge \min\limits_{b,w}\mathcal{L}(b,w,\alpha&#39;)\]</p>

<p>for best \(\alpha&#39;\ge 0\) on RHS,</p>

<p>\[\min\limits_{b,w} (\max\limits_{\text{all}\,\alpha_n \ge 0} \mathcal{L}(b,w,\alpha)) \ge \max\limits_{\text{all}\,\alpha_n \ge 0}(\min\limits_{b,w}  \mathcal{L}(b,w,\alpha))\]</p>

<p>Strong duality of Quadratic Programming<br/>
\[\min\limits_{b,w} (\max\limits_{\text{all}\,\alpha_n \ge 0} \mathcal{L}(b,w,\alpha)) \ge \max\limits_{\text{all}\,\alpha_n \ge 0}(\min\limits_{b,w}  \mathcal{L}(b,w,\alpha))\]</p>

<ul>
<li>&#39;&gt;=&#39;: weak duality</li>
<li>&#39;=&#39;: strong duality, true for QP if

<ul>
<li>convex primal</li>
<li>feasible primal</li>
<li>linear constraints</li>
</ul></li>
</ul>

<p>Dual Formulation of Support Vector Machine:</p>

<p><img src="media/15088454152744/Screen%20Shot%202017-10-25%20at%201.43.46%20PM.png" alt="Screen Shot 2017-10-25 at 1.43.46 P"/></p>

<p><img src="media/15088454152744/Screen%20Shot%202017-10-25%20at%201.46.05%20PM.png" alt="Screen Shot 2017-10-25 at 1.46.05 P"/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine Learning Foundations - Mathematical Foundations]]></title>
    <link href="http://larryim.cc/15088244818278.html"/>
    <updated>2017-10-24T13:54:41+08:00</updated>
    <id>http://larryim.cc/15088244818278.html</id>
    <content type="html"><![CDATA[
<ul>
<li>
<a href="#toc_0">(1) The Learning Problem</a>
<ul>
<li>
<a href="#toc_1">Key Essence of Machine Learning</a>
</li>
<li>
<a href="#toc_2">Formalize the Learning Problem</a>
</li>
<li>
<a href="#toc_3">Machine Learning and other Fields</a>
</li>
</ul>
</li>
<li>
<a href="#toc_4">(2) Learning to answer Yes or No</a>
<ul>
<li>
<a href="#toc_5">Perceptron Hypothesis Set</a>
</li>
</ul>
</li>
<li>
<a href="#toc_6">(3) Types of Learning</a>
<ul>
<li>
<a href="#toc_7">Learning with Different Output Space</a>
</li>
<li>
<a href="#toc_8">Learning with Different Data Label</a>
</li>
<li>
<a href="#toc_9">Learning with Different Protocol</a>
</li>
<li>
<a href="#toc_10">Learning with Different Input Space</a>
</li>
</ul>
</li>
<li>
<a href="#toc_11">(4) Feasibility of Learning</a>
<ul>
<li>
<a href="#toc_12">Learning outside training examples</a>
</li>
<li>
<a href="#toc_13">In and out of sample error</a>
</li>
<li>
<a href="#toc_14">Connection to Real Learning</a>
<ul>
<li>
<a href="#toc_15">Bound of BAD data</a>
</li>
<li>
<a href="#toc_16">The Statistical Learning Flow</a>
</li>
</ul>
</li>
</ul>
</li>
<li>
<a href="#toc_17">(5) Training versus Testing</a>
<ul>
<li>
<a href="#toc_18">Dichotomy</a>
</li>
<li>
<a href="#toc_19">Growth function</a>
</li>
<li>
<a href="#toc_20">Shatter and Break point</a>
</li>
</ul>
</li>
<li>
<a href="#toc_21">(6) Theory of generalization</a>
<ul>
<li>
<a href="#toc_22">Bounding Function</a>
</li>
<li>
<a href="#toc_23">VC Bound</a>
</li>
</ul>
</li>
<li>
<a href="#toc_24">(7) VC dimension</a>
<ul>
<li>
<a href="#toc_25">Interpreting VC Dimension</a>
</li>
</ul>
</li>
<li>
<a href="#toc_26">(8): Noise and error</a>
<ul>
<li>
<a href="#toc_27">Error Measure</a>
</li>
</ul>
</li>
</ul>


<p>The course, Machine Learning Foundations - Mathematical Foundations(机器学习基石), is taught by Hsuan-Tien Lin on Coursera (<a href="https://www.coursera.org/learn/ntumlone-mathematicalfoundations/home/info">course info page</a>).</p>

<p>As the name of the course indicates, the course teaches the most fundamental algorithmic, theoretical, mathematical background of machine learning.</p>

<p>The article summaries the key concepts and conclusions in the course. Each section in the article corresponding to one chapter in the course.</p>

<h2 id="toc_0">(1) The Learning Problem</h2>

<h3 id="toc_1">Key Essence of Machine Learning</h3>

<p>Machine Learning: improving some <strong>performance measure</strong> with experience <strong>computed</strong> from <strong>data</strong></p>

<ul>
<li>exists some &#39;underlying pattern&#39; to be learned — so &#39;performance measure&#39; can be improved</li>
<li>but no programmable (easy) definition — so &#39;ML&#39; is needed</li>
<li>somehow there is data about the pattern — so ML has some &#39;inputs&#39; to learn from</li>
</ul>

<h3 id="toc_2">Formalize the Learning Problem</h3>

<p><strong>Basic Notations</strong>:</p>

<ul>
<li>input: \(x\in\mathcal{X}\)</li>
<li>output: \(y\in\mathcal{Y}\)</li>
<li>unknown pattern to be learned \(\Leftrightarrow\) target function:
\(f:\mathcal{X}\rightarrow\mathcal{Y}\) </li>
<li>data \(\Leftrightarrow\) training examples: \(\mathcal{D}={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}\)</li>
<li>hypothesis \(\Leftrightarrow\) skill with hopefully good performance: \(g: \mathcal{X}\rightarrow \mathcal{Y}\)</li>
<li>final hypothesis: \(\mathcal{g}\approx f\)</li>
</ul>

<p><strong>Machine Learning: use data \(\mathcal{D}\) to compute hypothesis \(g\) that approximates target \(f\).</strong></p>

<p><img src="media/15088244818278/Screen%20Shot%202017-11-07%20at%2012.35.17%20PM.png" alt="Screen Shot 2017-11-07 at 12.35.17 P"/></p>

<h3 id="toc_3">Machine Learning and other Fields</h3>

<ul>
<li>Machine Learning: use data to compute hypothesis \(g\) that approximates target \(f\)</li>
<li>Data Mining: Use (huge) data to find property that is interesting</li>
</ul>

<p>\(\Longleftrightarrow\)If &#39;interesting property&#39; same as &#39;hypothesis that approximate target&#39;, Machine Learning = Data Mining<br/>
\(\Longleftrightarrow\) If &#39;interesting property&#39; related to &#39;hypothesis that approximate target&#39;, data mining can help machine learning.<br/>
\(\Longleftrightarrow\) traditional data mining also focuses on efficient computation in large database.</p>

<h2 id="toc_4">(2) Learning to answer Yes or No</h2>

<h3 id="toc_5">Perceptron Hypothesis Set</h3>

<p>Vector Form of Perceptron Hypothesis:</p>

<p>\[h(x) = \text{sign}(\sum\limits_{i=0}^dw_ix_i)=\text{sign}(W^Tx)\]</p>

<p><strong>Perceptron Learning Algorithm</strong>(PLA):</p>

<p><img src="media/15088244818278/PLA.png" alt="PLA"/></p>

<p><strong>Pocket Algorithm</strong>: modify <code>PLA</code> algorithm by keeping best weights in pocket</p>

<p><img src="media/15088244818278/pocket.png" alt="pocket"/></p>

<h2 id="toc_6">(3) Types of Learning</h2>

<h3 id="toc_7">Learning with Different Output Space</h3>

<ul>
<li>binary classification: \(\mathcal{Y}=\{-1,+1\}\)</li>
<li>multiclass classification: \(\mathcal{Y}=\{1, 2,..., K\}\)</li>
<li>regression: \(\mathcal {Y} =\mathbb{R}\)</li>
<li>structured learning: \(\mathcal{Y}=\) structures</li>
</ul>

<h3 id="toc_8">Learning with Different Data Label</h3>

<ul>
<li>supervised: all \(y_n\)</li>
<li>unsupervised: no \(y_n\)</li>
<li>semi-supervised: some \(y_n\)</li>
<li>reinforcement: implicit \(y_n\) by goodness (\(\hat y_n\))</li>
</ul>

<h3 id="toc_9">Learning with Different Protocol</h3>

<p>Protocol \(\Longleftrightarrow\) Learning Philosophy.</p>

<ul>
<li>batch: all known data</li>
<li>online: sequential (passive) data</li>
<li>active: strategically-observed data</li>
</ul>

<h3 id="toc_10">Learning with Different Input Space</h3>

<ul>
<li>concrete: sophisticated (and related) physical meaning e.g. user features(age, sex, education level)</li>
<li>raw: simple physical meaning e.g. image features</li>
<li>abstract: no (or little) physical meaning e.g. user/image IDs</li>
</ul>

<h2 id="toc_11">(4) Feasibility of Learning</h2>

<h3 id="toc_12">Learning outside training examples</h3>

<p><strong>No Free Lunch</strong>: Learning from \(D\) (\(D\): training examples) (to infer something outside \(D\)) is doomed to fail if any unknown \(f\) can happen.</p>

<ul>
<li>\(g\approx f\) inside \(D\): Possible!</li>
<li>\(g\approx f\) outside \(D\): Impossible!</li>
</ul>

<h3 id="toc_13">In and out of sample error</h3>

<p>For any fixed \(h\), in &#39;big&#39; data (N large), <code>in-sample error</code> \(E_{in}(h)\) is probably close to <code>out-of-sample error</code> \(E_{out}(h)\) (within \(\epsilon\)):</p>

<p>\(P(|E_{in}(h)-E_{out}(h)|&gt; \epsilon) \le 2\exp(-2\epsilon^2N)\)</p>

<p>The equation above is called <code>Hoeffding&#39;s Inequality</code>. The statement \(E_{in}(h)=E_{out}(h)\) is <code>probably approximately correct</code> (<strong>PAC</strong>, 大概近似正确).</p>

<p>If large \(N\), we can <strong>probably</strong> infer unknown \(E_{out}(h)\) by known \(E_{in}(h)\).</p>

<p><code>in-sample error</code> \(E_{in}(h)\) denotes orange fraction in sample, <code>out-of-sample error</code> \(E_{out}(h)\) denotes orange probability in bin, When we want to infer the orange probability in bin.</p>

<p><img src="media/15088244818278/Ein_Eout.png" alt="Ein_Eout"/></p>

<p><img src="media/15088244818278/Connection%20to%20Learning.png" alt="Connection to Learning"/></p>

<h3 id="toc_14">Connection to Real Learning</h3>

<p><strong>Bad data for One \(h\)</strong>: </p>

<p>\(E_{out}(h)\) and \(E_{in}(h)\) far away: e.g. \(E_{out}\) big (far from \(f\)), but \(E_{in}\) small (correct on most examples).</p>

<p><strong>Bad data for many \(h\)</strong>:</p>

<p>\(\Leftrightarrow\) no &#39;freedom of choices&#39; by learning algorithm \(\mathcal{A}\)<br/>
\(\Leftrightarrow\) there exists some \(h\) such that \(E_{out}(h)\) and \(E_{in}(h)\) far away</p>

<h4 id="toc_15">Bound of BAD data</h4>

<p>Form \(M\) hypotheses, what is the bound of \(\mathbb{P}_{\mathcal{D}} [\text{BAD} \; \mathcal{D}]\)</p>

<p><img src="media/15088244818278/bound_of_bad_data.png" alt="bound of bad data"/></p>

<p>The <code>union bound</code> <a href="https://en.wikipedia.org/wiki/Boole%27s_inequality">WIKI</a> (布尔不等式), also known as <code>Boole&#39;s inequality</code>, says that for any finite or countable set of events, the probability that at least one of the events happens is no greater than the sum of the probabilities of the individual events.</p>

<h4 id="toc_16">The Statistical Learning Flow</h4>

<p>If \(|\mathcal{H}|=M\) finite, \(N\) large enough, for whatever \(g\) picked by \(\mathcal{A}\), \(E_{out}(g)\approx E_{in}(g)\)</p>

<p>if \(\mathcal{A}\) finds one \(g\) with \(E_{in}(g)\approx 0\),<br/>
PAC gaurantee for \(E_{out}(g)\approx 0\) \(\rightarrow\) learning possilbe!</p>

<h2 id="toc_17">(5) Training versus Testing</h2>

<p>For batch and supervised binary classification, \(g\approx f\Longleftrightarrow E_{out}(g)\approx 0\) achieved through \(E_{out}(g)\approx E_{in}(g)\) and \(E_{in}(g)\approx 0\).</p>

<p>In order to achieve two conditions above, learning split to two central questions:</p>

<ul>
<li>Can we make sure that \(E_{out}(g)\) is close enough to \(E_{in}(g)\)?</li>
<li>Can we make \(E_{in}(g)\) small enough?</li>
</ul>

<p>In order to understand trade-off for &#39;right&#39; \(\mathcal{H}\), we establish a finite quantity that replace \(M\),</p>

<p>\[\mathbb{P}[|E_{in}9g)-E_{out}(g)|&gt;\epsilon] \le 2\cdot m_\mathcal{H} \cdot \exp(-2\epsilon^2 N)\]</p>

<h3 id="toc_18">Dichotomy</h3>

<p>Define hypothesis set \(\mathcal{H}\):</p>

<p>\(\mathcal{H}=\{\text{hypothesis}\quad h: \mathcal{X}\rightarrow{\text{x,o}}\}\)</p>

<p>And we call hypothesis &#39;limited&#39; to the eyes of \(x_1, x_2,..., x_N\) a dichotomy:</p>

<p>\[\mathcal{H}(x_1,x_2,...,x_N)=\{(h(x_1),h(x_2),...,h(x_N)) \quad | \quad h \in \mathcal{H} \}\]</p>

<p>One can think of the dichotomies \(\mathcal{H}(x_1,x_2,...,x_N)\) as a set of hypotheses just like \(\mathcal{H}\)  is, except that the hypotheses are seen through the eyes of \(N\) points only.</p>

<p><strong>Note</strong>: hypotheses \(\mathcal{H}\) and dichotomy \(\mathcal{H}(x_1,x_2,...,x_N)\) are different!</p>

<h3 id="toc_19">Growth function</h3>

<p>Growth function is <strong>the maximum number of dichotomies</strong> that can be generated by \(\mathcal{H}\) on <strong>any</strong> \(N\) points: </p>

<p>\[m_\mathcal{H}(N) = \max\limits_{x_1,x_2,...,x_N\in\mathcal{X}}|\mathcal{H}(x_1,x_2,...,x_N)|\]</p>

<p>Growth function is finite, its upper-bound is \(2^N\).</p>

<p>Growth functions in different situation:</p>

<ul>
<li>positive rays: \(m_\mathcal{H}(N)=N+1\)</li>
<li>positive intervals: \(m_\mathcal{H}(N)=\frac{1}{2}N^2+\frac{1}{2}N+1\)</li>
<li>convex sets: \(m_\mathcal{H}(N)=2^N \)</li>
<li>2D perceptrons: \(m_\mathcal{H}(N)=2^N \text{in some cases}\)</li>
</ul>

<h3 id="toc_20">Shatter and Break point</h3>

<p><strong>If no \(k\) inputs can be shattered by \(\mathcal{H}\) , call \(k\) a break point for \(\mathcal{H}\).</strong></p>

<p>if \(m_H(K) &lt; 2^k\), call k a <strong>break point</strong> for \(\mathcal{H}\).</p>

<ul>
<li>since \(k\) is a break point, \(k+1, k+2, k+3,...,\) also break points</li>
</ul>

<p>minimum break points for different \(\mathcal{H}\):</p>

<ul>
<li>positive rays: k=2</li>
<li>positive intervals: k=3</li>
<li>convex sets: none</li>
<li>2D perceptrons: k=4</li>
</ul>

<h2 id="toc_21">(6) Theory of generalization</h2>

<h3 id="toc_22">Bounding Function</h3>

<p>Bounding Function \(B(N,k)\) is maximum possible \(m_\mathcal{H}(N)\) when break point \(=k\).</p>

<p>\[B(N,k) \le \sum\limits_{i=0}^{k-1} \left(\stackrel N i\right)\]</p>

<p><u>For fixed</u> \(k, B(N,k)\) <u>upper bounded by</u> \(ploy(N)\), \(\rightarrow m_{\mathcal{H}}(N)\) is \(poly(N)\) <u>if break point exists</u>.</p>

<h3 id="toc_23">VC Bound</h3>

<p>When N large enough, </p>

<p>\[P[\exists h\in \mathcal{H} \quad s.t.\quad  |E_{in}(h)-E_{out}(h)|&gt; \epsilon]\le 4 m_\mathcal{H}(2N)\exp(-\frac{1}{8}\epsilon^2N)\]</p>

<p>The above equation called <u><strong>Vapnik-Chervonenkis (VC) bound</strong></u>.</p>

<h2 id="toc_24">(7) VC dimension</h2>

<p>VC dimension, the formal name of <strong>maximum non-break point</strong>, denoted \(d_{VC}(\mathcal{H})\), is </p>

<p>\[\text{largest N for which } m_{\mathcal{H}}(N)=2^N\]</p>

<ul>
<li>positive rays: \(d_{VC}=1\)</li>
<li>positive intervals: \(d_{VC}=2\)</li>
<li>convex sets: \(d_{VC}= \infty \)</li>
<li>2D perceptrons: \(d_{VC}=3\)</li>
<li>d dimension Perceptrons: \(d_{VC}=d+1\)</li>
</ul>

<h3 id="toc_25">Interpreting VC Dimension</h3>

<p><img src="media/15088244818278/the%20vc%20message.png" alt="the vc message"/><br/>
<img src="media/15088244818278/the%20vc%20message2.png" alt="the vc message2"/></p>

<h2 id="toc_26">(8): Noise and error</h2>

<p><strong>Noise</strong> in \(x\) and \(y\):</p>

<p><img src="media/15088244818278/Noises.png" alt="Noises"/></p>

<p>NOTE: <strong>VC holds for \(x \stackrel{i.i.d}{\backsim} P(x), y \stackrel{i.i.d}{\backsim} P(y|x)\)</strong></p>

<p><img src="media/15088244818278/New%20Learning%20Law.png" alt="New Learning Law"/></p>

<h3 id="toc_27">Error Measure</h3>

<p>Two Important Pointwise Error Measures</p>

<p><strong>0/1 error</strong>(often for classification): err(\(\tilde y,y)=[\tilde y\ne y]\) <br/>
<strong>squared error</strong>(often for regression): err(\(\tilde y,y)=(\tilde y\ne y)^2\)</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[主动学习]]></title>
    <link href="http://larryim.cc/active_learning_intro.html"/>
    <updated>2017-10-23T17:11:38+08:00</updated>
    <id>http://larryim.cc/active_learning_intro.html</id>
    <content type="html"><![CDATA[
<p>主动学习算法通过迭代抽样，主动寻找有利于提升效果的样本，从而减少了训练集的大小。</p>

<p>获取训练样本不仅费时、费力，而且训练集包含大量的冗余样本。</p>

<p>主动学习算法主要分为两阶段：第一阶段为<strong>初始化阶段</strong>，随机从未标注样本中选取小部分，由督导者\(\mathcal{S}\)标注，作为训练集建立初始分类器模型；第二阶段为<strong>循环查询阶段</strong>，督导者\(\mathcal{S}\)从未标注样本集\(U\)中，按照某种查询标准\(Q\)，选取一定的未标注样本进行标注，并加到训练样本集\(L\)中，重新训练分类器，直至达到训练停止标准为止。</p>

<h1 id="toc_0">Reference</h1>

<ul>
<li>LIU Kang, QIAN Xu, WANG Ziqiang. Survey on active learning algorithms. Computer Engineering and Applications, 2012, 48（34）：1-4.</li>
<li>Zongwei Zhou1, Jae Shin1, Lei Zhang, et al. 2017. Fine-tuning Convolutional Neural Networks for Biomedical Image Analysis: Actively and Incrementally. CVPR. <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_Fine-Tuning_Convolutional_Neural_CVPR_2017_paper.pdf">pdf</a> </li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[JSON/BSON]]></title>
    <link href="http://larryim.cc/15086547919641.html"/>
    <updated>2017-10-22T14:46:31+08:00</updated>
    <id>http://larryim.cc/15086547919641.html</id>
    <content type="html"><![CDATA[
<p><code>JSON</code> stands for <code>Javascript Object Notation</code>. It is a lightweight data interchange format, similar to <code>XML</code>, <code>SOAP</code> and <code>YAML</code>.</p>

<p><code>JSON</code> syntax is based around three simple data types:</p>

<ul>
<li>Name-value pair</li>
<li>JSON Object: a collection of name-value pairs or Arrays encased in curly brackets.</li>
<li>JSON Array: a lift of values encased in square brackets.</li>
</ul>

<p>E.g. a student in an university with basic information: name, id number, GPA etc:</p>

<pre><code class="language-python">
{
    &quot;name&quot;:&quot;Jacob Bellamy&quot;,
    &quot;id_number&quot;:3352976,
    &quot;gpa&quot;:8.2,
    &quot;courses&quot;:[&quot;Compsci 101&quot;, &quot;Compsci 105&quot;, &quot;Phil 101&quot;, &quot;Maths 108&quot;],
    &quot;fees_paid&quot;: True,
    &quot;address&quot;: {
        &quot;street_address&quot;: &quot;1 Horse Lane&quot;,
        &quot;city&quot;: &quot;Auckland&quot;,
        &quot;post_code&quot;: 1632}
}
</code></pre>

<h2 id="toc_0">Python之<code>json</code>模块</h2>

<p>Python的<code>json</code>模块可以用来编码和解码JSON数据。其最主要的两个函数是<code>json.dumps()</code>(encoding,编码)和<code>json.loads()</code>(decoding, 解码).</p>

<pre><code class="language-python">import json

student_info = {
    &quot;name&quot;:&quot;Jacob Bellamy&quot;,
    &quot;id_number&quot;:3352976,
    &quot;gpa&quot;:8.2,
    &quot;courses&quot;:[&quot;Compsci 101&quot;, &quot;Compsci 105&quot;, &quot;Phil 101&quot;, &quot;Maths 108&quot;],
    &quot;fees_paid&quot;: True,
    &quot;address&quot;: {
        &quot;street_address&quot;: &quot;1 Horse Lane&quot;,
        &quot;city&quot;: &quot;Auckland&quot;,
        &quot;post_code&quot;: 1632}
}

student_info_json = json.dumps(student_info)

print(student_info_json)
</code></pre>

<pre><code>{&quot;name&quot;: &quot;Jacob Bellamy&quot;, &quot;id_number&quot;: 3352976, &quot;gpa&quot;: 8.2, &quot;courses&quot;: [&quot;Compsci 101&quot;, &quot;Compsci 105&quot;, &quot;Phil 101&quot;, &quot;Maths 108&quot;], &quot;fees_paid&quot;: true, &quot;address&quot;: {&quot;street_address&quot;: &quot;1 Horse Lane&quot;, &quot;city&quot;: &quot;Auckland&quot;, &quot;post_code&quot;: 1632}}
</code></pre>

<p>如果你要处理的是文件而不是字符串，你可以使用<code>json.dump()</code> 和 <code>json.load()</code> 来编码和解码<code>JSON</code>数据。例如：</p>

<pre><code class="language-python"># Writing JSON data
with open(&#39;student_info.json&#39;, &#39;w&#39;) as f:
    json.dump(student_info_json, f)
 
# Reading data back
with open(&#39;student_info.json&#39;, &#39;r&#39;) as f:
    data = json.load(f)
data
</code></pre>

<pre><code>&#39;{&quot;name&quot;: &quot;Jacob Bellamy&quot;, &quot;id_number&quot;: 3352976, &quot;gpa&quot;: 8.2, &quot;courses&quot;: [&quot;Compsci 101&quot;, &quot;Compsci 105&quot;, &quot;Phil 101&quot;, &quot;Maths 108&quot;], &quot;fees_paid&quot;: true, &quot;address&quot;: {&quot;street_address&quot;: &quot;1 Horse Lane&quot;, &quot;city&quot;: &quot;Auckland&quot;, &quot;post_code&quot;: 1632}}&#39;
</code></pre>

<h2 id="toc_1">Binary JSON - BSON</h2>

<p><code>BSON</code> /ˈbiːsən/, standing for <code>Binary JSON</code>, is a computer data interchange format used mainly as a data storage and network transfer format in the <code>MongoDB</code> database.</p>

<pre><code class="language-python">import bson      #installed with the pymongo package
data = bson.decode_file_iter(open(&#39;example.bson&#39;, &#39;rb&#39;))
</code></pre>

<h2 id="toc_2">Reference</h2>

<ul>
<li>Brad Miller, David Ranum. 2013. Problem Solving with Algorithms and Data Structures.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Cdiscount’s Image Classification Challenge]]></title>
    <link href="http://larryim.cc/cdiscount_challenge.html"/>
    <updated>2017-10-22T14:07:43+08:00</updated>
    <id>http://larryim.cc/cdiscount_challenge.html</id>
    <content type="html"><![CDATA[
<p><a href="https://www.kaggle.com/c/cdiscount-image-classification-challenge">Link</a></p>

<p><code>Cdiscount.com</code> generated nearly 3 billion euros last year, making it France’s largest non-food e-commerce company. While the company already sells everything from TVs to trampolines, the list of products is still rapidly growing. By the end of this year, Cdiscount.com will have over 30 million products up for sale. This is up from 10 million products only 2 years ago. Ensuring that so many products are well classified is a challenging task.</p>

<p>Currently, <code>Cdiscount.com</code> applies machine learning algorithms to the text description of the products in order to automatically predict their category. As these methods now seem close to their maximum potential, Cdiscount.com believes that the next quantitative improvement will be driven by the application of data science techniques to images.</p>

<p><strong>In this challenge you will be building a model that automatically classifies the products based on their images</strong>. As a quick tour of Cdiscount.com&#39;s website can confirm, one product can have one or several images. The data set Cdiscount.com is making available is unique and characterized by superlative numbers in several ways:</p>

<ul>
<li>Almost 9 million products: half of the current catalogue</li>
<li>More than 15 million images at 180x180 resolution</li>
<li>More than 5000 categories: yes this is quite an extreme multi-class classification!</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Towards Accurate Multi-person Pose Estimation in the Wild]]></title>
    <link href="http://larryim.cc/15066169438683.html"/>
    <updated>2017-09-29T00:42:23+08:00</updated>
    <id>http://larryim.cc/15066169438683.html</id>
    <content type="html"><![CDATA[

]]></content>
  </entry>
  
</feed>
